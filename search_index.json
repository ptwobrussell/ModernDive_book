[["index.html", "Statistical Inference via Data Science A ModernDive into R and the Tidyverse Welcome to ModernDive", " Statistical Inference via Data Science A ModernDive into R and the Tidyverse Chester Ismay and Albert Y. Kim Foreword by Kelly S. McConville December 05, 2023 Welcome to ModernDive This is the website for Statistical Inference via Data Science: A ModernDive into R and the Tidyverse! Visit the GitHub repository for this site and find the book on Amazon. You can also purchase it at CRC Press. This work by Chester Ismay and Albert Y. Kim is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["foreword.html", "Foreword", " Foreword "],["1-CS2.html", "Chapter 1 Probability Case Study 1.1 Objectives 1.2 Introduction to probability models 1.3 Probability models 1.4 Case study 1.5 Homework Problems Solutions Manual", " Chapter 1 Probability Case Study 1.1 Objectives Use R to simulate a probabilistic model. Use basic counting methods. 1.2 Introduction to probability models In this second block of material we will focus on probability models. We will take two approaches, one is mathematical and the other is computational. In some cases we can use both methods on a problem and in others only the computational approach is feasible. The mathematical approach to probability modeling allows us insight into the problem and the ability to understand the process. Simulation has a much greater ability to generalize but can be time intensive to run and often requires the writing of custom functions. This case study is extensive and may seem overwhelming, but do not worry. We will discuss these ideas again in the many chapters we have coming up this block. 1.3 Probability models Probability models are an important tool for data analysts. They are used to explain variation in outcomes that cannot be explained by other variables. We will use these ideas in the Statistical Modeling Block to help us make decisions about our statistical models. Often probability models are used to answer a question of the form “What is the chance that …..?” This means that we typically have an experiment or trial where multiple outcomes are possible and we only have an idea of the frequency of those outcomes. We use this frequency as a measure of the probability of a particular outcome. For this block we will focus just on probability models. To apply a probability model we will need to Select the experiment and its possible outcomes. Have probability values for the outcomes which may include parameters that determine the probabilities. Understand the assumptions behind the model. 1.4 Case study There is a famous example of a probability question that we will attack in this case study. The question we want to answer is “In a room of \\(n\\) people what is the chance that at least two people have the same birthday?” Exercise: The typical classroom at USAFA has 18 students in it. What do you think the chance that at least two students have the same birthday?1 1.4.1 Break down the question The first action we should take is to understand what is being asked. What is the experiment or trial? What does it mean to have the same birthday? What about leap years? What about the frequency of births? Are some days less likely than others? Exercise: Discuss these questions and others that you think are relevant.2 The best first step is to make a simple model, often these are the only ones that will have a mathematical solution. For our problem this means we answer the above questions. We have a room of 18 people and we look at their birthdays. We either have two or more birthdays matching or not; thus there are two outcomes. We don’t care about the year, only the day and month. Thus two people born on May 16th are a match. We will ignore leap years. We will assume that a person has equal probability of being born on any of the 365 days of the year. At least two means we could have multiple matches on the same day or several different days where multiple people have matching birthdays. 1.4.2 Simulate (computational) Now that we have an idea about the structure of the problem, we next need to think about how we would simulate a single classroom. We have 18 students in the classroom and they all could have any of the 365 days of the year as a birthday. What we need to do is sample birthdays for each of the 18 students. But how do we code the days of the year? An easy solution is to just label the days from 1 to 365. The function seq() does this for us. days &lt;- seq(1, 365) Next we need to pick one of the days using the sample function. Note that we set the seed to get repeatable results, this is not required. set.seed(2022) sample(days, 1) [1] 228 The first person was born on the 228th day of the year. Since R works on vectors, we don’t have to write a loop to select 18 days, we just have sample() do it for us. class &lt;- sample(days, size = 18, replace = TRUE) class [1] 206 311 331 196 262 191 206 123 233 270 248 7 349 112 1 307 288 354 What do we want R to do? Sample from the numbers 1 to 365 with replacement, which means a number can be picked more than once. Notice in our sample we have at least one match, although it is difficult to look at this list and see the match. Let’s sort them to make it easier for us to see. sort(class) [1] 1 7 112 123 191 196 206 206 233 248 262 270 288 307 311 331 349 354 The next step is to find a way in R for the code to detect that there is a match. Exercise: What idea(s) can we use to determine if a match exists? We could sort the data and look at differences in sequential values and then check if the set of differences contains a zero. This seems to be computationally expensive. Instead we will use the function unique() which gives a vector of unique values in an object. The function length() gives the number of elements in the vector. length(unique(class)) [1] 17 Since we only have 17 unique values in a vector of size 18, we have a match. Now let’s put this all together to generate another classroom of size 18. length(unique(sample(days, size = 18, replace = TRUE))) [1] 16 The next problem that needs to be solved is how to repeat the classrooms and keep track of those that have a match. There are several functions we could use to include replicate() but we will use do() from the mosaic package because it returns a data frame so we can use tidyverse verbs to wrangle the data. The do() function allows us to repeat an operation many times. The following template do(n) * {stuff to do} # pseudo-code where {stuff to do} is typically a single R command, but may be something more complicated. Load the libraries. library(mosaic) library(tidyverse) do(5)*length(unique(sample(days, size = 18, replace = TRUE))) length 1 18 2 17 3 17 4 17 5 18 Let’s repeat for a larger number of simulated classroom, remember you should be asking yourself: What do I want R to do? What does R need to do this? (do(1000)*length(unique(sample(days, size = 18, replace = TRUE)))) %&gt;% mutate(match = if_else(length == 18, 0, 1)) %&gt;% summarize(prob = mean(match)) prob 1 0.36 This is within 2 decimal places of the mathematical solution we develop shortly. How many classrooms do we need to simulate to get an accurate estimate of the probability of a match? That is a statistical modeling question and it depends on how much variability we can accept. We will discuss these ideas later in the book. For now, you can run the code multiple times and see how the estimate varies. If computational power is cheap, you can increase the number of simulations. (do(10000)*length(unique(sample(days, size = 18, replace = TRUE)))) %&gt;% mutate(match = if_else(length == 18, 0, 1)) %&gt;% summarize(prob = mean(match)) prob 1 0.3442 1.4.3 Plotting By the way, the method we have used to create the data allows us to summarize the number of unique birthdays using a table or bar chart. Let’s do that now. Note that since the first argument in tally() is not data then the pipe operator will not work without some extra effort. We must tell R that the data is the previous argument in the pipeline and thus use the symbol . to denote this. (do(1000)*length(unique(sample(days, size = 18, replace = TRUE)))) %&gt;% tally(~length, data = .) length 14 15 16 17 18 1 7 52 253 687 Figure 1.1 is a plot of the number of unique birthdays in our sample. (do(1000)*length(unique(sample(days, size = 18, replace = TRUE)))) %&gt;% gf_bar(~length) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;Number of unique birthdays&quot;, y = &quot;Count&quot;) FIGURE 1.1: Bar chart of the number of unique birthdays in the sample. Exercise: What does it mean if the length of unique birthdays is 16, in terms of matches?3 1.4.4 Mathematical solution To solve this problem mathematically, we will step through the logic one step at a time. One of the key ideas that we will see many times is the idea of the multiplication rule. This idea is the foundation for permutation and combinations which are counting methods frequently used in probability calculations. The first step that we take is to understand the idea of 2 or more people with the same birthday. With 18 people, there are a great deal of possibilities for 2 or more birthdays. We could have exactly 2 people with the same birthday. We could have 18 people with the same birthday, We could have 3 people with the same birthday and another 2 people with the same birthday but different from the other 3. Accounting for all these possibilities is too large a counting process. Instead, we will take the approach of finding the probability of no one having a matching birthday. Then the probability of at least 2 people having a matching birthday is 1 minus the probability that no one has a matching birthday. This is known as a complementary probability. A simpler example is to think about rolling a single die. The probability of rolling a 6 is equivalent to 1 minus the probability of not rolling a 6. We first need to think about all the different ways we could get 18 birthdays. This is going to be our denominator in the probability calculation. First let’s just look at 2 people. The first person could have 365 different days for their birthday. The second person could also have 365 different birthdays. So for each birthday of the first person there could be 365 birthdays for the second. Thus for 2 people there are \\(365^2\\) possible sets of birthdays. This is an example of the multiplication rule. For 18 people there are \\(365^{18}\\) sets of birthdays. That is a large number. Again, this will be our denominator in calculating the probability. The numerator is the number of sets of birthdays with no matches. Again, let’s consider 2 people. The first person can have a birthday on any day of the year, so 365 possibilities. Since we don’t want a match, the second person can only have 364 possibilities for a birthday. Thus we have \\(365 \\times 364\\) possibilities for two people to have different birthdays. Exercise: What is the number of possibilities for 18 people so that no one has the same birthday. The answer for 18 people is \\(365 \\times 364 \\times 363 ... \\times 349 \\times 348\\). This looks like a truncated factorial. Remember a factorial, written as \\(n!\\) with an explanation point, is the product of successive positive integers. As an example \\(3!\\) is \\(3 \\times 2 \\times 1\\) or 6. We could write the multiplication for the numerator as \\[\\frac{365!}{(365-n)!}\\] As we will learn, the multiplication rule for the numerator is known as a permutation. We are ready to put it all together. For 18 people, the probability of 2 or more people with the same birthday is 1 minus the probability that no one has the same birthday, which is \\[1 - \\frac{\\frac{365!}{(365-18)!}}{365^{18}}\\] or \\[1 - \\frac{\\frac{365!}{347!}}{365^{18}}\\] In R there is a function called factorial() but factorials get large fast and we will overflow the memory. Try factorial(365) in R to see what happens. factorial(365) [1] Inf It is returning infinity because the number is too large for the buffer. As is often the case we will have when using a computational method, we must be clever about our approach. Instead of using factorials we can make use of Rs ability to work on vectors. If we provide R with a vector of values, the prod() will perform a product of all the elements. 365*364 [1] 132860 prod(365:364) [1] 132860 1 - prod(365:348)/(365^18) [1] 0.3469114 1.4.5 General solution We now have the mathematics to understand the problem. We can easily generalize this to any number of people. To do this, we have to write a function in R. As with everything in R, we save a function as an object. The general format for creating a function is my_function &lt;- function(parameters){ code for function } For this problem we will call the function birthday_prob(). The only parameter we need is the number of people in the room, n. Let’s write this function. birthday_prob &lt;- function(n = 20){ 1 - prod(365:(365 - (n - 1)))/(365^n) } Notice we assigned the function to the name birthday_prob, we told R to expect one argument to the function, which we are calling n, and then we provide R with the code to find the probability. We set a default value for n in case one is not provided to prevent an error when the function is run. We will learn more about writing functions throughout this book and in the follow-on USAFA course, Math 378: Applied Statistical Modeling. Test the code with a know answer. birthday_prob(18) [1] 0.3469114 Now we can determine the probability for any size room. You may have heard that it only takes about 23 people in a room to have a 50% probability of at least 2 people matching birthdays. birthday_prob(23) [1] 0.5072972 Let’s create a plot of the probability versus number of people in the room. To do this, we need to apply the function to a vector of values. The function sapply() will work or we can also use Vectorize() to alter our existing function. We choose the latter option. First notice what happens if we input a vector into our function. birthday_prob(1:20) Warning in 365:(365 - (n - 1)): numerical expression has 20 elements: only the first used [1] 0.0000000 0.9972603 0.9999925 1.0000000 1.0000000 1.0000000 1.0000000 [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 It only uses the first value. There are several ways to solve this problem. We can use the map() function in the purrr package. This idea of mapping a function to a vector is important in data science. It is used in scenarios where there is a lot of data. In this case the idea of map-reduce is used to make the analysis amenable to parallel computing. map_dbl(1:20, birthday_prob) [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484 [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789 [13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418 [19] 0.379118526 0.411438384 We could also just vectorize the function. birthday_prob &lt;- Vectorize(birthday_prob) Now notice what happens. birthday_prob(1:20) [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484 [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789 [13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418 [19] 0.379118526 0.411438384 We are good to go. Let’s create our line plot, Figure 1.2. gf_line(birthday_prob(1:100) ~ seq(1, 100), xlab = &quot;Number of People&quot;, ylab = &quot;Probability of Match&quot;, title = &quot;Probability of at least 2 people with matching birthdays&quot;) %&gt;% gf_theme(theme_bw()) FIGURE 1.2: The probability of at least 2 people having mathcing birthdays Is this what you expected the curve to look like? We, the authors, did not expect this. It has a sigmodial shape with a large increase in the middle range and flatten in the tails. 1.4.6 Data science approach The final approach we will take is one based on data, a data science approach. In the mosaicData package is a data set called Births that contains the number of births in the US from 1969 to 1988. This data will allow us to estimate the number of births on any day of the year. This allows us to eliminate the reliance on the assumption that each day is equally likely. Let’s first inspect() the data object. inspect(Births) categorical variables: name class levels n missing 1 wday ordered 7 7305 0 distribution 1 Wed (14.3%), Thu (14.3%), Fri (14.3%) ... Date variables: name class first last min_diff max_diff n missing 1 date Date 1969-01-01 1988-12-31 1 days 1 days 7305 0 quantitative variables: name class min Q1 median Q3 max mean sd 1 births integer 6675 8792 9622 10510 12851 9648.940178 1127.315229 2 year integer 1969 1974 1979 1984 1988 1978.501027 5.766735 3 month integer 1 4 7 10 12 6.522930 3.448939 4 day_of_year integer 1 93 184 275 366 183.753593 105.621885 5 day_of_month integer 1 8 16 23 31 15.729637 8.800694 6 day_of_week integer 1 2 4 6 7 4.000274 1.999795 n missing 1 7305 0 2 7305 0 3 7305 0 4 7305 0 5 7305 0 6 7305 0 It could be argued that we could randomly pick one year and use it. Let’s see what happens if we just used 1969. Figure 1.3 is a scatter plot of the number of births in 1969 for each day of the year. Births %&gt;% filter(year == 1969) %&gt;% gf_point(births ~ day_of_year) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;Day of the Year&quot;, y = &quot;Number of Births&quot;) FIGURE 1.3: The number of births for each day of the year in 1969 Exercise: What patterns do you see in Figure 1.3? What might explain them? There are definitely bands appearing in the data which could be the day of the week; there are less birthdays on the weekend. There is also seasonality with more birthdays in the summer and fall. There is also probably an impact from holidays. Quickly, let’s look at the impact of day of the week by using color for day of the week. Figure 1.4 makes it clear that the weekends have less number of births as compared to the work week. Births %&gt;% filter(year == 1969) %&gt;% gf_point(births ~ day_of_year, color = ~factor(day_of_week)) %&gt;% gf_labs(x = &quot;Day of the Year&quot;, col = &quot;Day of Week&quot;) %&gt;% gf_theme(theme_bw()) FIGURE 1.4: The number of births for each day of the year in 1969 broken down by day of the week By only using one year, this data might give poor results since holidays will fall on certain days of the week and the weekends will also be impacted. Note that we also still have the problem of leap years. Births %&gt;% group_by(year) %&gt;% summarize(n = n()) # A tibble: 20 × 2 year n &lt;int&gt; &lt;int&gt; 1 1969 365 2 1970 365 3 1971 365 4 1972 366 5 1973 365 6 1974 365 7 1975 365 8 1976 366 9 1977 365 10 1978 365 11 1979 365 12 1980 366 13 1981 365 14 1982 365 15 1983 365 16 1984 366 17 1985 365 18 1986 365 19 1987 365 20 1988 366 The years 1972, 1976, 1980, 1984, and 1988 are all leap years. At this point, to make the analysis easier, we will drop those years. Births %&gt;% filter(!(year %in% c(1972, 1976, 1980, 1984, 1988))) %&gt;% group_by(year) %&gt;% summarize(n = n()) # A tibble: 15 × 2 year n &lt;int&gt; &lt;int&gt; 1 1969 365 2 1970 365 3 1971 365 4 1973 365 5 1974 365 6 1975 365 7 1977 365 8 1978 365 9 1979 365 10 1981 365 11 1982 365 12 1983 365 13 1985 365 14 1986 365 15 1987 365 Notice in filter() we used the %in% argument. This is a logical argument checking if year is one of the values. The ! at the front negates this in a sense requiring year not to be one of those values.` We are almost ready to simulate. We need to get the count of births on each day of the year for the non-leap years. birth_data &lt;- Births %&gt;% filter(!(year %in% c(1972, 1976, 1980, 1984, 1988))) %&gt;% group_by(day_of_year) %&gt;% summarize(n = sum(births)) head(birth_data) # A tibble: 6 × 2 day_of_year n &lt;int&gt; &lt;int&gt; 1 1 120635 2 2 129042 3 3 135901 4 4 136298 5 5 137319 6 6 140044 Let’s look at a plot of the number of births versus day of the year. We combined years in Figure 1.5. birth_data %&gt;% gf_point(n ~ day_of_year, xlab = &quot;Day of the year&quot;, ylab = &quot;Number of births&quot;) %&gt;% gf_theme(theme_bw()) FIGURE 1.5: Number of births by day of the year for all years. This curve has the seasonal cycling we would expect. The smaller scale cycling is unexpected. Maybe because we are dropping the leap years, we are getting some days appearing in our time interval more frequently on weekends. We leave it to you to investigate this phenomenon. We use these counts as weights in a sampling process. Days with more births will have a higher probability of being selected. Days such as Christmas and Christmas Eve have a lower probability of being selected. Let’s save the weights in an object to use in the sample() function. birth_data_weights &lt;- birth_data %&gt;% select(n) %&gt;% pull() The pull() function pulls the vectors of values out of the data frame format into a vector format which the sample() needs. Now let’s simulate the problem. The probability of a match should change slightly, maybe go down slightly?, but not much since most of the days have about the same probability or number of occurrences. set.seed(20) (do(1000)*length(unique(sample(days, size = 18,replace = TRUE, prob = birth_data_weights)))) %&gt;% mutate(match = if_else(length == 18, 0, 1)) %&gt;% summarize(prob = mean(match)) prob 1 0.352 We could not solve this problem of varying frequency of birth days using mathematics, at least as far as we know. Cool stuff, let’s get to learning more about probability models in the next chapters. 1.5 Homework Problems Exactly 2 people with the same birthday - Simulation. Complete a similar analysis for case where exactly 2 people in a room of 23 people have the same birthday. In this exercise you will use a computational simulation. Create a new R Markdown file and create a report. Yes, we know you could use this file but we want you to practice generating your own report. Simulate having 23 people in the class with each day of the year equally likely. Find the cases where exactly 2 people have the same birthday, you will have to alter the code from the Notes more than changing 18 to 23. Plot the frequency of occurrences as a bar chart. Estimate the probability of exactly two people having the same birthday. Exactly 2 people with the same birthday - Mathematical. Repeat problem 1 but do it mathematically. As a big hint, you will need to use the choose() function. The idea is that with 23 people we need to choose 2 of them to match. We thus need to multiply, the multiplication rule again, by choose(23,2). If you are having trouble, work with a total of 3 people in the room first. Find a formula to determine the exact probability of exactly 2 people in a room of 23 having the same birthday. Generalize your solution to any number n people in the room and create a function. Vectorize the function. Plot the probability of exactly 2 people having the same birthday versus number of people in the room. Comment on the shape of the curve and explain it. Solutions Manual The answer is around 34.7%, how close were you?↩︎ Another question may be What does it mean at least two people have matching birthdays?↩︎ It is possible that 3 people all have the same birthday or two sets of 2 people have the same birthday but different from the other pair.↩︎ "],["2-PROBRULES.html", "Chapter 2 Probability Rules 2.1 Objectives 2.2 Probability vs Statistics 2.3 Basic probability terms 2.4 Probability 2.5 Counting rules 2.6 Homework Problems Solutions Manual", " Chapter 2 Probability Rules 2.1 Objectives Define and use properly in context all new terminology related to probability, including: sample space, outcome, event, subset, intersection, union, complement, probability, mutually exclusive, exhaustive, independent, multiplication rule, permutation, combination. Apply basic probability and counting rules to find probabilities. Describe the basic axioms of probability. Use R to calculate and simulate probabilities of events. 2.2 Probability vs Statistics As a review, remember this book is divided into four general blocks: data collection/summary, probability models, inference and statistical modeling/prediction. This second block, probability, is the study of stochastic (random) processes and their properties. Specifically, we will explore random experiments. As its name suggests, a random experiment is an experiment whose outcome is not predictable with exact certainty. In the statistical models we develop in the last two blocks of this book, we will use other variables to explain the variance of the outcome of interest. Any remaining variance is modeled with probability models. Even though an outcome is determined by chance, this does not mean that we know nothing about the random experiment. Our favorite simple example is that of a coin flip. If we flip a coin, the possible outcomes are heads and tails. We don’t know for sure what outcome will occur, but this doesn’t mean we don’t know anything about the experiment. If we assume the coin is fair, we know that each outcome is equally likely. Also, we know that if we flip the coin 100 times (independently), we are likely, the highest frequency event, to see around 50 heads, and very unlikely to see 10 heads or fewer. It is important to distinguish probability from inference and modeling. In probability, we consider a known random experiment, including knowing the parameters, and answer questions about what we expect to see from this random experiment. In statistics (inference and modeling), we consider data (the results of a mysterious random experiment) and infer about the underlying process. For example, suppose we have a coin and we are unsure whether this coin is fair or unfair, the parameter is unknown. We flipped it 20 times and it landed on heads 14 times. Inferential statistics will help us answer questions about the underlying process (could this coin be unfair?). FIGURE 2.1: A graphical representation of probability and statistics. In probability, we describe what we expect to happen if we know that underlying process; in statistics, we don’t know the underlying process, and must infer based on representative samples. This block (10 chapters or so) is devoted to the study of random experiments. First, we will explore simple experiments, counting rule problems, and conditional probability. Next, we will introduce the concept of a random variable and the properties of random variables. Following this, we will cover common distributions of discrete and continuous random variables. We will end the block on multivariate probability (joint distributions and covariance). 2.3 Basic probability terms We will start our work with some definitions and examples. 2.3.1 Sample space Suppose we have a random experiment. The sample space of this experiment, \\(S\\), is the set of all possible results of that experiment. For example, in the case of a coin flip, we could write \\(S=\\{H,T\\}\\). Each element of the sample space is considered an outcome. An event is a set of outcomes, it is a subset of the sample space. Example: Let’s let R flip a coin for us and record the number of heads and tails. We will have R flip the coin twice. What is the sample space, what is an example of an outcome, and what is an example of an event. We will load the mosaic package as it has a function rflip() that will simulate flipping a coin. library(mosaic) set.seed(18) rflip(2) Flipping 2 coins [ Prob(Heads) = 0.5 ] ... H H Number of Heads: 2 [Proportion Heads: 1] The sample space is \\(S=\\{HH, TH, HT, TT\\}\\), an example of an outcome is \\(HH\\) which we see in the output from R, and finally an example of an event is the number of heads, which in this case takes on the values 0, 1, and 2. Another example of an event is “At least one heads”. In this case the event would be \\(\\{HH,TH, HT\\}\\). Also notice that \\(TH\\) is different from \\(HT\\) as an outcome; this is because those are different outcomes from flipping a coin twice. Example of Event: Suppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[ S=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}. \\] Each vehicle represents a possible outcome of the experiment. Let \\(A\\) be the event that a blue vehicle is selected. This event contains the outcomes blue sedan and blue SUV. 2.3.2 Union and intersection Suppose we have two events \\(A\\) and \\(B\\). \\(A\\) is considered a subset of \\(B\\) if all of the outcomes of \\(A\\) are also contained in \\(B\\). This is denoted as \\(A \\subset B\\). The intersection of \\(A\\) and \\(B\\) is all of the outcomes contained in both \\(A\\) and \\(B\\). This is denoted as \\(A \\cap B\\). The union of \\(A\\) and \\(B\\) is all of the outcomes contained in either \\(A\\) or \\(B\\), or both. This is denoted as \\(A \\cup B\\). The complement of \\(A\\) is all of the outcomes not contained in \\(A\\). This is denoted as \\(A^C\\) or \\(A&#39;\\). Note: Here we are treating events as sets and the above definitions are basic set operations. It is sometimes helpful when reading probability notation to think of Union as an or and Intersection as an and. Example: Consider our rental car example above. Let \\(A\\) be the event that a blue vehicle is selected, let \\(B\\) be the event that a black vehicle is selected, and let \\(C\\) be the event that an SUV is selected. First, let’s list all of the outcomes of each event. \\(A = \\{\\mbox{blue sedan},\\mbox{blue SUV}\\}\\), \\(B=\\{\\mbox{black SUV}\\}\\), and \\(C= \\{\\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\). Since all outcomes in \\(B\\) are contained in \\(C\\), we know that \\(B\\) is a subset of \\(C\\), or \\(B\\subset C\\). Also, since \\(A\\) and \\(B\\) have no outcomes in common, \\(A \\cap B = \\emptyset\\). Note that \\(\\emptyset = \\{ \\}\\) is the empty set and contains no elements. Further, \\(A \\cup C = \\{\\mbox{blue sedan}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\). 2.4 Probability Probability is a number assigned to an event or outcome that describes how likely it is to occur. A probability model assigns a probability to each element of the sample space. What makes a probability model is not just the values assigned to each element but the idea this model contains all the information about the outcomes and there are no other explanatory variables involved. A probability model can be thought of as a function that maps outcomes, or events, to a real number in the interval \\([0,1]\\). There are some basic axioms of probability you should know, although this list is not complete. Let \\(S\\) be the sample space of a random experiment and let \\(A\\) be an event where \\(A\\subset S\\). \\(\\mbox{P}(A) \\geq 0\\). \\(\\mbox{P}(S) = 1\\). These two axioms essentially say that probability must be positive, and the probability of all outcomes must sum to 1. 2.4.1 Probability properties Let \\(A\\) and \\(B\\) be events in a random experiment. Most of these can be proven fairly easily. \\(\\mbox{P}(\\emptyset) = 0\\) \\(\\mbox{P}(A&#39;) = 1 - \\mbox{P}(A)\\) We used this in the case study. If \\(A\\subset B\\), then \\(\\mbox{P}(A)\\leq \\mbox{P}(B)\\). \\(\\mbox{P}(A\\cup B) = \\mbox{P}(A)+\\mbox{P}(B)-\\mbox{P}(A\\cap B)\\). This property can be generalized to more than two events. The intersection is subtracted because outcomes in both events \\(A\\) and \\(B\\) get counted twice in the first sum. Law of Total Probability: Let \\(B_1, B_2,...,B_n\\) be mutually exclusive, this means disjoint or no outcomes in common, and exhaustive, this means the union of all the events labeled with a \\(B\\) is the sample space. Then \\[ \\mbox{P}(A)=\\mbox{P}(A\\cap B_1)+\\mbox{P}(A\\cap B_2)+...+\\mbox{P}(A\\cap B_n) \\] A specific application of this law appears in Bayes’ Rule (more to follow). It says that \\(\\mbox{P}(A)=\\mbox{P}(A \\cap B)+\\mbox{P}(A \\cap B&#39;)\\). Essentially, it points out that \\(A\\) can be partitioned into two parts: 1) everything in \\(A\\) and \\(B\\) and 2) everything in \\(A\\) and not in \\(B\\). Example: Consider rolling a six sided die. Let event \\(A\\) be the number showing is less than 5. Let event \\(B\\) be the number is even. Then \\[\\mbox{P}(A)=\\mbox{P}(A \\cap B) + \\mbox{P}(A \\cap B&#39;)\\] \\[ \\mbox{P}(&lt; 5) = \\mbox{P}(&lt;5 \\cap Even) + \\mbox{P}(&lt;5 \\cap Odd) \\] DeMorgan’s Laws: \\[ \\mbox{P}((A \\cup B)&#39;) = \\mbox{P}(A&#39; \\cap B&#39;) \\] \\[ \\mbox{P}((A \\cap B)&#39;) = \\mbox{P}(A&#39; \\cup B&#39;) \\] 2.4.2 Equally likely scenarios In some random experiments, outcomes can be defined such that each individual outcome is equally likely. In this case, probability becomes a counting problem. Let \\(A\\) be an event in an experiment where each outcome is equally likely. \\[ \\mbox{P}(A)=\\frac{\\mbox{\\# of outcomes in A}}{\\mbox{\\# of outcomes in S}} \\] Example: Suppose a family has three children, with each child being either a boy (B) or girl (G). Assume that the likelihood of boys and girls are equal and independent, this is the idea that the probability of the gender of the second child does not change based on the gender of the first child. The sample space can be written as: \\[ S=\\{\\mbox{BBB},\\mbox{BBG},\\mbox{BGB},\\mbox{BGG},\\mbox{GBB},\\mbox{GBG},\\mbox{GGB},\\mbox{GGG}\\} \\] What is the probability that the family has exactly 2 girls? This only happens in three ways: BGG, GBG, and GGB. Thus, the probability of exactly 2 girls is 3/8 or 0.375. 2.4.3 Using R (Equally likely scenarios) The previous example above is an example of an “Equally Likely” scenario, where the sample space of a random experiment contains a list of outcomes that are equally likely. In these cases, we can sometimes use R to list out the possible outcomes and count them to determine probability. We can also use R to simulate. Example: Use R to simulate the family of three children where each child has the same probability of being a boy or a girl. Instead of writing our own function, we can use rflip() in the mosaic package. We will let \\(H\\) stand for girl. First simulate one family. set.seed(73) rflip(3) Flipping 3 coins [ Prob(Heads) = 0.5 ] ... T T H Number of Heads: 1 [Proportion Heads: 0.333333333333333] In this case we got 1 girl. Next we will use the do() function to repeat this simulation. results &lt;- do(10000)*rflip(3) head(results) n heads tails prop 1 3 1 2 0.3333333 2 3 3 0 1.0000000 3 3 3 0 1.0000000 4 3 3 0 1.0000000 5 3 1 2 0.3333333 6 3 1 2 0.3333333 Next we can visualize the distribution of the number of girls, heads, in Figure 2.2. results %&gt;% gf_bar(~heads) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;NUmber of girls&quot;, y = &quot;Count&quot;) FIGURE 2.2: Number of girls in a family of size 3. Finally we can estimate the probability of exactly 2 girls. We need the tidyverse library. library(tidyverse) results %&gt;% filter(heads == 2) %&gt;% summarize(prob = n()/10000) prob 1 0.3782 Or slightly different code. results %&gt;% count(heads) %&gt;% mutate(prop = n/sum(n)) heads n prop 1 0 1241 0.1241 2 1 3786 0.3786 3 2 3782 0.3782 4 3 1191 0.1191 Not a bad estimate of the exact probability. Let’s now use an example of cards to simulate some probabilities as well as learning more about counting. The file Cards.csv contains the data for cards from a 52 card deck. Let’s read it in and summarize. Cards &lt;- read_csv(&quot;data/Cards.csv&quot;) inspect(Cards) categorical variables: name class levels n missing 1 rank character 13 52 0 2 suit character 4 52 0 distribution 1 10 (7.7%), 2 (7.7%), 3 (7.7%) ... 2 Club (25%), Diamond (25%) ... quantitative variables: name class min Q1 median Q3 max 1 probs numeric 0.01923077 0.01923077 0.01923077 0.01923077 0.01923077 mean sd n missing 1 0.01923077 0 52 0 head(Cards) # A tibble: 6 × 3 rank suit probs &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 2 Club 0.0192 2 3 Club 0.0192 3 4 Club 0.0192 4 5 Club 0.0192 5 6 Club 0.0192 6 7 Club 0.0192 We can see 4 suits, and 13 ranks, the value on the face of the card. Example: Suppose we draw one card out of a standard deck. Let \\(A\\) be the event that we draw a Club. Let \\(B\\) be the event that we draw a 10 or a face card (Jack, Queen, King or Ace). We can use R to define these events and find probabilities. Let’s find all the Clubs. Cards %&gt;% filter(suit == &quot;Club&quot;) %&gt;% select(rank, suit) # A tibble: 13 × 2 rank suit &lt;chr&gt; &lt;chr&gt; 1 2 Club 2 3 Club 3 4 Club 4 5 Club 5 6 Club 6 7 Club 7 8 Club 8 9 Club 9 10 Club 10 J Club 11 Q Club 12 K Club 13 A Club So just by counting, we find the probability of drawing a Club is \\(\\frac{13}{52}\\) or 0.25. We can do this by simulation, this is over kill but gets the idea of simulation across. Remember, ask what do we want R to do and what does R need to do this? results &lt;- do(10000)*sample(Cards, 1) head(results) # A tibble: 6 × 6 rank suit probs orig.id .row .index &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 9 Spade 0.0192 47 1 1 2 5 Club 0.0192 4 1 2 3 5 Spade 0.0192 43 1 3 4 7 Heart 0.0192 32 1 4 5 4 Club 0.0192 3 1 5 6 A Spade 0.0192 52 1 6 results %&gt;% filter(suit == &quot;Club&quot;) %&gt;% summarize(prob = n()/10000) # A tibble: 1 × 1 prob &lt;dbl&gt; 1 0.243 results %&gt;% count(suit) %&gt;% mutate(prob = n/sum(n)) # A tibble: 4 × 3 suit n prob &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 Club 2432 0.243 2 Diamond 2558 0.256 3 Heart 2417 0.242 4 Spade 2593 0.259 Now let’s count the number of outcomes in \\(B\\). Cards %&gt;% filter(rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;)) %&gt;% select(rank, suit) # A tibble: 20 × 2 rank suit &lt;chr&gt; &lt;chr&gt; 1 10 Club 2 J Club 3 Q Club 4 K Club 5 A Club 6 10 Diamond 7 J Diamond 8 Q Diamond 9 K Diamond 10 A Diamond 11 10 Heart 12 J Heart 13 Q Heart 14 K Heart 15 A Heart 16 10 Spade 17 J Spade 18 Q Spade 19 K Spade 20 A Spade So just by counting, we find the probability of drawing a 10 or greater is \\(\\frac{20}{52}\\) or 0.3846154. Exercise: Using simulation to estimate the probability of 10 or higher. results &lt;- do(10000)*sample(Cards, 1) head(results) # A tibble: 6 × 6 rank suit probs orig.id .row .index &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 10 Heart 0.0192 35 1 1 2 6 Heart 0.0192 31 1 2 3 8 Spade 0.0192 46 1 3 4 J Heart 0.0192 36 1 4 5 Q Spade 0.0192 50 1 5 6 10 Club 0.0192 9 1 6 results %&gt;% filter(rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;)) %&gt;% summarize(prob = n()/10000) # A tibble: 1 × 1 prob &lt;dbl&gt; 1 0.389 Notice that this code is not robust to change the number of simulations. If we change from 10000, then we have to change the denominator in the summarize() function. We can change this by using mutate() instead of filter(). results %&gt;% mutate(face = rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;))%&gt;% summarize(prob = mean(face)) # A tibble: 1 × 1 prob &lt;dbl&gt; 1 0.389 Notice that in the mutate() function, we are creating a new logical variable called face. This variable takes on the values of TRUE and FALSE. In the next line we use a summarize() command with the function mean(). In R a function that requires numeric input takes a logical variable and converts the TRUE into 1 and the FALSE into 0. Thus the mean() will find the proportion of TRUE values and that is why we report it as a probability. Next, let’s find a card that is 10 or greater and a club. Cards %&gt;% filter(rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;), suit == &quot;Club&quot;) %&gt;% select(rank, suit) # A tibble: 5 × 2 rank suit &lt;chr&gt; &lt;chr&gt; 1 10 Club 2 J Club 3 Q Club 4 K Club 5 A Club We find the probability of drawing a 10 or greater club is \\(\\frac{5}{52}\\) or 0.0961538. Exercise: Simulate drawing one card and estimate the probability of a club that is 10 or greater. results %&gt;% mutate(face = (rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;)) &amp; (suit == &quot;Club&quot;))%&gt;% summarize(prob = mean(face)) # A tibble: 1 × 1 prob &lt;dbl&gt; 1 0.0963 2.4.4 Note We have been using R to count the number of outcomes in an event. This helped us to determine probabilities. We limited the problems to simple ones. In our cards example, it would be more interesting for us to explore more complex events such as drawing 5 cards from a standard deck. Each draw of 5 cards is equally likely, so in order to find the probability of a flush (5 cards of the same suit), we could simply list all the possible flushes and compare that to the sample space. Because of the large number of possible outcomes, this becomes difficult. Thus we need to explore counting rules in more detail to help us solve more complex problems. In this book, we will limit our discussion to three basic cases. You should know that there are entire courses on discrete math and counting rules, so we will still be limited in our methods and the type of problems we can solve in this book. 2.5 Counting rules There are three types of counting problems we will consider. In each case, the multiplication rule is being used and all that changes is whether an element is allowed to be reused, replacement, and whether the order of selection matters. This latter question is difficult. Each case will be demonstrated with an example. 2.5.1 Multiplication rule 1: Order matters, sample with replacement The multiplication rule is at the center of each of the three methods. In this first case we are using the idea that order matters and items can be reused. Let’s use an example to help. Example: A license plate consists of three numeric digits (0-9) followed by three single letters (A-Z). How many possible license plates exist? We can divide this problem into two sections. In the numeric section, we are selecting 3 objects from 10, with replacement. This means that a number can be used more than once. Order clearly matters because a license plate starting with “432” is distinct from a license plate starting with “234”. There are \\(10^3 = 1000\\) ways to select the first three digits; 10 for the first, 10 for the second, and 10 for the third. Why do you multiply and not add?4 In the alphabet section, we are selecting 3 objects from 26, where order matters. Thus, there are \\(26^3=17576\\) ways to select the last three letters of the plate. Combined, there are \\(10^3 \\times 26^3 = 17576000\\) ways to select license plates. Visually, \\[ \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} = 17,576,000 \\] Next we are going to use this new counting method to find a probability. Exercise: What is the probability a license plate starts with the number “8” or “0” and ends with the letter “B”? In order to find this probability, we simply need to determine the number of ways to select a license plate starting with “8” or “0” and ending with the letter “B”. We can visually represent this event: \\[ \\underbrace{\\underline{\\quad 2 \\quad }}_\\text{8 or 0} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 1 \\quad }}_\\text{B} = 135,200 \\] Dividing this number by the total number of possible license plates yields the probability of this event occurring. denom &lt;- 10*10*10*26*26*26 num &lt;- 2*10*10*26*26*1 num/denom [1] 0.007692308 The probability of obtaining a license plate starting with “8” or “0” and ending with “B” is 0.0077. Simulating this would be difficult because we would need special functions to check the first number and last letter. This gets into text mining an important subject in data science but unfortunately we don’t have much time in this book for the topic. 2.5.2 Multiplication rule 2 (Permutation): Order Matters, Sampling Without Replacement Consider a random experiment where we sample from a group of size \\(n\\), without replacement, and the outcome of the experiment depends on the order of the outcomes. The number of ways to select \\(k\\) objects is given by \\(n(n-1)(n-2)...(n-k+1)\\). This is known as a permutation and is sometimes written as \\[ {}_nP_{k} = \\frac{n!}{(n-k)!} \\] Recall that \\(n!\\) is read as \\(n\\) factorial and represents the number of ways to arrange \\(n\\) objects. Example: Twenty-five friends participate in a Halloween costume party. Three prizes are given during the party: most creative costume, scariest costume, and funniest costume. No one can win more than one prize. How many possible ways can the prizes by distributed? There are \\(k=3\\) prizes to be assigned to \\(n=25\\) people. Once someone is selected for a prize, they are removed from the pool of eligibles. In other words, we are sampling without replacement. Also, order matters. For example, if Tom, Mike, and Jane, win most creative, scariest and funniest costume, respectively, this is a different outcome than if Mike won creative, Jane won scariest and Tom won funniest. Thus, the number of ways the prizes can be distributed is given by \\({}_{25}P_3 = \\frac{25!}{22!} = 13,800\\). A more visually pleasing way to express this would be: \\[ \\underbrace{\\underline{\\quad 25 \\quad }}_\\text{most creative} \\times \\underbrace{\\underline{\\quad 24 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{funniest} = 13,800 \\] Notice that it is sometime difficult to determine if order matters or not in a problem, but in this example the name of the prize was a hint that indeed order matters. Let’s use the idea of a permutation to calculate a probability. Exercise: Assume that all 25 participants are equally likely to win any one of the three prizes. What is the probability that Tom doesn’t win any of them? Just like in the previous probability calculation, we simply need to count the number of ways Tom doesn’t win any prize. In other words, we need to count the number of ways that prizes are distributed without Tom. So, remove Tom from the group of 25 eligible participants. The number of ways Tom doesn’t get a prize is \\({}_{24}P_3 = \\frac{24!}{21!}=12,144\\). Again visually: \\[ \\underbrace{\\underline{\\quad 24 \\quad }}_\\text{most creative} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 22 \\quad }}_\\text{funniest} = 12,144 \\] The probability Tom doesn’t get a prize is simply the second number divided by the first: denom &lt;- factorial(25)/factorial(25 - 3) # Or, denom&lt;-25*24*23 num &lt;- 24*23*22 num/denom [1] 0.88 2.5.3 Multiplication rule 3 (Combination): Order Does Not Matter, Sampling Without Replacement Consider a random experiment where we sample from a group of size \\(n\\), without replacement, and the outcome of the experiment does not depend on the order of the outcomes. The number of ways to select \\(k\\) objects is given by \\(\\frac{n!} {(n-k)!k!}\\). This is known as a combination and is written as: \\[ \\binom{n}{k} = \\frac{n!}{(n-k)!k!} \\] This is read as “\\(n\\) choose \\(k\\)”. Take a moment to compare combinations to permutations, discussed in Rule 2. The difference between these two rules is that in a combination, order no longer matters. A combination is equivalent to a permutation divided by \\(k!\\), the number of ways to arrange the \\(k\\) objects selected. Example: Suppose we draw 5 cards out of a standard deck (52 cards, no jokers). How many possible 5 card hands are there? In this example, order does not matter. I don’t care if I receive 3 jacks then 2 queens or 2 queens then 3 jacks. Either way, it’s the same collection of 5 cards. Also, we are drawing without replacement. Once a card is selected, it cannot be selected again. Thus, the number of ways to select 5 cards is given by: \\[ \\binom{52}{5} = \\frac{52!}{(52-5)!5!} = 2,598,960 \\] Example: When drawing 5 cards, what is the probability of drawing a “flush” (5 cards of the same suit)? Let’s determine how many ways to draw a flush. There are four suits (clubs, hearts, diamonds and spades). Each suit has 13 cards. We would like to pick 5 of those 13 cards and 0 of the remaining 39. Let’s consider just one of those suits (clubs): \\[ \\mbox{P}(\\mbox{5 clubs})=\\frac{\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}} \\] The second part of the numerator (\\(\\binom{39}{0}\\)) isn’t necessary, since it simply represents the number of ways to select 0 objects from a group (1 way), but it helps clearly lay out the events. This brings up the point of what \\(0!\\) equals. By definition it is 1. This allows us to use \\(0!\\) in our work. Now, we expand this to all four suits by multiplying by 4, or \\(\\binom{4}{1}\\) since we are selecting 1 suit out of the 4: \\[ \\mbox{P}(\\mbox{flush})=\\frac{\\binom{4}{1}\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}} \\] num &lt;- 4*choose(13, 5)*1 denom &lt;- choose(52, 5) num/denom [1] 0.001980792 There is a probability of 0.0020 of drawing a flush in a draw of 5 cards from a standard deck of cards. Exercise: When drawing 5 cards, what is the probability of drawing a “full house” (3 cards of the same rank and the other 2 of the same rank)? This problem uses several ideas from this chapter. We need to pick the rank of the three of a kind. Then pick 3 cards from the 4 possible. Next we pick the rank of the pair from the remaining 12 ranks. Finally pick 2 cards of that rank from the 4 possible. \\[ \\mbox{P}(\\mbox{full house})=\\frac{\\binom{13}{1}\\binom{4}{3}\\binom{12}{1}\\binom{4}{2}}{\\binom{52}{5}} \\] num &lt;- choose(13, 1)*choose(4, 3)*choose(12, 1)*choose(4, 2) denom &lt;- choose(52, 5) num/denom [1] 0.001440576 Why not use \\(\\binom{13}{2}\\) instead of \\(\\binom{13}{1}\\binom{12}{1}\\)?5 We have just determined that a full house has a lower probability of occurring than a flush. This is why in gambling, a flush is valued less than a full house. 2.6 Homework Problems Let \\(A\\), \\(B\\) and \\(C\\) be events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), and \\(\\mbox{P}(C)=0.4\\). Also, we know that \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Find the following: \\(\\mbox{P}(A\\cup B)\\) \\(\\mbox{P}(A\\cup B \\cup C)\\) \\(\\mbox{P}(B&#39;\\cap C&#39;)\\) \\(\\mbox{P}(A\\cup (B\\cap C))\\) \\(\\mbox{P}((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)&#39;)\\) Consider the example of the family in the reading. What is the probability that the family has at least one boy? The Birthday Problem Revisited. Suppose there are \\(n=20\\) students in a classroom. My birthday, the instructor, is April 3rd. What is the probability that at least one student shares my birthday? Assume only 365 days in a year and assume that all birthdays are equally likely. In R, find the probability that at least one other person shares my birthday for each value of \\(n\\) from 1 to 300. Plot these probabilities with \\(n\\) on the \\(x\\)-axis and probability on the \\(y\\)-axis. At what value of \\(n\\) would the probability be at least 50%? Thinking of the cards again. Answer the following questions: Define two events that are mutually exclusive. Define two events that are independent. Define an event and its complement. Consider the license plate example from the reading. What is the probability that a license plate contains exactly one “B”? What is the probability that a license plate contains at least one “B”? Consider the party example in the reading. Suppose 8 people showed up to the party dressed as zombies. What is the probability that all three awards are won by people dressed as zombies? What is the probability that zombies win “most creative” and “funniest” but not “scariest”? Consider the cards example from the reading. How many ways can we obtain a “two pairs” (2 of one number, 2 of another, and the final different)? What is the probability of drawing a “four of a kind” (four cards of the same value)? Advanced Question: Consider rolling 5 dice. What is the probability of a pour resulting in a full house? Solutions Manual Multiplication is repeated adding so in a sense we are adding. However in a more serious tone, for this problem for every first number there are 10 possibilities for the second number and for every second number there are 10 possibilities for the third numbers. This is multiplication.↩︎ Because this implies the order selection of the ranks does not matter. In other words, this assumes that for example 3 Kings and 2 fours is the same full house as 3 fours and 2 Kings. This is not true so we break the rank selection about essentially making it a permutation.↩︎ "],["3-CONDPROB.html", "Chapter 3 Conditional Probability 3.1 Objectives 3.2 Conditional Probability 3.3 Independence 3.4 Bayes’ Rule 3.5 Homework Problems Solutions Manual", " Chapter 3 Conditional Probability 3.1 Objectives Define conditional probability and distinguish it from joint probability. Find a conditional probability using its definition. Using conditional probability, determine whether two events are independent. Apply Bayes’ Rule mathematically and via simulation. 3.2 Conditional Probability So far, we’ve covered the basic axioms of probability, the properties of events (set theory) and counting rules. Another important concept, perhaps one of the most important, is conditional probability. Often, we know a certain event or sequence of events has occurred and we are interested in the probability of another event. Example: Suppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[ S=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}. \\] What is the probability that a blue vehicle is selected, given a sedan was selected? Since we know that a sedan was selected, our sample space has been reduced to just “red sedan” and “blue sedan”. The probability of selecting a blue vehicle out of this sample space is simply 1/2. In set notation, let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. We are looking for \\(\\mbox{P}(A \\mbox{ given } B)\\), which is also written as \\(\\mbox{P}(A|B)\\). By definition, \\[ \\mbox{P}(A|B)=\\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)} \\] It is important to distinguish between the event \\(A|B\\) and \\(A \\cap B\\). This is a common misunderstanding about probability. \\(A \\cap B\\) is the event that an outcome was selected at random from the total sample space, and that outcome was contained in both \\(A\\) and \\(B\\). On the other hand, \\(A|B\\) assumes the \\(B\\) has occurred, and an outcome was drawn from the remaining sample space, and that outcome was contained in \\(A\\). Another common misunderstanding involves the direction of conditional probability. Specifically, \\(A|B\\) is NOT the same event as \\(B|A\\). For example, consider a medical test for a disease. The probability that someone tests positive given they had the disease is different than the probability that someone has the disease given they tested positive. We will explore this example further in our Bayes’ Rule section. 3.3 Independence Two events, \\(A\\) and \\(B\\), are said to be independent if the probability of one occurring does not change whether or not the other has occurred. We looked at this last chapter but now we have another way of looking at it using conditional probabilities. For example, let’s say the probability that a randomly selected student has seen the latest superhero movie is 0.55. What if we randomly select a student and we see that he/she is wearing a black backpack? Does that probability change? Likely not, since movie attendance is probably not related to choice of backpack color. These two events are independent. Mathematically, \\(A\\) and \\(B\\) are considered independent if and only if \\[ \\mbox{P}(A|B)=\\mbox{P}(A) \\] Result: \\(A\\) and \\(B\\) are independent if and only if \\[ \\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B) \\] This follows from the definition of conditional probability and from above: \\[ \\mbox{P}(A|B)=\\frac{\\mbox{P}(A\\cap B)}{\\mbox{P}(B)}=\\mbox{P}(A) \\] Thus, \\(\\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B)\\). Example: Consider the example above. Recall events \\(A\\) and \\(B\\). Let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. Are \\(A\\) and \\(B\\) independent? No. First, recall that \\(\\mbox{P}(A|B)=0.5\\). The probability of selecting a blue vehicle (\\(\\mbox{P}(A)\\)) is \\(2/7\\) (the number of blue vehicles in our sample space divided by 7, the total number vehicles in \\(S\\)). This value is different from 0.5; thus, \\(A\\) and \\(B\\) are not independent. We could also use the result above to determine whether \\(A\\) and \\(B\\) are independent. Note that \\(\\mbox{P}(A)= 2/7\\). Also, we know that \\(\\mbox{P}(B)=2/7\\). So, \\(\\mbox{P}(A)\\mbox{P}(B)=4/49\\). But, \\(\\mbox{P}(A\\cap B) = 1/7\\), since there is just one blue sedan in the sample space. \\(4/49\\) is not equal to \\(1/7\\); thus, \\(A\\) and \\(B\\) are not independent. 3.4 Bayes’ Rule As mentioned in the introduction to this section, \\(\\mbox{P}(A|B)\\) is not the same quantity as \\(\\mbox{P}(B|A)\\). However, if we are given information about \\(A|B\\) and \\(B\\), we can use Bayes’ Rule to find \\(\\mbox{P}(B|A)\\). Let \\(B_1, B_2, ..., B_n\\) be mutually exclusive and exhaustive events and let \\(\\mbox{P}(A)&gt;0\\). Then, \\[ \\mbox{P}(B_k|A)=\\frac{\\mbox{P}(A|B_k)\\mbox{P}(B_k)}{\\sum_{i=1}^n \\mbox{P}(A|B_i)\\mbox{P}(B_i)} \\] Let’s use an example to dig into where this comes from. Example: Suppose a doctor has developed a blood test for a certain rare disease (only one out of every 10,000 people have this disease). After careful and extensive evaluation of this blood test, the doctor determined the test’s sensitivity and specificity. Sensitivity is the probability of detecting the disease for those who actually have it. Note that this is a conditional probability. Specificity is the probability of correctly identifying “no disease” for those who do not have it. Again, another conditional probability. See Figure 3.1 for a visual representation of these terms and others related to what is termed a confusion matrix. FIGURE 3.1: A table of true results and test results for a hypothetical disease. The terminology is included in the table. These ideas are important when evaluating machine learning classification models. In fact, this test had a sensitivity of 100% and a specificity of 99.9%. Now suppose a patient walks in, the doctor administers the blood test, and it returns positive. What is the probability that that patient actually has the disease? This is a classic example of how probability could be misunderstood. Upon reading this question, you might guess that the answer to our question is quite high. After all, this is a nearly perfect test. After exploring the problem more in depth, we find a different result. 3.4.1 Approach using whole numbers Without going directly to the formulaic expression above, let’s consider a collection of 100,000 randomly selected people. What do we know? Based on the prevalence of this disease (one out of every 10,000 people have this disease), we know that 10 of them should have the disease. This test is perfectly sensitive. Thus, of the 10 people that have the disease, all of them test positive. This test has a specificity of 99.9%. Of the 99,990 that don’t have the disease, \\(0.999*99990\\approx 99890\\) will test negative. The remaining 100 will test positive. Thus, of our 100,000 randomly selected people, 110 will test positive. Of these 110, only 10 actually have the disease. Thus, the probability that someone has the disease given they’ve tested positive is actually around \\(10/110 = 0.0909\\). 3.4.2 Mathematical approach Now let’s put this in context of Bayes’ Rule as stated above. First, let’s define some events. Let \\(D\\) be the event that someone has the disease. Thus, \\(D&#39;\\) would be the event that someone does not have the disease. Similarly, let \\(T\\) be the event that someone has tested positive. What do we already know? \\[ \\mbox{P}(D) = 0.0001 \\hspace{1cm} \\mbox{P}(D&#39;)=0.9999 \\] \\[ \\mbox{P}(T|D)= 1 \\hspace{1cm} \\mbox{P}(T&#39;|D)=0 \\] \\[ \\mbox{P}(T&#39;|D&#39;)=0.999 \\hspace{1cm} \\mbox{P}(T|D&#39;) = 0.001 \\] We are looking for \\(\\mbox{P}(D|T)\\), the probability that someone has the disease, given he/she has tested positive. By the definition of conditional probability, \\[ \\mbox{P}(D|T)=\\frac{\\mbox{P}(D \\cap T)}{\\mbox{P}(T)} \\] The numerator can be rewritten, again utilizing the definition of conditional probability: \\(\\mbox{P}(D\\cap T)=\\mbox{P}(T|D)\\mbox{P}(D)\\). The denominator can be rewritten using the Law of Total Probability (discussed here) and then the definition of conditional probability: \\(\\mbox{P}(T)=\\mbox{P}(T\\cap D) + \\mbox{P}(T \\cap D&#39;) = \\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D&#39;)\\mbox{P}(D&#39;)\\). So, putting it all together, \\[ \\mbox{P}(D|T)=\\frac{\\mbox{P}(T|D)\\mbox{P}(D)}{\\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D&#39;)\\mbox{P}(D&#39;)} \\] Now we have stated our problem in the context of quantities we know: \\[ \\mbox{P}(D|T)=\\frac{1\\cdot 0.0001}{1\\cdot 0.0001 + 0.001\\cdot 0.9999} = 0.0909 \\] Note that in the original statement of Bayes’ Rule, we considered \\(n\\) partitions, \\(B_1, B_2,...,B_n\\). In this example, we only have two: \\(D\\) and \\(D&#39;\\). 3.4.3 Simulation To do the simulation, we can think of it as flipping a coin. First let’s assume we are pulling 1,000,000 people from the population. The probability that any one person has the disease is 0.0001. We will use rflip() to get the 1,000,000 people and designate as no disease or disease. set.seed(43) results &lt;- rflip(1000000, 0.0001, summarize = TRUE) results n heads tails prob 1 1e+06 100 999900 1e-04 In this case 100 people had the disease. Now let’s find the positive test results. Of the 100 with the disease, all will test positive. Of those without disease, there is a 0.001 probability of testing positive. rflip(as.numeric(results[&#39;tails&#39;]), prob = 0.001, summarize = TRUE) n heads tails prob 1 999900 959 998941 0.001 Now 959 tested positive. Thus the probability of having the disease given a positive test result is approximately: 100/(100 + 959) [1] 0.09442871 3.5 Homework Problems Consider: \\(A\\), \\(B\\) and \\(C\\) are events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\), \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Are \\(A\\) and \\(B\\) independent? Are \\(B\\) and \\(C\\) independent? Suppose I have a biased coin (the probability I flip a heads is 0.6). I flip that coin twice. Assume that the coin is memoryless (flips are independent of one another). What is the probability that the second flip results in heads? What is the probability that the second flip results in heads, given the first also resulted in heads? What is the probability both flips result in heads? What is the probability exactly one coin flip results in heads? Now assume I flip the coin five times. What is the probability the result is 5 heads? What is the probability the result is exactly 2 heads (out of 5 flips)? Suppose there are three assistants working at a company: Moe, Larry and Curly. All three assist with a filing process. Only one filing assistant is needed at a time. Moe assists 60% of the time, Larry assists 30% of the time and Curly assists the remaining 10% of the time. Occasionally, they make errors (misfiles); Moe has a misfile rate of 0.01, Larry has a misfile rate of 0.025, and Curly has a rate of 0.05. Suppose a misfile was discovered, but it is unknown who was on schedule when it occurred. Who is most likely to have committed the misfile? Calculate the probabilities for each of the three assistants. You are playing a game where there are two coins. One coin is fair and the other comes up heads 80% of the time. One coin is flipped 3 times and the result is three heads, what is the probability that the coin flipped is the fair coin? You will need to make an assumption about the probability of either coin being selected. Use Bayes formula to solve this problem. Use simulation to solve this problem. Solutions Manual "],["4-RANDVAR.html", "Chapter 4 Random Variables 4.1 Objectives 4.2 Random variables 4.3 Moments 4.4 Homework Problems Solutions Manual", " Chapter 4 Random Variables 4.1 Objectives Define and use properly in context all new terminology, to include: random variable, discrete random variable, continuous random variable, mixed random variable, distribution function, probability mass function, cumulative distribution function, moment, expectation, mean, variance. Given a discrete random variable, obtain the pmf and cdf, and use them to obtain probabilities of events. Simulate random variables for a discrete distribution. Find the moments of a discrete random variable. Find the expected value of a linear transformation of a random variable. 4.2 Random variables We have already discussed random experiments. We have also discussed \\(S\\), the sample space for an experiment. A random variable essentially maps the events in the sample space to the real number line. For a formal definition: A random variable \\(X\\) is a function \\(X: S\\rightarrow \\mathbb{R}\\) that assigns exactly one number to each outcome in an experiment. Example: Suppose you flip a coin three times. The sample space, \\(S\\), of this experiment is \\[ S=\\{\\mbox{HHH}, \\mbox{HHT}, \\mbox{HTH}, \\mbox{HTT}, \\mbox{THH}, \\mbox{THT}, \\mbox{TTH}, \\mbox{TTT}\\} \\] Let the random variable \\(X\\) be the number of heads in three coin flips. Whenever introduced to a new random variable, you should take a moment to think about what possible values can \\(X\\) take? When tossing a coin 3 times, we can get no heads, one head, two heads or three heads. The random variable \\(X\\) assigns each outcome in our experiment to one of these values. Visually: \\[ S=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\} \\] The sample space of \\(X\\), sometimes referred to as the support, is the list of numerical values that \\(X\\) can take. \\[ S_X=\\{0,1,2,3\\} \\] Because the sample space of \\(X\\) is a countable list of numbers, we consider \\(X\\) to be a discrete random variable (more on that later). 4.2.1 How does this help? Sticking with our example, we can now frame a problem of interest in the context of our random variable \\(X\\). For example, suppose we wanted to know the probability of at least two heads. Without our random variable, we have to write this as: \\[ \\mbox{P}(\\mbox{at least two heads})= \\mbox{P}(\\{\\mbox{HHH},\\mbox{HHT},\\mbox{HTH},\\mbox{THH}\\}) \\] In the context of our random variable, this simply becomes \\(\\mbox{P}(X\\geq 2)\\). It may not seem important in a case like this, but imagine if we were flipping a coin 50 times and wanted to know the probability of obtaining at least 30 heads. It would be unfeasible to write out all possible ways to obtain at least 30 heads. It is much easier to write \\(\\mbox{P}(X\\geq 30)\\) and explore the distribution of \\(X\\). Essentially, a random variable often helps us reduce a complex random experiment to a simple variable that is easy to characterize. 4.2.2 Discrete vs Continuous A discrete random variable has a sample space that consists of a countable set of values. \\(X\\) in our example above is a discrete random variable. Note that “countable” does not necessarily mean “finite”. For example, a random variable with a Poisson distribution (a topic for a later chapter) has a sample space of \\(\\{0,1,2,...\\}\\). This sample space is unbounded, but it is considered countably infinite, and thus the random variable would be considered discrete. A continuous random variable has a sample space that is a continuous interval. For example, let \\(Y\\) be the random variable corresponding to the height of a randomly selected individual. \\(Y\\) is a continuous random variable because a person could measure 68.1 inches, 68.2 inches, or perhaps any value in between. Note that when we measure height, our precision is limited by our measuring device, so we are technically “discretizing” height. However, even in these cases, we typically consider height to be a continuous random variable. A mixed random variable is exactly what it sounds like. It has a sample space that is both discrete and continuous. How could such a thing occur? Consider an experiment where a person rolls a standard six-sided die. If it lands on anything other than one, the result of the die roll is recorded. If it lands on one, the person spins a wheel, and the angle in degrees of the resulting spin, divided by 360, is recorded. If our random variable \\(Z\\) is the number that is recorded in this experiment, the sample space of \\(Z\\) is \\([0,1] \\cup \\{2,3,4,5,6\\}\\). We will not be spending much time on mixed random variables. However they do occur in practice, consider the job of analyzing bomb error data. If the bomb hits within a certain radius, the error is 0. Otherwise it is measured in a radial direction. This data is mixed. 4.2.3 Discrete distribution functions Once we have defined a random variable, we need a way to describe its behavior and we will use probabilities for this purpose. Distribution functions describe the behavior of random variables. We can use these functions to determine the probability that a random variable takes a value or a range of values. For discrete random variables, there are two distribution functions of interest: the probability mass function (pmf) and the cumulative distribution function (cdf). 4.2.4 Probability mass function Let \\(X\\) be a discrete random variable. The probability mass function (pmf) of \\(X\\), given by \\(f_X(x)\\), is a function that assigns probability to each possible outcome of \\(X\\). \\[ f_X(x)=\\mbox{P}(X=x) \\] Note that the pmf is a function. Functions have input and output. The input of a pmf is any real number. The output of a pmf is the probability that the random variable takes the inputted value. The pmf must follow the axioms of probability described in the Probability Rules chapter. Primarily, For all \\(x \\in \\mathbb{R}\\), \\(0 \\leq f_X(x) \\leq 1\\). \\(\\sum_x f_X(x) = 1\\), where the \\(x\\) in the index of the sum simply denotes that we are summing across the entire domain or support of \\(X\\). Example: Recall our example again. You flip a coin three times and let \\(X\\) be the number of heads in those three coin flips. We know that \\(X\\) can only take values 0, 1, 2 or 3. But at what probability does it take these three values? In that example, we had listed out the possible outcomes of the experiment and denoted what value of \\(X\\) corresponds to each outcome. \\[ S=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\} \\] Each of these eight outcomes is equally likely (each with a probability of \\(\\frac{1}{8}\\)). Thus, building the pmf of \\(X\\) becomes a matter of counting the number of outcomes associated with each possible value of \\(X\\): \\[ f_X(x)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{1}{8}, &amp; x=0 \\\\ \\frac{3}{8}, &amp; x=1 \\\\ \\frac{3}{8}, &amp; x=2 \\\\ \\frac{1}{8}, &amp; x=3 \\\\ 0, &amp; \\mbox{otherwise} \\end{array} \\right . \\] Note that this function specifies the probability that \\(X\\) takes any of the four values in the sample space (0, 1, 2, and 3). Also, it specifies that the probability that \\(X\\) takes any other value is 0. Graphically, the pmf is not terribly interesting. The pmf is 0 at all values of \\(X\\) except for 0, 1, 2 and 3, Figure 4.1. FIGURE 4.1: Probability Mass Function of \\(X\\) from Coin Flip Example Example: We can use a pmf to answer questions about an experiment. For example, consider the same context. What is the probability that we flip at least one heads? We can write this in the context of \\(X\\): \\[ \\mbox{P}(\\mbox{at least one heads})=\\mbox{P}(X\\geq 1)=\\mbox{P}(X=1)+\\mbox{P}(X=2)+\\mbox{P}(X=3)=\\frac{3}{8} + \\frac{3}{8}+\\frac{1}{8}=\\frac{7}{8} \\] Alternatively, we can recognize that \\(\\mbox{P}(X\\geq 1)=1-\\mbox{P}(X=0)=1-\\frac{1}{8}=\\frac{7}{8}\\). 4.2.5 Cumulative distribution function Let \\(X\\) be a discrete random variable. The cumulative distribution function (cdf) of \\(X\\), given by \\(F_X(x)\\), is a function that assigns to each value of \\(X\\) the probability that \\(X\\) takes that value or lower: \\[ F_X(x)=\\mbox{P}(X\\leq x) \\] Again, note that the cdf is a function with an input and output. The input of a cdf is any real number. The output of a cdf is the probability that the random variable takes the inputted value or less. If we know the pmf, we can obtain the cdf: \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\sum_{y\\leq x} f_X(y) \\] Like the pmf, the cdf must be between 0 and 1. Also, since the pmf is always non-negative, the cdf must be non-decreasing. Example: Obtain and plot the cdf of \\(X\\) of the previous example. \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\left\\{\\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} 0, &amp; x &lt;0 \\\\ \\frac{1}{8}, &amp; 0\\leq x &lt; 1 \\\\ \\frac{4}{8}, &amp; 1\\leq x &lt; 2 \\\\ \\frac{7}{8}, &amp; 2\\leq x &lt; 3 \\\\ 1, &amp; x\\geq 3 \\end{array}\\right . \\] Visually, the cdf of a discrete random variable has a stairstep appearance. In this example, the cdf takes a value 0 up until \\(X=0\\), at which point the cdf increases to 1/8. It stays at this value until \\(X=1\\), and so on. At and beyond \\(X=3\\), the cdf is equal to 1, Figure 4.2. FIGURE 4.2: Cumulative Distribution Function of \\(X\\) from Coin Flip Example 4.2.6 Simulating random variables We can simulate values from a random variable using the cdf, we will use a similar idea for continuous random variables. Since the range of the cdf is in the interval \\([0,1]\\) we will generate a random number in that same interval and then use the inverse function to find the value of the random variable. The pseudo code is: 1) Generate a random number, \\(U\\). 2) Find the index \\(k\\) such that \\(\\sum_{j=1}^{k-1}f_X(x_{j}) \\leq U &lt; \\sum_{j=1}^{k}f_X(x_{j})\\) or \\(F_x(k-1) \\leq U &lt; F_{x}(k)\\). Example: Simulate a random variable for the number of heads in flipping a coin three times. First we will create the pmf. pmf &lt;- c(1/8, 3/8, 3/8, 1/8) values &lt;- c(0, 1, 2, 3) pmf [1] 0.125 0.375 0.375 0.125 We get the cdf from the cumulative sum. cdf &lt;- cumsum(pmf) cdf [1] 0.125 0.500 0.875 1.000 Next, we will generate a random number between 0 and 1. set.seed(1153) ran_num &lt;- runif(1) ran_num [1] 0.7381891 Finally, we will find the value of the random variable. We will do each step separately first so you can understand the code. ran_num &lt; cdf [1] FALSE FALSE TRUE TRUE which(ran_num &lt; cdf) [1] 3 4 which(ran_num &lt; cdf)[1] [1] 3 values[which(ran_num &lt; cdf)[1]] [1] 2 Let’s make this a function. simple_rv &lt;- function(values, cdf){ ran_num &lt;- runif(1) return(values[which(ran_num &lt; cdf)[1]]) } Now let’s generate 10000 values from this random variable. results &lt;- do(10000)*simple_rv(values, cdf) inspect(results) quantitative variables: name class min Q1 median Q3 max mean sd n missing 1 simple_rv numeric 0 1 2 2 3 1.5048 0.860727 10000 0 tally(~simple_rv, data = results, format = &quot;proportion&quot;) simple_rv 0 1 2 3 0.1207 0.3785 0.3761 0.1247 Not a bad approximation. 4.3 Moments Distribution functions are excellent characterizations of random variables. The pmf and cdf will tell you exactly how often the random variables takes particular values. However, distribution functions are often a lot of information. Sometimes, we may want to describe a random variable \\(X\\) with a single value or small set of values. For example, we may want to know the average or some measure of center of \\(X\\). We also may want to know a measure of spread of \\(X\\). Moments are values that summarize random variables with single numbers. Since we are dealing with the population, these moments are population values and not summary statistics as we used in the first block of material. 4.3.1 Expectation At this point, we should define the term expectation. Let \\(g(X)\\) be some function of a discrete random variable \\(X\\). The expected value of \\(g(X)\\) is given by: \\[ \\mbox{E}(g(X))=\\sum_x g(x) \\cdot f_X(x) \\] 4.3.2 Mean The most common moments used to describe random variables are mean and variance. The mean (often referred to as the expected value of \\(X\\)), is simply the average value of a random variable. It is denoted as \\(\\mu_X\\) or \\(\\mbox{E}(X)\\). In the discrete case, the mean is found by: \\[ \\mu_X=\\mbox{E}(X)=\\sum_x x \\cdot f_X(x) \\] The mean is also known as the first moment of \\(X\\) around the origin. It is a weighted sum with the weight being the probability. If each outcome were equally likely, the expected value would just be the average of the values of the random variable since each weight is the reciprocal of the number of values. Example: Find the expected value (or mean) of \\(X\\): the number of heads in three flips of a fair coin. \\[ \\mbox{E}(X)=\\sum_x x\\cdot f_X(x) = 0*\\frac{1}{8} + 1*\\frac{3}{8} + 2*\\frac{3}{8} + 3*\\frac{1}{8}=1.5 \\] We are using \\(\\mu\\) because it is a population parameter. From our simulation above, we can find the mean as an estimate of the expected value. This is really a statistic since our simulation is data from the population and thus will have variance from sample to sample. mean(~simple_rv, data = results) [1] 1.5048 4.3.3 Variance Variance is a measure of spread of a random variable. The variance of \\(X\\) is denoted as \\(\\sigma^2_X\\) or \\(\\mbox{Var}(X)\\). It is equivalent to the average squared deviation from the mean: \\[ \\sigma^2_X=\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2] \\] In the discrete case, this can be evaluated by: \\[ \\mbox{E}[(X-\\mu_X)^2]=\\sum_x (x-\\mu_X)^2f_X(x) \\] Variance is also known as the second moment of \\(X\\) around the mean. The square root of \\(\\mbox{Var}(X)\\) is denoted as \\(\\sigma_X\\), the standard deviation of \\(X\\). The standard deviation is often reported because it is measured in the same units as \\(X\\), while the variance is measured in squared units and is thus harder to interpret. Example: Find the variance of \\(X\\): the number of heads in three flips of a fair coin. \\[ \\mbox{Var}(X)=\\sum_x (x-\\mu_X)^2 \\cdot f_X(x) \\] \\[ = (0-1.5)^2 \\times \\frac{1}{8} + (1-1.5)^2 \\times \\frac{3}{8}+(2-1.5)^2 \\times \\frac{3}{8} + (3-1.5)^2\\times \\frac{1}{8} \\] In R this is: (0-1.5)^2*1/8 + (1-1.5)^2*3/8 + (2-1.5)^2*3/8 + (3-1.5)^2*1/8 [1] 0.75 The variance of \\(X\\) is 0.75. We can find the variance of the simulation but R uses the sample variance and this is the population variance. So we need to multiply by \\(\\frac{n-1}{n}\\) var(~simple_rv, data = results)*(10000 - 1)/10000 [1] 0.740777 4.3.4 Mean and variance of Linear Transformations Lemma: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Then: \\[ \\mbox{E}(aX+b)=a\\mbox{E}(X)+b \\] and \\[ \\mbox{Var}(aX+b)=a^2\\mbox{Var}(X) \\] The proof of this is left as a homework problem. 4.4 Homework Problems Suppose we are flipping a fair coin, and the result of a single coin flip is either heads or tails. Let \\(X\\) be a random variable representing the number of flips until the first heads. Is \\(X\\) discrete or continuous? What is the domain, support, of \\(X\\)? What values do you expect \\(X\\) to take? What do you think is the average of \\(X\\)? Don’t actually do any formal math, just think about if you were flipping a regular coin, how long it would take you to get the first heads. Advanced: In R, generate 10,000 observations from \\(X\\). What is the empirical, from the simulation, pmf? What is the average value of \\(X\\) based on this simulation? Create a bar chart of the proportions. Note: Unlike the example in the Notes, we don’t have the pmf, so you will have to simulate the experiment and using R to find the number of flips until the first heads. Note: There are many ways to do this. Below is a description of one approach. It assumes we are extremely unlikely to go past 1000 flips. First, let’s sample with replacement from the vector c(“H”,“T”), 1000 times with replacement, use sample(). As we did in the reading, use which() and a logical argument to find the first occurrence of a heads. Find the theoretical distribution, use math to come up with a closed for solution for the pmf.   Repeat Problem 1,except part d, but with a different random variable, \\(Y\\): the number of coin flips until the fifth heads.   Suppose you are a data analyst for a large international airport. Your boss, the head of the airport, is dismayed that this airport has received negative attention in the press for inefficiencies and sluggishness. In a staff meeting, your boss gives you a week to build a report addressing the “timeliness” at the airport. Your boss is in a big hurry and gives you no further information or guidance on this task. Prior to building the report, you will need to conduct some analysis. To aid you in this, create a list of at least three random variables that will help you address timeliness at the airport. For each of your random variables, Determine whether it is discrete or continuous. Report its domain. What is the experimental unit? Explain how this random variable will be useful in addressing timeliness at the airport. We will provide one example: Let \\(D\\) be the difference between a flight’s actual departure and its scheduled departure. This is a continuous random variable, since time can be measured in fractions of minutes. A flight can be early or late, so domain is any real number. The experimental unit is each individual (non-canceled) flight. This is a useful random variable because the average value of \\(D\\) will describe whether flights take off on time. We could also find out how often \\(D\\) exceeds 0 (implying late departure) or how often \\(D\\) exceeds 30 minutes, which could indicate a “very late” departure.   Consider the experiment of rolling two fair six-sided dice. Let the random variable \\(Y\\) be the absolute difference between the two numbers that appear upon rolling the dice. What is the domain/support of \\(Y\\)? What values do you expect \\(Y\\) to take? What do you think is the average of \\(Y\\)? Don’t actually do any formal math, just think about the experiment. Find the probability mass function and cumulative distribution function of \\(Y\\). Find the expected value and variance of \\(Y\\). Advanced: In R, obtain 10,000 realizations of \\(Y\\). In other words, simulate the roll of two fair dice, record the absolute difference and repeat this 10,000 times. Construct a frequency table of your results (what percentage of time did you get a difference of 0? difference of 1? etc.) Find the mean and variance of your simulated sample of \\(Y\\). Were they close to your answers in part d?   Prove the Lemma from the Notes: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Show \\(\\mbox{E}(aX + b)=a\\mbox{E}(X)+b\\).   We saw that \\(\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\\). Show that \\(\\mbox{Var}(X)\\) is also equal to \\(\\mbox{E}(X^2)-[\\mbox{E}(X)]^2\\). Solutions Manual "],["5-CONRANDVAR.html", "Chapter 5 Continuous Random Variables 5.1 Objectives 5.2 Continuous random variables 5.3 Moments 5.4 Homework Problems Solutions Manual", " Chapter 5 Continuous Random Variables 5.1 Objectives Define and properly use in context all new terminology, to include: probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables. Given a continuous random variable, find probabilities using the pdf and/or the cdf. Find the mean and variance of a continuous random variable. 5.2 Continuous random variables In the last chapter, we introduced random variables, and explored discrete random variables. In this chapter, we will move into continuous random variables, their properties, their distribution functions, and how they differ from discrete random variables. Recall that a continuous random variable has a domain that is a continuous interval (or possibly a group of intervals). For example, let \\(Y\\) be the random variable corresponding to the height of a randomly selected individual. While our measurement will necessitate “discretizing” height to some degree, technically, height is a continuous random variable since a person could measure 67.3 inches or 67.4 inches or anything in between. 5.2.1 Continuous distribution functions So how do we describe the randomness of continuous random variables? In the case of discrete random variables, the probability mass function (pmf) and the cumulative distribution function (cdf) are used to describe randomness. However, recall that the pmf is a function that returns the probability that the random variable takes the inputted value. Due to the nature of continuous random variables, the probability that a continuous random variable takes on any one individual value is technically 0. Thus, a pmf cannot apply to a continuous random variable. Rather, we describe the randomness of continuous random variables with the probability density function (pdf) and the cumulative distribution function (cdf). Note that the cdf has the same interpretation and application as in the discrete case. 5.2.2 Probability density function Let \\(X\\) be a continuous random variable. The probability density function (pdf) of \\(X\\), given by \\(f_X(x)\\) is a function that describes the behavior of \\(X\\). It is important to note that in the continuous case, \\(f_X(x)\\neq \\mbox{P}(X=x)\\), as the probability of \\(X\\) taking any one individual value is 0. The pdf is a function. The input of a pdf is any real number. The output is known as the density. The pdf has three main properties: \\(f_X(x)\\geq 0\\) \\(\\int_{S_X} f_X(x)\\mathop{}\\!\\mathrm{d}x = 1\\) \\(\\mbox{P}(X\\in A)=\\int_{x\\in A} f_X(x)\\mathop{}\\!\\mathrm{d}x\\) or another way to write this \\(\\mbox{P}(a \\leq X \\leq b)=\\int_{a}^{b} f_X(x)\\mathop{}\\!\\mathrm{d}x\\) Properties 2) and 3) imply that the area underneath a pdf represents probability. The pdf is a non-negative function, it cannot have negative values. 5.2.3 Cumulative distribution function The cumulative distribution function (cdf) of a continuous random variable has the same interpretation as it does for a discrete random variable. It is a function. The input of a cdf is any real number, and the output is the probability that the random variable takes a value less than or equal to the inputted value. It is denoted as \\(F\\) and is given by: \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\int_{-\\infty}^x f_x(t) \\mathop{}\\!\\mathrm{d}t \\] Example: Let \\(X\\) be a continuous random variable with \\(f_X(x)=2x\\) where \\(0 \\leq x \\leq 1\\). Verify that \\(f\\) is a valid pdf. Find the cdf of \\(X\\). Also, find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\), \\(\\mbox{P}(X&gt;0.5)\\), and \\(\\mbox{P}(0.1\\leq X &lt; 0.75)\\). Finally, find the median of \\(X\\). To verify that \\(f\\) is a valid pdf, we simply note that \\(f_X(x) \\geq 0\\) on the range \\(0 \\leq x \\leq 1\\). Also, we note that \\(\\int_0^1 2x \\mathop{}\\!\\mathrm{d}x = x^2\\bigg|_0^1 = 1\\). Using R, we find integrate(function(x)2*x, 0, 1) 1 with absolute error &lt; 1.1e-14 Or we can use the mosaicCalc package to find the anti-derivative. If the package is not installed, you can use the Packages tab in RStudio or type install.packages(\"mosaicCalc\") at the command prompt. Load the library. library(mosaicCalc) (Fx &lt;- antiD(2*x ~ x)) function (x, C = 0) x^2 + C Fx(1) - Fx(0) [1] 1 Graphically, the pdf is displayed in Figure 5.1: FIGURE 5.1: pdf of \\(X\\) The cdf of \\(X\\) is found by \\[ \\int_0^x 2t \\mathop{}\\!\\mathrm{d}t = t^2\\bigg|_0^x = x^2 \\] This is antiD found from the calculations above. So, \\[ F_X(x)=\\left\\{ \\begin{array}{ll} 0, &amp; x&lt;0 \\\\ x^2, &amp; 0\\leq x \\leq 1 \\\\ 1, &amp; x&gt;1 \\end{array}\\right. \\] The plot of the cdf of \\(X\\) is shown in Figure 5.2. FIGURE 5.2: cdf of \\(X\\) Probabilities are found either by integrating the pdf or using the cdf: \\(\\mbox{P}(X &lt; 0.5)=\\mbox{P}(X\\leq 0.5)=F_X(0.5)=0.5^2=0.25\\). See Figure 5.3. FIGURE 5.3: Probability represented by shaded area \\(\\mbox{P}(X &gt; 0.5) = 1-\\mbox{P}(X\\leq 0.5)=1-0.25 = 0.75\\) See Figure 5.4. FIGURE 5.4: Probability represented by shaded area \\(\\mbox{P}(0.1\\leq X &lt; 0.75) = \\int_{0.1}^{0.75}2x\\mathop{}\\!\\mathrm{d}x = 0.75^2 - 0.1^2 = 0.5525\\) See Figure 5.5. integrate(function(x)2*x, 0.1, 0.75) 0.5525 with absolute error &lt; 6.1e-15 Alternatively, \\(\\mbox{P}(0.1\\leq X &lt; 0.75) = \\mbox{P}(X &lt; 0.75) -\\mbox{P}(x \\leq 0.1) = F(0.75)-F(0.1)=0.75^2-0.1^2 =0.5525\\) Fx(0.75) - Fx(0.1) [1] 0.5525 Notice for a continuous random variable, we are loose with the use of the = sign. This is because for a continuous random variable \\(\\mbox{P}(X=x)=0\\). Do not get sloppy when working with discrete random variables. FIGURE 5.5: Probability represented by shaded area The median of \\(X\\) is the value \\(x\\) such that \\(\\mbox{P}(X\\leq x)=0.5\\), the area under a single point is 0. So we simply solve \\(x^2=0.5\\) for \\(x\\). Thus, the median of \\(X\\) is \\(\\sqrt{0.5}=0.707\\). Or using R uniroot(function(x)(Fx(x) - 0.5), c(0, 1))$root [1] 0.7071067 5.2.4 Simulation As in the case of the discrete random variable, we can simulate a continuous random variable if we have an inverse for the cdf. The range of the cdf is \\([0,1]\\), so we generate a random number in this interval and then apply the inverse cdf to obtain a random variable. In a similar manner, for a continuous random variable, we use the following pseudo code: 1. Generate a random number in the interval \\([0,1]\\), \\(U\\). 2. Find the random variable \\(X\\) from \\(F_{X}^{-1}(U)\\). In R for our example, this looks like the following. sqrt(runif(1)) [1] 0.6137365 results &lt;- do(10000)*sqrt(runif(1)) inspect(results) quantitative variables: name class min Q1 median Q3 max mean 1 sqrt numeric 0.005321359 0.4977011 0.7084257 0.8656665 0.9999873 0.6669452 sd n missing 1 0.2358056 10000 0 Figure 5.6 is a density plot of the simulated density function. results %&gt;% gf_density(~sqrt, xlab = &quot;X&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;X&quot;, y = &quot;&quot;) FIGURE 5.6: Density plot of the simulated random variable. 5.3 Moments As with discrete random variables, moments can be calculated to summarize characteristics such as center and spread. In the discrete case, expectation is found by multiplying each possible value by its associated probability and summing across the domain (\\(\\mbox{E}(X)=\\sum_x x\\cdot f_X(x)\\)). In the continuous case, the domain of \\(X\\) consists of an infinite set of values. From your calculus days, recall that the sum across an infinite domain is represented by an integral. Let \\(g(X)\\) be any function of \\(X\\). The expectation of \\(g(X)\\) is found by: \\[ \\mbox{E}(g(X)) = \\int_{S_X} g(x)f_X(x)\\mathop{}\\!\\mathrm{d}x \\] 5.3.1 Mean and variance Let \\(X\\) be a continuous random variable. The mean of \\(X\\), or \\(\\mu_X\\), is simply \\(\\mbox{E}(X)\\). Thus, \\[ \\mbox{E}(X)=\\int_{S_X}x\\cdot f_X(x)\\mathop{}\\!\\mathrm{d}x \\] As in the discrete case, the variance of \\(X\\) is the expected squared difference from the mean, or \\(\\mbox{E}[(X-\\mu_X)^2]\\). Thus, \\[ \\sigma^2_X = \\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]= \\int_{S_X} (x-\\mu_X)^2\\cdot f_X(x) \\mathop{}\\!\\mathrm{d}x \\] Recall homework problem 6 from the last chapter. In this problem, you showed that \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\). Thus, \\[ \\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2 = \\int_{S_X} x^2\\cdot f_X(x)\\mathop{}\\!\\mathrm{d}x - \\mu_X^2 \\] Example: Consider the random variable \\(X\\) from above. Find the mean and variance of \\(X\\). \\[ \\mu_X= \\mbox{E}(X)=\\int_0^1 x\\cdot 2x\\mathop{}\\!\\mathrm{d}x = \\frac{2x^3}{3}\\bigg|_0^1 = \\frac{2}{3}=0.667 \\] Side note: Since the mean of \\(X\\) is smaller than the median of \\(X\\), we say that \\(X\\) is skewed to the left, or negatively skewed. Using R. integrate(function(x)x*2*x, 0, 1) 0.6666667 with absolute error &lt; 7.4e-15 Or using antiD() Ex &lt;- antiD(2*x^2 ~ x) Ex(1) - Ex(0) [1] 0.6666667 Using our simulation. mean(~sqrt, data = results) [1] 0.6669452 \\[ \\sigma^2_X = \\mbox{Var}(X)= \\mbox{E}(X^2)-\\mbox{E}(X)^2 = \\int_0^1 x^2\\cdot 2x\\mathop{}\\!\\mathrm{d}x - \\left(\\frac{2}{3}\\right)^2 = \\frac{2x^4}{4}\\bigg|_0^1-\\frac{4}{9}=\\frac{1}{2}-\\frac{4}{9}=\\frac{1}{18}=0.056 \\] integrate(function(x)x^2*2*x, 0, 1)$value - (2/3)^2 [1] 0.05555556 or Vx &lt;- antiD(x^2*2*x ~ x) Vx(1) - Vx(0) - (2/3)^2 [1] 0.05555556 var(~sqrt, data = results)*9999/10000 [1] 0.05559873 And finally, the standard deviation of \\(X\\) is \\(\\sigma_X = \\sqrt{\\sigma^2_X}=\\sqrt{1/18}=0.236\\). 5.4 Homework Problems Let \\(X\\) be a continuous random variable on the domain \\(-k \\leq X \\leq k\\). Also, let \\(f(x)=\\frac{x^2}{18}\\). Assume that \\(f(x)\\) is a valid pdf. Find the value of \\(k\\). Plot the pdf of \\(X\\). Find and plot the cdf of \\(X\\). Find \\(\\mbox{P}(X&lt;1)\\). Find \\(\\mbox{P}(1.5&lt;X\\leq 2.5)\\). Find the 80th percentile of \\(X\\) (the value \\(x\\) for which 80% of the distribution is to the left of that value). Find the value \\(x\\) such that \\(\\mbox{P}(-x \\leq X \\leq x)=0.4\\). Find the mean and variance of \\(X\\). Simulate 10000 values from this distribution and plot the density. Let \\(X\\) be a continuous random variable. Prove that the cdf of \\(X\\), \\(F_X(x)\\) is a non-decreasing function. (Hint: show that for any \\(a &lt; b\\), \\(F_X(a) \\leq F_X(b)\\).) Solutions Manual "],["6-DISCRETENAMED.html", "Chapter 6 Named Discrete Distributions 6.1 Objectives 6.2 Named distributions 6.3 Homework Problems Solutions Manual", " Chapter 6 Named Discrete Distributions 6.1 Objectives Recognize and set up for use common discrete distributions (Uniform, Binomial, Poisson, Hypergeometric) to include parameters, assumptions, and moments. Use R to calculate probabilities and quantiles involving random variables with common discrete distributions. 6.2 Named distributions In the previous two chapters, we introduced the concept of random variables, distribution functions, and expectations. In some cases, the nature of an experiment may yield random variables with common distributions. In these cases, we can rely on easy-to-use distribution functions and built-in R functions in order to calculate probabilities and quantiles. 6.2.1 Discrete uniform distribution The first distribution we will discuss is the discrete uniform distribution. It is not a very commonly used distribution, especially compared to its continuous counterpart. A discrete random variable has the discrete uniform distribution if probability is evenly allocated to each value in the sample space. A variable with this distribution has parameters \\(a\\) and \\(b\\) representing the minimum and maximum of the sample space, respectively. (By default, that sample space is assumed to consist of integers only, but that is by no means always the case.) Example: Rolling a fair die is an example of the discrete uniform. Each side of the die has an equal probability. Let \\(X\\) be a discrete random variable with the uniform distribution. If the sample space is consecutive integers, this distribution is denoted as \\(X\\sim\\textsf{DUnif}(a,b)\\). The pmf of \\(X\\) is given by: \\[ f_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{b-a+1}, &amp; x \\in \\{a, a+1,...,b\\} \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] For the die: \\[ f_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6-1+1} = \\frac{1}{6}, &amp; x \\in \\{1, 2,...,6\\} \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] The expected value of \\(X\\) is found by: \\[ \\mbox{E}(X)=\\sum_{x=a}^b x\\cdot\\frac{1}{b-a+1}= \\frac{1}{b-a+1} \\cdot \\sum_{x=a}^b x=\\frac{1}{b-a+1}\\cdot\\frac{b-a+1}{2}\\cdot (a+b) = \\frac{a+b}{2} \\] Where the sum of consecutive integers is a common result from discrete math, research it for more information. The variance of \\(X\\) is found by: (derivation not included) \\[ \\mbox{Var}(X)=\\mbox{E}[(X-\\mbox{E}(X))^2]=\\frac{(b-a+1)^2-1}{12} \\] Summarizing for the die: Let \\(X\\) be the result of a single roll of a fair die. We will report the distribution of \\(X\\), the pmf, \\(\\mbox{E}(X)\\) and \\(\\mbox{Var}(X)\\). The sample space of \\(X\\) is \\(S_X=\\{1,2,3,4,5,6\\}\\). Since each of those outcomes is equally likely, \\(X\\) follows the discrete uniform distribution with \\(a=1\\) and \\(b=6\\). Thus, \\[ f_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6}, &amp; x \\in \\{1,2,3,4,5,6\\} \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Finally, \\(\\mbox{E}(X)=\\frac{1+6}{2}=3.5\\). Also, \\(\\mbox{Var}(X)=\\frac{(6-1+1)^2-1}{12}=\\frac{35}{12}=2.917\\). 6.2.2 Simulating To simulate the discrete uniform, we use sample(). Example: To simulate rolling a die 4 times, we use sample(). set.seed(61) sample(1:6, 4, replace = TRUE) [1] 4 2 2 1 Let’s roll it 10,000 times and find results &lt;- do(10000)*sample(1:6, 1, replace = TRUE) tally(~sample, data = results, format = &quot;percent&quot;) sample 1 2 3 4 5 6 16.40 16.46 16.83 17.15 16.92 16.24 mean(~sample, data = results) [1] 3.5045 var(~sample, data = results)*(10000 - 1)/10000 [1] 2.87598 Again as a reminder, we multiply by \\(\\frac{(10000-1)}{10000}\\) because the function var() is calculating a sample variance using \\(n-1\\) in the denominator but we need the population variance. 6.2.3 Binomial distribution The binomial distribution is extremely common, and appears in many situations. In fact, we have already discussed several examples where the binomial distribution is heavily involved. Consider an experiment involving repeated independent trials of a binary process (two outcomes), where in each trial, there is a constant probability of “success” (one of the outcomes which is arbitrary). If the random variable \\(X\\) represents the number of successes out of \\(n\\) independent trials, then \\(X\\) is said to follow the binomial distribution with parameters \\(n\\) and \\(p\\) (the probability of a success in each trial). The pmf of \\(X\\) is given by: \\[ f_X(x)=\\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x} \\] for \\(x \\in \\{0,1,...,n\\}\\) and 0 otherwise. Let’s take a moment to dissect this pmf. We are looking for the probability of obtaining \\(x\\) successes out of \\(n\\) trials. The \\(p^x\\) represents the probability of \\(x\\) successes, using the multiplication rule because of the independence assumption. The term \\((1-p)^{n-x}\\) represents the probability of the remainder of the trials as failures. Finally, the \\(n\\choose x\\) term represents the number of ways to obtain \\(x\\) successes out of \\(n\\) trials. For example, there are three ways to obtain 1 success out of 3 trials (one success followed by two failures; one failure, one success and then one failure; or two failures followed by a success). The expected value of a binomially distributed random variable is given by \\(\\mbox{E}(X)=np\\) and the variance is given by \\(\\mbox{Var}(X)=np(1-p)\\). Example: Let \\(X\\) be the number of heads out of 20 independent flips of a fair coin. Note that this is a binomial because the trials are independent and the probability of success, in this case a heads, is constant, and there are two outcomes. Find the distribution, mean and variance of \\(X\\). Find \\(\\mbox{P}(X=8)\\). Find \\(\\mbox{P}(X\\leq 8)\\). \\(X\\) has the binomial distribution with \\(n=20\\) and \\(p=0.5\\). The pmf is given by: \\[ f_X(x)=\\mbox{P}(X=x)={20 \\choose x}0.5^x (1-0.5)^{20-x} \\] Also, \\(\\mbox{E}(X)=20*0.5=10\\) and \\(\\mbox{Var}(X)=20*0.5*0.5=5\\). To find \\(\\mbox{P}(X=8)\\), we can simply use the pmf: \\[ \\mbox{P}(X=8)=f_X(8)={20\\choose 8}0.5^8 (1-0.5)^{12} \\] choose(20, 8)*0.5^8*(1 - 0.5)^12 [1] 0.1201344 To find \\(\\mbox{P}(X\\leq 8)\\), we would need to find the cumulative probability: \\[ \\mbox{P}(X\\leq 8)=\\sum_{x=0}^8 {20\\choose 8}0.5^x (1-0.5)^{20-x} \\] x &lt;- 0:8 sum(choose(20, x)*0.5^x*(1 - 0.5)^(20 - x)) [1] 0.2517223 6.2.4 Software Functions One of the advantages of using named distributions is that most software packages have built-in functions that compute probabilities and quantiles for common named distributions. Over the course of this chapter, you will notice that each named distribution is treated similarly in R. There are four main functions tied to each distribution. For the binomial distribution, these are dbinom(), pbinom(), qbinom(), and rbinom(). dbinom(): This function is equivalent to the probability mass function. We use this to find \\(\\mbox{P}(X=x)\\) when \\(X\\sim \\textsf{Binom}(n,p)\\). This function takes three inputs: x (the value of the random variable), size (the number of trials, \\(n\\)), and prob (the probability of success, \\(p\\)). So, \\[ \\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x}=\\textsf{dbinom(x,n,p)} \\] pbinom(): This function is equivalent to the cumulative distribution function. We use this to find \\(\\mbox{P}(X\\leq x)\\) when \\(X\\sim \\textsf{Binom}(n,p)\\). This function takes the same inputs as dbinom() but returns the cumulative probability: \\[ \\mbox{P}(X\\leq x)=\\sum_{k=0}^x{n\\choose{k}}p^k(1-p)^{n-k}=\\textsf{pbinom(x,n,p)} \\] qbinom(): This is the inverse of the cumulative distribution function and will return a percentile. This function has three inputs: p (a probability), size and prob. It returns the smallest value \\(x\\) such that \\(\\mbox{P}(X\\leq x) \\geq p\\). rbinom(): This function is used to randomly generate values from the binomial distribution. It takes three inputs: n (the number of values to generate), size and prob. It returns a vector containing the randomly generated values. To learn more about these functions, type ? followed the function in the console. Exercise: Use the built-in functions for the binomial distribution to plot the pmf of \\(X\\) from the previous example. Also, use the built-in functions to compute the probabilities from the example. Figure 6.1 gf_dist(&quot;binom&quot;, size = 20, prob = 0.5) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;X&quot;, y = &quot;P(X = x)&quot;) FIGURE 6.1: The pmf of a binomial random variable ###P(X=8) dbinom(8, 20, 0.5) [1] 0.1201344 ###P(X&lt;=8) pbinom(8, 20, 0.5) [1] 0.2517223 ## or sum(dbinom(0:8, 20, 0.5)) [1] 0.2517223 6.2.5 Poisson distribution The Poisson distribution is very common when considering count or arrival data. Consider a random process where events occur according to some rate over time (think arrivals to a retail register). Often, these events are modeled with the Poisson process. The Poisson process assumes a consistent rate of arrival and a memoryless arrival process (the time until the next arrival is independent of time since the last arrival). If we assume a particular process is a Poisson process, then there are two random variables that take common named distributions. The number of arrivals in a specified amount of time follows the Poisson distribution. Also, the amount of time until the next arrival follows the exponential distribution. We will defer discussion of the exponential distribution until the next chapter. What is random in the Poisson is the number of occurrences while the interval is fixed. That is why it is a discrete distribution. The parameter \\(\\lambda\\) is the average number of occurrences in the specific interval, note that the interval must be the same as is specified in the random variable. Let \\(X\\) be the number of arrivals in a length of time, \\(T\\), where arrivals occur according to a Poisson process with an average of \\(\\lambda\\) arrivals in length of time \\(T\\). Then \\(X\\) follows a Poisson distribution with parameter \\(\\lambda\\): \\[ X\\sim \\textsf{Poisson}(\\lambda) \\] The pmf of \\(X\\) is given by: \\[ f_X(x)=\\mbox{P}(X=x)=\\frac{\\lambda^xe^{-\\lambda}}{x!}, \\hspace{0.5cm} x=0,1,2,... \\] One unique feature of the Poisson distribution is that \\(\\mbox{E}(X)=\\mbox{Var}(X)=\\lambda\\). Example: Suppose fleet vehicles arrive to a maintenance garage at an average rate of 0.4 per day. Let’s assume that these vehicles arrive according to a Poisson process. Let \\(X\\) be the number of vehicles that arrive to the garage in a week (7 days). Notice that the time interval has changed! What is the random variable \\(X\\)? What is the distribution (with parameter) of \\(X\\). What are \\(\\mbox{E}(X)\\) and \\(\\mbox{Var}(X)\\)? Find \\(\\mbox{P}(X=0)\\), \\(\\mbox{P}(X\\leq 6)\\), \\(\\mbox{P}(X \\geq 2)\\), and \\(\\mbox{P}(2 \\leq X \\leq 8)\\). Also, find the median of \\(X\\), and the 95th percentile of \\(X\\) (the value of \\(x\\) such that \\(\\mbox{P}(X\\leq x)\\geq 0.95\\)). Further, plot the pmf of \\(X\\). Since vehicles arrive according to a Poisson process, the probability question leads us to define the random variable \\(X\\) as The number of vehicles that arrive in a week. We know that \\(X\\sim \\textsf{Poisson}(\\lambda=0.4*7=2.8)\\). Thus, \\(\\mbox{E}(X)=\\mbox{Var}(X)=2.8\\). The parameter is the average number of vehicles that arrive in a week. \\[ \\mbox{P}(X=0)=\\frac{2.8^0 e^{-2.8}}{0!}=e^{-2.8}=0.061 \\] Alternatively, we can use the built-in R functions for the Poisson distribution: ##P(X=0) dpois(0, 2.8) [1] 0.06081006 ##P(X&lt;=6) ppois(6, 2.8) [1] 0.9755894 ## or sum(dpois(0:6, 2.8)) [1] 0.9755894 ##P(X&gt;=2)=1-P(X&lt;2)=1-P(X&lt;=1) 1 - ppois(1, 2.8) [1] 0.7689218 ## or sum(dpois(2:1000, 2.8)) [1] 0.7689218 Note that when considering \\(\\mbox{P}(X\\geq 2)\\), we recognize that this is equivalent to \\(1-\\mbox{P}(X\\leq 1)\\). We can use ppois() to find this probability. When considering \\(\\mbox{P}(2\\leq X \\leq 8)\\), we need to make sure we formulate this correctly. Below are two possible methods: ##P(2 &lt;= X &lt;= 8) = P(X &lt;= 8)-P(X &lt;= 1) ppois(8, 2.8) - ppois(1, 2.8) [1] 0.766489 ## or sum(dpois(2:8, 2.8)) [1] 0.766489 To find the median and the 95th percentiles, we use qpois: qpois(0.5, 2.8) [1] 3 qpois(0.95, 2.8) [1] 6 Figure 6.2 is a plot of the pmf of a Poisson random variable. gf_dist(&quot;pois&quot;, lambda = 2.8) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;X&quot;, y = &quot;P(X = x)&quot;) FIGURE 6.2: The pmf of a Poisson random variable. Figure 6.3 is the cdf of the same Poisson random variable in Figure 6.2. gf_dist(&quot;pois&quot;, lambda = 2.8, kind = &quot;cdf&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;X&quot;, y = &quot;P(X &lt;= x)&quot;) FIGURE 6.3: The cdf of the Poisson random variable in Figure 6.2 6.2.6 Hypergeometric Consider an experiment where \\(k\\) objects are to be selected from a larger, but finite, group consisting of \\(m\\) “successes” and \\(n\\) “failures”. This is similar to the binomial process; after all, we are selecting successes and failures. However, in this case, the results are effectively selected without replacement. If the random variable \\(X\\) represents the number of successes selected in our sample of size \\(k\\), then \\(X\\) follows a hypergeometric distribution with parameters \\(m\\), \\(n\\), and \\(k\\). The pmf of \\(X\\) is given by: \\[ f_X(x) = \\frac{{m \\choose{x}}{n \\choose{k-x}}}{{m+n \\choose{k}}}, \\qquad x = 0,1,...,m \\] Also, \\(\\mbox{E}(X)=\\frac{km}{m+n}\\) and \\(\\mbox{Var}(X)=k\\frac{m}{m+n}\\frac{n}{m+n}\\frac{m+n-k}{m+n-1}\\) If you draw on your knowledge of combinations, you can see why this pmf makes sense. Example: Suppose a bag contains 12 red chips and 8 black chips. I reach in blindly and randomly select 6 chips. What is the probability I select no black chips? All black chips? Between 2 and 5 black chips? First we should identify a random variable that will help us with this problem. Let \\(X\\) be the number of black chips selected when randomly selecting 6 from the bag. Then \\(X\\sim \\textsf{HyperGeom}(8,12,6)\\). We can use R to find these probabilities. First, the plot of the pmf of the hypergeometric is in Figure 6.4. gf_dist(&quot;hyper&quot;, m = 8, n = 12, k = 6) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;X&quot;, y = &quot;P(X = x)&quot;) FIGURE 6.4: The pmf of a hypergeometric random variable. ##P(X=0) dhyper(0, 8, 12, 6) [1] 0.02383901 ##P(X=6) dhyper(6, 8, 12, 6) [1] 0.0007223942 ##P(2 &lt;= X &lt;=5) sum(dhyper(2:5, 8, 12, 6)) [1] 0.8119711 6.3 Homework Problems For each of the problems below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question. We will demonstrate using 1a and 1b. The T-6 training aircraft is used during UPT. Suppose that on each training sortie, aircraft return with a maintenance-related failure at a rate of 1 per 100 sorties. Find the probability of no maintenance failures in 15 sorties. \\(X\\): the number of maintenance failures in 15 sorties. \\(X\\sim \\textsf{Bin}(n=15,p=0.01)\\) \\(\\mbox{E}(X)=15*0.01=0.15\\) and \\(\\mbox{Var}(X)=15*0.01*0.99=0.1485\\). \\(\\mbox{P}(\\mbox{No mainteance failures})=\\mbox{P}(X=0)={15\\choose 0}0.01^0(1-0.01)^{15}=0.99^{15}\\) 0.99^15 [1] 0.8600584 ## or dbinom(0, 15, 0.01) [1] 0.8600584 This probability makes sense, since the expected value is fairly low. Because, on average, only 0.15 failures would occur every 15 trials, 0 failures would be a very common result. Graphically, the pmf looks like Figure 6.5. gf_dist(&quot;binom&quot;, size = 15, prob = 0.01) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x = &quot;X&quot;, y = &quot;P(X = x)&quot;) FIGURE 6.5: The pmf for binomail in Homework Problem 1a. Find the probability of at least two maintenance failures in 15 sorties. We can use the same \\(X\\) as above. Now, we are looking for \\(\\mbox{P}(X\\geq 2)\\). This is equivalent to finding \\(1-\\mbox{P}(X\\leq 1)\\): ## Directly 1 - (0.99^15 + 15*0.01*0.99^14) [1] 0.009629773 ## or, using R sum(dbinom(2:15, 15, 0.01)) [1] 0.009629773 ## or 1 - sum(dbinom(0:1, 15, 0.01)) [1] 0.009629773 ## or 1 - pbinom(1, 15, 0.01) [1] 0.009629773 ## or pbinom(1, 15, 0.01, lower.tail = F) [1] 0.009629773 Find the probability of at least 30 successful (no mx failures) sorties before the first failure. Find the probability of at least 50 successful sorties before the third failure. On a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour. Find the probability no vehicles arrive in 10 minutes. Find the probability at least 50 vehicles arrive in an hour. Find the probability that at least 5 minutes will pass before the next arrival. Suppose there are 12 male and 7 female cadets in a classroom. I select 5 completely at random (without replacement). Find the probability I select no female cadets. Find the probability I select more than 2 female cadets. Solutions Manual "],["7-CONTNNAMED.html", "Chapter 7 Named Continuous Distributions 7.1 Objectives 7.2 Continuous distributions 7.3 Homework Problems Solutions Manual", " Chapter 7 Named Continuous Distributions 7.1 Objectives Recognize when to use common continuous distributions (Uniform, Exponential, Gamma, Normal, Weibull, and Beta), identify parameters, and find moments. Use R to calculate probabilities and quantiles involving random variables with common continuous distributions. Understand the relationship between the Poisson process and the Poisson &amp; Exponential distributions. Know when to apply and then use the memory-less property. 7.2 Continuous distributions In this chapter we will explore continuous distributions. This means we work with probability density functions and use them to find probabilities. Thus we must integrate, either numerically, graphically, or mathematically. The cumulative distribution function will also play an important role in this chapter. There are many more distributions than the ones in this chapter but these are the most common and will set you up to learn and use any others in the future. 7.2.1 Uniform distribution The first continuous distribution we will discuss is the uniform distribution. By default, when we refer to the uniform distribution, we are referring to the continuous version. When referring to the discrete version, we use the full term “discrete uniform distribution.” A continuous random variable has the uniform distribution if probability density is constant, uniform. The parameters of this distribution are \\(a\\) and \\(b\\), representing the minimum and maximum of the sample space. This distribution is commonly denoted as \\(U(a,b)\\). Let \\(X\\) be a continuous random variable with the uniform distribution. This is denoted as \\(X\\sim \\textsf{Unif}(a,b)\\). The pdf of \\(X\\) is given by: \\[ f_X(x)=\\left\\{\\begin{array}{ll} \\frac{1}{b-a}, &amp; a\\leq x \\leq b \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] The mean of \\(X\\) is \\(\\mbox{E}(X)=\\frac{a+b}{2}\\) and the variance is \\(\\mbox{Var}(X)=\\frac{(b-a)^2}{12}\\). The derivation of the mean is left to the exercises. The most common uniform distribution is \\(U(0,1)\\) which we have already used several times in this book. Again, notice in Figure 7.1 that the plot of the pdf is a constant or uniform value. gf_dist(&quot;unif&quot;, title = &quot;Pdf of Uniform random variable&quot;, ylab = &quot;f(x)&quot;) %&gt;% gf_theme(theme_bw()) FIGURE 7.1: The pdf of Uniform random variable. To check that it is a proper pdf, all values must be non-negative and the total probability must be 1. In R the function for probability density will start with the letter d and have some short descriptor for the distribution. For the uniform we use dunif(). integrate(function(x)dunif(x), 0, 1) 1 with absolute error &lt; 1.1e-14 7.2.2 Exponential distribution Recall from the chapter on named discrete distributions, we discussed the Poisson process. If arrivals follow a Poisson process, we know that the number of arrivals in a specified amount of time follows a Poisson distribution, and the time until the next arrival follows the exponential distribution. In the Poisson distribution, the number of arrivals is random and the interval is fixed. In the exponential distribution we change this, the interval is random and the arrivals are fixed at 1. This is a subtle point but worth the time to make sure you understand. Let \\(X\\) be the number of arrivals in a time interval \\(T\\), where arrivals occur according to a Poisson process with an average of \\(\\lambda\\) arrivals per unit time interval. From the previous chapter, we know that \\(X\\sim \\textsf{Poisson}(\\lambda T)\\). Now let \\(Y\\) be the time until the next arrival. Then \\(Y\\) follows the exponential distribution with parameter \\(\\lambda\\) which has units of inverse base time: \\[ Y \\sim \\textsf{Expon}(\\lambda) \\] Note on \\(\\lambda\\): One point of confusion involving the parameters of the Poisson and exponential distributions. The parameter of the Poisson distribution (usually denoted as \\(\\lambda\\)) represents the average number of arrivals in whatever amount of time specified by the random variable. In the case of the exponential distribution, the parameter (also denoted as \\(\\lambda\\)) represents the average number of arrivals per unit time. For example, suppose arrivals follow a Poisson process with an average of 10 arrivals per day. \\(X\\), the number of arrivals in 5 days, follows a Poisson distribution with parameter \\(\\lambda=50\\), since that is the average number of arrivals in the amount of time specified by \\(X\\). Meanwhile, \\(Y\\), the time in days until the next arrival, follows an exponential distribution with parameter \\(\\lambda=10\\) (the average number of arrivals per day). The pdf of \\(Y\\) is given by: \\[ f_Y(y)=\\lambda e^{-\\lambda y}, \\hspace{0.3cm} y&gt;0 \\] The mean and variance of \\(Y\\) are: \\(\\mbox{E}(Y)=\\frac{1}{\\lambda}\\) and \\(\\mbox{Var}(Y)=\\frac{1}{\\lambda^2}\\). You should be able to derive these results but they require integration by parts and can be lengthy algebraic exercises. Example: Suppose at a local retail store, customers arrive to a checkout counter according to a Poisson process with an average of one arrival every three minutes. Let \\(Y\\) be the time (in minutes) until the next customer arrives to the counter. What is the distribution (and parameter) of \\(Y\\)? What are \\(\\mbox{E}(Y)\\) and \\(\\mbox{Var}(Y)\\)? Find \\(\\mbox{P}(Y&gt;5)\\), \\(\\mbox{P}(Y\\leq 3)\\), and \\(\\mbox{P}(1 \\leq Y &lt; 5)\\)? Also, find the median and 95th percentile of \\(Y\\). Finally, plot the pdf of \\(Y\\). Since one arrival shows up every three minutes, the average number of arrivals per unit time is 1/3 arrival per minute. Thus, \\(Y\\sim \\textsf{Expon}(\\lambda=1/3)\\). This means that \\(\\mbox{E}(Y)=3\\) and \\(\\mbox{Var}(Y)=9\\). To find \\(\\mbox{P}(Y&gt;5)\\), we could integrate the pdf of \\(Y\\): \\[ \\mbox{P}(Y&gt;5)=\\int_5^\\infty \\frac{1}{3}e^{-\\frac{1}{3}y}\\mathop{}\\!\\mathrm{d}y = \\lim_{a \\to +\\infty}\\int_5^a \\frac{1}{3}e^{-\\frac{1}{3}y}\\mathop{}\\!\\mathrm{d}y = \\] \\[ \\lim_{a \\to +\\infty} -e^{-\\frac{1}{3}y}\\bigg|_5^a=\\lim_{a \\to +\\infty} -e^{-\\frac{a}{3}}-(-e^{-\\frac{5}{3}})= 0 + 0.189 = 0.189 \\] Alternatively, we could use R: ##Prob(Y&gt;5)=1-Prob(Y&lt;=5) 1 - pexp(5, 1/3) [1] 0.1888756 Or using integrate() integrate(function(x)1/3*exp(-1/3*x), 5, Inf) 0.1888756 with absolute error &lt; 8.5e-05 For the remaining probabilities, we will use R: ##Prob(Y&lt;=3) pexp(3, 1/3) [1] 0.6321206 ##Prob(1&lt;=Y&lt;5) pexp(5, 1/3) - pexp(1, 1/3) [1] 0.5276557 The median is \\(y\\) such that \\(\\mbox{P}(Y\\leq y)=0.5\\). We can find this by solving the following for \\(y\\): \\[ \\int_0^y \\frac{1}{3}e^{-\\frac{1}{3}y}\\mathop{}\\!\\mathrm{d}y = 0.5 \\] Alternatively, we can use qexp in R: ##median qexp(0.5, 1/3) [1] 2.079442 ##95th percentile qexp(0.95, 1/3) [1] 8.987197 FIGURE 7.2: pdf of exponential random varible \\(Y\\) Both from Figure 7.2 and the mean and median, we know that the exponential distribution is skewed to the right. 7.2.3 Memory-less property The Poisson process is known for its memory-less property. Essentially, this means that the time until the next arrival is independent of the time since last arrival. Thus, the probability of an arrival within the next 5 minutes is the same regardless of whether an arrival just occurred or an arrival has not occurred for a long time. To show this let’s consider random variable \\(Y\\) ( time until the next arrival in minutes) where \\(Y\\sim\\textsf{Expon}(\\lambda)\\). We will show that, given it has been at least \\(t\\) minutes since the last arrival, the probability we wait at least \\(y\\) additional minutes is equal to the marginal probability that we wait \\(y\\) additional minutes. First, note that the cdf of \\(Y\\), \\(F_Y(y)=\\mbox{P}(Y\\leq y)=1-e^{-\\lambda y}\\), you should be able to derive this. So, \\[ \\mbox{P}(Y\\geq y+t|Y\\geq t) = \\frac{\\mbox{P}(Y\\geq y+t \\cap Y\\geq t)}{\\mbox{P}(Y\\geq t)}=\\frac{\\mbox{P}(Y\\geq y +t)}{\\mbox{P}(Y\\geq t)} = \\frac{1-(1-e^{-(y+t)\\lambda})}{1-(1-e^{-t\\lambda})} \\] \\[ =\\frac{e^{-\\lambda y }e^{-\\lambda t}}{e^{-\\lambda t }}=e^{-\\lambda y} = 1-(1-e^{-\\lambda y})=\\mbox{P}(Y\\geq y). \\blacksquare \\] Let’s simulate values for a Poisson. The Poisson is often used in modeling customer service situations such as service at Chipotle. However, some people have the mistaken idea that arrivals will be equally spaced. In fact, arrivals will come in clusters and bunches. Maybe this is the root of the common expression, “Bad news comes in threes”? FIGURE 7.3: Simulations of Poisson random variable. In Figure 7.3, the number of events in a box is \\(X\\sim \\textsf{Poisson}(\\lambda = 5)\\). As you can see, some boxes have more than 5 and some less because 5 is the average number of arrivals. Also note that the spacing is not equal. The 8 different runs are just repeated simulations of the same process. We can see spacing and clusters in each run. 7.2.4 Gamma distribution The gamma distribution is a generalization of the exponential distribution. In the exponential distribution, the parameter \\(\\lambda\\) is sometimes referred to as the rate parameter. The gamma distribution is sometimes used to model wait times (as with the exponential distribution), but in cases without the memory-less property. The gamma distribution has two parameters, rate and shape. In some texts, scale (the inverse of rate) is used as an alternative parameter to rate. Suppose \\(X\\) is a random variable with the gamma distribution with shape parameter \\(\\alpha\\) and rate parameter \\(\\lambda\\): \\[ X \\sim \\textsf{Gamma}(\\alpha,\\lambda) \\] \\(X\\) has the following pdf: \\[ f_X(x)=\\frac{\\lambda^\\alpha}{\\Gamma (\\alpha)}x^{\\alpha-1}e^{-\\lambda x}, \\hspace{0.3cm} x&gt;0 \\] and 0 otherwise. The mean and variance of \\(X\\) are \\(\\mbox{E}(X)=\\frac{\\alpha}{\\lambda}\\) and \\(\\mbox{Var}(X)=\\frac{\\alpha}{\\lambda^2}\\). Looking at the pdf, the mean and the variance, one can easily see that if \\(\\alpha=1\\), the resulting distribution is equivalent to \\(\\textsf{Expon}(\\lambda)\\). 7.2.4.1 Gamma function You may have little to no background with the Gamma function (\\(\\Gamma (\\alpha)\\)). This is different from the gamma distribution. The gamma function is simply a function and is defined by: \\[ \\Gamma (\\alpha)=\\int_0^\\infty t^{\\alpha-1}e^{-t}\\mathop{}\\!\\mathrm{d}t \\] There are some important properties of the gamma function. Notably, \\(\\Gamma (\\alpha)=(\\alpha-1)\\Gamma (\\alpha -1)\\), and if \\(\\alpha\\) is a non-negative integer, \\(\\Gamma(\\alpha)=(\\alpha-1)!\\). Suppose \\(X \\sim \\textsf{Gamma}(\\alpha,\\lambda)\\). The pdf of \\(X\\) for various values of \\(\\alpha\\) and \\(\\lambda\\) is shown in Figure 7.4. FIGURE 7.4: pdf of Gamma for various values of alpha and lambda Example: Let \\(X \\sim \\textsf{Gamma}(\\alpha=5,\\lambda=1)\\). Find the mean and variance of \\(X\\). Also, compute \\(\\mbox{P}(X\\leq 2)\\) and \\(\\mbox{P}(1\\leq X &lt; 8)\\). Find the median and 95th percentile of \\(X\\). The mean and variance of \\(X\\) are \\(\\mbox{E}(X)=5\\) and \\(\\mbox{Var}(X)=5\\). To find probabilities and quantiles, integration will be difficult, so it’s best to use the built-in R functions: ## Prob(X&lt;=2) pgamma(2, 5, 1) [1] 0.05265302 ##Prob(1 &lt;= X &lt; 8) pgamma(8, 5, 1) - pgamma(1, 5, 1) [1] 0.8967078 ## median qgamma(0.5, 5, 1) [1] 4.670909 ## 95th percentile qgamma(0.95, 5, 1) [1] 9.153519 7.2.5 Weibull distribution Another common distribution used in modeling is the Weibull distribution. Like the gamma, the Weibull distribution is a generalization of the exponential distribution and is meant to model wait times. A random variable with the Weibull distribution has parameters \\(\\alpha\\) and \\(\\beta\\). In R, these are referred to as shape and scale respectively. Note that in some resources, these are represented by \\(k\\) and \\(\\lambda\\) or even \\(k\\) and \\(\\theta\\). Let \\(X \\sim \\textsf{Weibull}(\\alpha,\\beta)\\). The pdf of \\(X\\) is given by: \\[ f_X(x)=\\frac{\\alpha}{\\beta} \\left(\\frac{x}{\\beta}\\right)^{\\alpha-1} e^{-\\left(\\frac{x}{\\beta}\\right)^\\alpha}, \\hspace{0.3cm} x\\geq 0 \\] The mean and variance of a random variable with a Weibull distribution can be found by consulting R documentation. Look them up and make sure you can use them. 7.2.6 Normal distribution The normal distribution (also referred to as Gaussian) is a common distribution found in natural processes. You have likely seen a bell curve in various contexts. The bell curve is often indicative of an underlying normal distribution. There are two parameters of the normal distribution: \\(\\mu\\) (the mean of \\(X\\)) and \\(\\sigma\\) (the standard deviation of \\(X\\)). Suppose a random variable \\(X\\) has a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). The pdf of \\(X\\) is given by: \\[ f_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\hspace{0.3cm} -\\infty &lt; x &lt;\\infty \\] Some plots of normal distributions for different parameters are plotted in Figure 7.5. FIGURE 7.5: pdf of Normal for various values of mu and sigma 7.2.6.1 Standard normal When random variable \\(X\\) is normally distributed with \\(\\mu=0\\) and \\(\\sigma=1\\), \\(X\\) is said to follow the standard normal distribution. Sometimes, the standard normal pdf is denoted by \\(\\phi(x)\\). Note that any normally distributed random variable can be transformed to have the standard normal distribution. Let \\(X \\sim \\textsf{Norm}(\\mu,\\sigma)\\). Then, \\[ Z=\\frac{X-\\mu}{\\sigma} \\sim \\textsf{Norm}(0,1) \\] Partially, one can show this is true by noting that the mean of \\(Z\\) is 0 and the variance (and standard deviation) of \\(Z\\) is 1: \\[ \\mbox{E}(Z)=\\mbox{E}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma}\\left(\\mbox{E}(X)-\\mu\\right)=\\frac{1}\\sigma(\\mu-\\mu)=0 \\] \\[ \\mbox{Var}(Z)=\\mbox{Var}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma^2}\\left(\\mbox{Var}(X)-0\\right)=\\frac{1}{\\sigma^2} \\sigma^2=1 \\] Note that this does not prove that \\(Z\\) follows the standard normal distribution; we have merely shown that \\(Z\\) has a mean of 0 and a variance of 1. We will discuss transformation of random variables in a later chapter. Example: Let \\(X \\sim \\textsf{Norm}(\\mu=200,\\sigma=15)\\). Compute \\(\\mbox{P}(X\\leq 160)\\), \\(\\mbox{P}(180\\leq X &lt; 230)\\), and \\(\\mbox{P}(X&gt;\\mu+\\sigma)\\). Find the median and 95th percentile of \\(X\\). As with the gamma distribution, to find probabilities and quantiles, integration will be difficult, so it’s best to use the built-in R functions: ## Prob(X&lt;=160) pnorm(160, 200, 15) [1] 0.003830381 ##Prob(180 &lt;= X &lt; 230) pnorm(230, 200, 15) - pnorm(180, 200, 15) [1] 0.8860386 ##Prob(X&gt;mu+sig) 1 - pnorm(215, 200, 15) [1] 0.1586553 ## median qnorm(0.5, 200, 15) [1] 200 ## 95th percentile qnorm(0.95, 200, 15) [1] 224.6728 7.2.7 Beta distribution The last common continuous distribution we will study is the beta distribution. This has a unique application in that the domain of a random variable with the beta distribution is \\([0,1]\\). Thus it is typically used to model proportions. The beta distribution has two parameters, \\(\\alpha\\) and \\(\\beta\\). (In R, these are denoted not so cleverly as shape1 and shape2.) Let \\(X \\sim \\textsf{Beta}(\\alpha,\\beta)\\). The pdf of \\(X\\) is given by: \\[ f_X(x)=\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}, \\hspace{0.3cm} 0\\leq x \\leq 1 \\] Yes, our old friend the Gamma function. In some resources, \\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\) is written as \\(\\frac{1}{B(\\alpha,\\beta)}\\), where \\(B\\) is known as the beta function. Note that \\(\\mbox{E}(X)=\\frac{\\alpha}{\\alpha+\\beta}\\) and \\(\\mbox{Var}(X)=\\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\). For various values \\(\\alpha\\) and \\(\\beta\\), the pdf of a beta distributed random variable is shown in Figure 7.6. FIGURE 7.6: pdf of Beta for various values of alpha and beta Exercise What is the distribution if \\(\\alpha=\\beta=1\\)? It is the uniform. It is easy to verify that \\(\\Gamma(1)=1\\) so that \\(B(1,1)=1\\). 7.3 Homework Problems For problems 1-3 below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question. On a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour. Find the probability no vehicles arrive in 10 minutes. Find the probability that at least 5 minutes will pass before the next arrival. Find the probability that the next vehicle will arrive between 2 and 10 minutes from now. Find the probability that at least 7 minutes will pass before the next arrival, given that 2 minutes have already passed. Compare this answer to part (b). This is an example of the memory-less property of the exponential distribution. Fill in the blank. There is a probability of 90% that the next vehicle will arrive within __ minutes. This value is known as the 90% percentile of the random variable. Use the function stripplot() to visualize the arrival of 30 vehicles using a random sample from the appropriate exponential distribution. Suppose time until computer errors on the F-35 follows a Gamma distribution with mean 20 hours and variance 10. Find the probability that 20 hours pass without a computer error. Find the probability that 45 hours pass without a computer error, given that 25 hours have already passed. Does the memory-less property apply to the Gamma distribution? Find \\(a\\) and \\(b\\) where there is a 95% probability time until next computer error will be between \\(a\\) and \\(b\\). (note: technically, there are many answers to this question, but find \\(a\\) and \\(b\\) such that each tail has equal probability.) Suppose PFT scores in the cadet wing follow a normal distribution with mean 330 and standard deviation 50. Find the probability a randomly selected cadet has a PFT score higher than 450. Find the probability a randomly selected cadet has a PFT score within 2 standard deviations of the mean. Find \\(a\\) and \\(b\\) such that 90% of PFT scores will be between \\(a\\) and \\(b\\). Find the probability a randomly selected cadet has a PFT score higher than 450 given he/she is among the top 10% of cadets. Let \\(X \\sim \\textsf{Beta}(\\alpha=1,\\beta=1)\\). Show that \\(X\\sim \\textsf{Unif}(0,1)\\). Hint: write out the beta distribution pdf where \\(\\alpha=1\\) and \\(\\beta=1\\). When using R to calculate probabilities related to the gamma distribution, we often use pgamma. Recall that pgamma is equivalent to the cdf of the gamma distribution. If \\(X\\sim\\textsf{Gamma}(\\alpha,\\lambda)\\), then \\[ \\mbox{P}(X\\leq x)=\\textsf{pgamma(x,alpha,lambda)} \\] The dgamma function exists in R too. In plain language, explain what dgamma returns. I’m not looking for the definition found in R documentation. I’m looking for a simple description of what that function returns. Is the output of dgamma useful? If so, how? Advanced. You may have heard of the 68-95-99.7 rule. This is a helpful rule of thumb that says if a population has a normal distribution, then 68% of the data will be within one standard deviation of the mean, 95% of the data will be within two standard deviations and 99.7% of the data will be within three standard deviations. Create a function in R that has two inputs (a mean and a standard deviation). It should return a vector with three elements: the probability that a randomly selected observation from the normal distribution with the inputted mean and standard deviation lies within one, two and three standard deviations. Test this function with several values of the mu and sd. You should get the same answer each time. Derive the mean of a general uniform distribution, \\(U(a,b)\\). Solutions Manual "],["8-CS1.html", "Chapter 8 Data Case Study 8.1 Introduction to Data: Unleashing the Power of Information in R 8.2 Case Study 8.3 Homework Problems Solutions Manual", " Chapter 8 Data Case Study 8.1 Introduction to Data: Unleashing the Power of Information in R Welcome to the world of data! In today’s information-driven age, data is the lifeblood of decision-making, problem-solving, and understanding the world around us. Whether you’re an aspiring data scientist, a business analyst, a researcher, or simply curious about the role of data in our lives, this module will introduce you to the fundamental concepts and tools needed to harness the immense potential of data. But first, let’s address a fundamental question: Why do we need data to answer questions? Data is not just numbers, text, or images; it’s a powerful tool that helps us unravel mysteries, make informed choices, and gain insights into the complex phenomena that shape our world. Here are a few reasons why data is indispensable for answering questions: Evidence-Based Decision Making: Data provides concrete evidence that allows us to make decisions based on facts rather than intuition or guesswork. Whether it’s optimizing business operations, developing public policies, or even making personal choices, data-driven decisions are more likely to lead to success. Pattern Recognition: Data allows us to identify patterns, trends, and relationships within vast and complex datasets. By analyzing data, we can detect hidden correlations and gain a deeper understanding of how various factors interact. Validation and Testing: Data allows us to test hypotheses and validate our assumptions. It helps us confirm or refute ideas, theories, or claims, ensuring that our conclusions are grounded in empirical evidence. Predictive Modeling: With data, we can build models that forecast future outcomes based on historical data. This predictive power is invaluable in fields like finance, healthcare, and climate science, where anticipating future trends is crucial. Continuous Improvement: Data facilitates ongoing improvement and optimization. By tracking performance metrics and collecting feedback, organizations and individuals can continually refine their processes and make incremental progress. Informed Communication: Data provides a common language for communication and collaboration. When discussing complex topics or presenting information to others, data helps convey ideas more clearly and objectively. Throughout this module, you’ll learn how to work with data using the R programming language, a versatile tool widely used for data analysis and visualization. You’ll explore data collection, exploration, wrangling, and visualization techniques, equipping you with the skills to extract valuable insights and answer questions using data. 8.2 Case Study Before we dive into principles of data, data collection, visualization and exploration, we will walk through an introductory “case study”. The idea is to provide a preview of what we’ll cover in this section. You are not expected to understand much of the execution at this time, but you should be able to follow along with the example. All concepts and tools will be covered more deeply in the subsequent chapters. We will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke. 6 7 Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer: 8.2.1 Research question Does the use of stents reduce the risk of stroke? 8.2.2 Collect the relevant data The researchers who asked this question collected data on 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups: Treatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification. Control group. Patients in the control group received the same medical management as the treatment group but did not receive stents. Researchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group. This is an experiment and not an observational study. We will learn more about these ideas in this block. Researchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment. 8.2.3 Import data We begin our first use of R. If you need to install a package, most likely it will be on CRAN, the Comprehensive R Archive Network. Before a package can be used, it must be installed on the computer (once per computer or account) and loaded into a session (once per R session). When you exit R, the package stays installed on the computer but will not be reloaded when R is started again. In summary, R has packages that can be downloaded and installed from online repositories such as CRAN. When you install a package, which only needs to be done once per computer or account, in R all it is doing is placing the source code in a library folder designated during the installation of R. Packages are typically collections of functions and variables that are specific to a certain task or subject matter. For example, to install the mosaic package, enter: install.packages(&quot;mosaic&quot;) # fetch package from CRAN In RStudio, there is a Packages tab that makes it easy to add and maintain packages. To use a package in a session, we must load it. This makes it available to the current session only. When you start R again, you will have to load packages again. The command library() with the package name supplied as the argument is all that is needed. For this session, we will load tidyverse and mosaic. Note: the box below is executing the R commands, this is known as reproducible research since you can see the code and then you can run or modify as you need. library(tidyverse) Next read in the data into the working environment. stent_study &lt;- read_csv(&quot;data/stent_study.csv&quot;) Let’s break this code down. We are reading from a .csv file and assigning the results into an object called stent_study. The assignment arrow &lt;- means we assign what is on the right to what is on the left. The R function we use in this case is read_csv(). When using R functions, you should ask yourself: What do I want R to do? What information must I provide for R to do this? We want R to read in a .csv file. We can get help on this function by typing ?read_csv or help(read_csv) at the prompt. The only required input to read_csv() is the file location. We have our data stored in a folder called “data” under the working directory. We can determine the working directory by typing getwd() at the prompt. getwd() Similarly, if we wish to change the working directory, we can do so by using the setwd() function: setwd(&#39;C:/Users/Brianna.Hitt/Documents/ProbStat/Another Folder&#39;) In R if you use the view(), you will see the data in what looks like a standard spreadsheet. View(stent_study) 8.2.4 Explore data Before we attempt to answer the research question, let’s look at the data. We want R to print out the first 10 rows of the data. The appropriate function is head() and it needs the data object. By default, R will output the first 6 rows. By using the n = argument, we can specify how many rows we want to view. head(stent_study, n = 10) # A tibble: 10 × 3 group outcome30 outcome365 &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 control no_event no_event 2 trmt no_event no_event 3 control no_event no_event 4 trmt no_event no_event 5 trmt no_event no_event 6 control no_event no_event 7 trmt no_event no_event 8 control no_event no_event 9 control no_event no_event 10 control no_event no_event We also want to “glimpse” the data. The function is glimpse() and R needs the data object stent_study. glimpse(stent_study) Rows: 451 Columns: 3 $ group &lt;chr&gt; &quot;control&quot;, &quot;trmt&quot;, &quot;control&quot;, &quot;trmt&quot;, &quot;trmt&quot;, &quot;control&quot;, &quot;t… $ outcome30 &lt;chr&gt; &quot;no_event&quot;, &quot;no_event&quot;, &quot;no_event&quot;, &quot;no_event&quot;, &quot;no_event&quot;,… $ outcome365 &lt;chr&gt; &quot;no_event&quot;, &quot;no_event&quot;, &quot;no_event&quot;, &quot;no_event&quot;, &quot;no_event&quot;,… To keep things simple, we will only look at the outcome30 variable in this case study. We will summarize the data in a table. We will learn to do this using the tidyverse package. We want to summarize the data by making a table. Using tidyverse, we would like to count the number of observations broken down by group and outcome30. The basic format is: stent_study %&gt;% group_by(group,outcome30)%&gt;% summarize(total=n()) # A tibble: 4 × 3 # Groups: group [2] group outcome30 total &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 control no_event 214 2 control stroke 13 3 trmt no_event 191 4 trmt stroke 33 Of the 224 patients in the treatment group, 33 had a stroke by the end of the first month. Using these two numbers, we can use R to compute the proportion of patients in the treatment group who had a stroke by the end of their first month. 33 / (33 + 191) [1] 0.1473214 Let’s have R calculate proportions for us. Starting with the table above, we want to divide each count by the total, broken down by group. So, we need to group by the group variable and divide: stent_study %&gt;% group_by(group,outcome30)%&gt;% summarize(total=n())%&gt;% group_by(group)%&gt;% mutate(prop = total/sum(total)) # A tibble: 4 × 4 # Groups: group [2] group outcome30 total prop &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 control no_event 214 0.943 2 control stroke 13 0.0573 3 trmt no_event 191 0.853 4 trmt stroke 33 0.147 These tables present summary statistics. A summary statistic is a single number summarizing a large amount of data.8 For instance, the primary results of the study after 1 month could be described by two summary statistics: the proportion of people who had a stroke in the treatment group and the proportion of people who had a stroke in the control group. Proportion who had a stroke in the treatment (stent) group: \\(33/224 = 0.15 = 15\\%\\) Proportion who had a stroke in the control group: \\(13/227 = 0.06 = 6\\%\\) 8.2.5 Visualize the data It is often important to visualize the data. The table is a type of visualization, but in this section we will introduce a graphical method called bar charts. We will use the ggplot2 package to visualize the data. This package is becoming the industry standard for generating professional graphics. The interface for ggplot2 can seem difficult at first, but we will ease into it. The ggplot2 package was loaded when we loaded tidyverse. To generate a basic graphic, we need to ask ourselves what information we are trying to see, what particular type of graph is best, what corresponding R function to use, and what information that R function needs in order to build a plot. For categorical data, we want a bar chart and the ggplot2 function geom_bar(). Inside the aes() argument, we will specify our variable(s) of interest. Here is our first attempt. In Figure 8.1, we leave the y portion of our formula blank. Doing this implies that we simply want to view the number/count of outcome30 by type. We will see the two levels of outcome30 on the x-axis and counts on the y-axis. stent_study%&gt;% ggplot(aes(x=outcome30))+ geom_bar() FIGURE 8.1: Using ggplot2 to create a bar chart. Exercise: Explain Figure 8.1. This plot graphically shows us the total number of “stroke” and the total number of “no_event”. However, this is not what we want. We want to compare the 30-day outcomes for both treatment groups. So, we need to break the data into different groups based on treatment type. In ggplot this means “faceting” your plot: stent_study%&gt;% ggplot(aes(x=outcome30))+ geom_bar()+ facet_wrap(~group) FIGURE 8.2: Bar charts conditioned on the group variable. 8.2.5.1 More advanced graphics As a prelude for things to come, the above graphic needs work. The labels don’t help and there is no title. We could add color. Does it make more sense to use proportions? Here is the code and results for a better graph, see Figure 8.3. Don’t worry if this seems a bit advanced, but feel free to examine each new component of this code. stent_study%&gt;% ggplot(aes(x=group,fill=outcome30))+ geom_bar(position = &quot;fill&quot;)+ labs(title = &quot;Impact of Stents on Stroke&quot;, subtitle = &quot;Experiment with 451 Patients&quot;, x = &quot;Experimental Group&quot;, y = &quot;Number of Events&quot;)+ theme_bw() FIGURE 8.3: Better graph. Notice throughout the above, we used the pipe operator, %&gt;%. This operator allows us to string functions together in a manner that makes it easier to read the code. In the above code, we are sending the data object stent_study into the function gf_props() to use as data, so we don’t need the data = argument. In math, this is a composition of functions. Instead of f(g(x)) we could use a pipe f(g(x)) = g(x) %&gt;% f(). 8.2.6 Conclusion These two summary statistics (the proportions of people who had a stroke) are useful in looking for differences in the groups, and we are in for a surprise: an additional 9% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a real difference due to the treatment? This second question is subtle. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 9% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance? This is a preview of inferential statistics, a subsequent section of this text. While we haven’t yet covered statistical tools to fully address these steps, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients. Be careful: do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises. 8.3 Homework Problems Stent study continued. Complete a similar analysis for the stent data, but this time use the one year outcome. In particular, Read the data into your working directory. stent_study &lt;- read_csv(___) Complete the steps below. The start of code is provided below. You will need to add {r} to the start of each code chunk or insert your own code chunks to use the code. i. Use `glimpse()` on the data. glimpse(___) ii. Create a table of `outcome365` and `group`. Comment on the results. stent_study %&gt;% group_by(group,_________)%&gt;% summarize(total=n()) iii. Create a barchart of the data. stent_study%&gt;% ggplot(aes(x = _________, fill = ________))+ geom_bar(position = &quot;fill&quot;)+ labs(title = &quot;__________&quot;, subtitle = &quot;__________&quot;, x = &quot;__________&quot;, y = &quot;__________&quot;) Migraine and acupuncture. A migraine is a particularly painful type of headache, which patients sometimes wish to treat with acupuncture. To determine whether acupuncture relieves migraine pain, researchers conducted a randomized controlled study where 89 females diagnosed with migraine headaches were randomly assigned to one of two groups: treatment or control. The 43 patients in the treatment group received acupuncture that is specifically designed to treat migraines. The 46 patients in the control group received placebo acupuncture (needle insertion at nonacupoint locations). Then 24 hours after patients received acupuncture, they were asked if they were pain free.9 The data is in the file migraine_study.csv in the data folder. Complete the following work: Read the data into an object called migraine_study. migraine_study &lt;- read_csv(&quot;data/___&quot;) head(migraine_study) Create a table of the data. Report the percent of patients in the treatment group who were pain free 24 hours after receiving acupuncture. Repeat for the control group. At first glance, does acupuncture appear to be an effective treatment for migraines? Explain your reasoning. Do the data provide convincing evidence that there is a real pain reduction for those patients in the treatment group? Or do you think that the observed difference might just be due to chance? Solutions Manual Chimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Medical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993-1003.↩︎ NY Times article reporting on the study: http://www.nytimes.com/2011/09/08/health/research/08stent.html↩︎ Formally, a summary statistic is a value computed from the data. Some summary statistics are more useful than others.↩︎ G. Allais et al. “Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints”. http://www.ncbi.nlm.nih.gov/pubmed/21533739 In: Neurological Sci. 32.1 (2011), pp. 173–175.↩︎ "],["9-DB.html", "Chapter 9 Data Basics 9.1 Introduction to Data: Unleashing the Power of Information in R 9.2 The Data Analysis Process 9.3 Data Frames in R Solutions Manual", " Chapter 9 Data Basics 9.1 Introduction to Data: Unleashing the Power of Information in R Welcome to the world of data! In today’s information-driven age, data is the lifeblood of decision-making, problem-solving, and understanding the world around us. Whether you’re an aspiring data scientist, a business analyst, a researcher, or simply curious about the role of data in our lives, this module will introduce you to the fundamental concepts and tools needed to harness the immense potential of data. But first, let’s address a fundamental question: Why do we need data to answer questions? Data is not just numbers, text, or images; it’s a powerful tool that helps us unravel mysteries, make informed choices, and gain insights into the complex phenomena that shape our world. Here are a few reasons why data is indispensable for answering questions: Evidence-Based Decision Making: Data provides concrete evidence that allows us to make decisions based on facts rather than intuition or guesswork. Whether it’s optimizing business operations, developing public policies, or even making personal choices, data-driven decisions are more likely to lead to success. Pattern Recognition: Data allows us to identify patterns, trends, and relationships within vast and complex datasets. By analyzing data, we can detect hidden correlations and gain a deeper understanding of how various factors interact. Validation and Testing: Data allows us to test hypotheses and validate our assumptions. It helps us confirm or refute ideas, theories, or claims, ensuring that our conclusions are grounded in empirical evidence. Predictive Modeling: With data, we can build models that forecast future outcomes based on historical data. This predictive power is invaluable in fields like finance, healthcare, and climate science, where anticipating future trends is crucial. Continuous Improvement: Data facilitates ongoing improvement and optimization. By tracking performance metrics and collecting feedback, organizations and individuals can continually refine their processes and make incremental progress. Informed Communication: Data provides a common language for communication and collaboration. When discussing complex topics or presenting information to others, data helps convey ideas more clearly and objectively. Throughout this module, you’ll learn how to work with data using the R programming language, a versatile tool widely used for data analysis and visualization. You’ll explore data collection, exploration, wrangling, and visualization techniques, equipping you with the skills to extract valuable insights and answer questions using data. 9.2 The Data Analysis Process The following steps make up the Data Analysis process, a general roadmap for using data for any of the purposes described in the prior section. Identify a research question or problem. Collect relevant data on the topic. Explore and understand the data. Analyze the data. Form a conclusion. Make decisions based on the conclusion. This may seem reminiscent of the scientific process, and this is not a coincidence. Please note that the first step is to identify the problem. This cannot be understated. It is unfortunately too common for analysis to start at the second step, and a great deal of work is done to analyze data without clearly understanding why we are interested in this data in the first place. In this module, we will focus on steps 1-4. In the Statistical Inference and Regression modules, we will focus on steps 3-5. In these modules, we will discuss how to extend results of our exploration and analysis to a broader population. Traditional methods for inference and regression are rooted in mathematical theory, while modern methods utilize simulation. These methods all start with conducting exploration and visualization using the methods we’ll learn in this module. 9.3 Data Frames in R In this module, we will use R and various packages (primarily tidyverse) for the exploration and analysis of data. At this point, we will assume that you already have R and RStudio installed. We also assume that you are familiar with installing and loading packages. If you need a refresher, see the R installation guide. 9.3.1 Two Ways of Loading Data All of the data that we will use in the module will either be loaded via an R package or read in from a file. For example, the ncbirths data frame comes from the openintro package. This means if you haven’t already, you’ll need to install the package: install.package(&quot;openintro&quot;) Next, you’ll need to load it: library(openintro) Loading required package: airports Loading required package: cherryblossom Loading required package: usdata Attaching package: &#39;openintro&#39; The following object is masked from &#39;package:mosaic&#39;: dotPlot The following objects are masked from &#39;package:lattice&#39;: ethanol, lsegments Now that the package is loaded, ncbirths is ready to use. Because it is part of an R package, it comes with helpful documentation, accessed with ?ncbirths. This documentation contains helpful information about the origin of the data and types of data included: ?ncbirths We will also load data by reading from a file. For example, we have the stent_study.csv file contained in a folder called data. We can use the read_csv() function to load it and assign it to the object stent_study. stent_study &lt;- read_csv(&quot;data/stent_study.csv&quot;) You can access this .csv file via the module documentation. When you use read_csv(), you will need to replace data with the appropriate path for your computer. Note that because we loaded this data from a file, no embedded documentation exists: ?stent_study No documentation for &#39;stent_study&#39; in specified packages and libraries: you could try &#39;??stent_study&#39; 9.3.2 Exploring a Data Frame Once we have access to a data frame, we should explore it. ALWAYS take the time to get familiar with the structure of the data frame prior to conducting further analysis. You should have some idea of the research question prior to this exploration. Some useful tools are described below: head(): displays the first 6 rows of a data frame. Note that you can add a n = argument to change the number of rows displayed. View(): opens a tab displaying the data frame in spreadsheet format. str(): displays a list of variables (columns) along with how they are stored and some example values. glimpse(): from tidyverse; similar to str() but slightly cleaner. summary(): displays numerical summaries of variables based on the respective variable types. This is by no means an exhaustive list. Take some time to explore these tools on your own. Data frames are typically arranged with cases or observational units as the rows and variables as the columns. cases/observational units: individual experimental units in a data frame variables: characteristics collected on cases/observational units For example, consider the ncbirths data frame: head(ncbirths) # A tibble: 6 × 13 fage mage mature weeks premie visits marital gained weight lowbirthweight &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; 1 NA 13 younger … 39 full … 10 not ma… 38 7.63 not low 2 NA 14 younger … 42 full … 15 not ma… 20 7.88 not low 3 19 15 younger … 37 full … 11 not ma… 38 6.63 not low 4 21 15 younger … 41 full … 6 not ma… 34 8 not low 5 NA 15 younger … 39 full … 9 not ma… 27 6.38 not low 6 NA 15 younger … 38 full … 19 not ma… 22 5.38 low # ℹ 3 more variables: gender &lt;fct&gt;, habit &lt;fct&gt;, whitemom &lt;fct&gt; str(ncbirths) tibble [1,000 × 13] (S3: tbl_df/tbl/data.frame) $ fage : int [1:1000] NA NA 19 21 NA NA 18 17 NA 20 ... $ mage : int [1:1000] 13 14 15 15 15 15 15 15 16 16 ... $ mature : Factor w/ 2 levels &quot;mature mom&quot;,&quot;younger mom&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ weeks : int [1:1000] 39 42 37 41 39 38 37 35 38 37 ... $ premie : Factor w/ 2 levels &quot;full term&quot;,&quot;premie&quot;: 1 1 1 1 1 1 1 2 1 1 ... $ visits : int [1:1000] 10 15 11 6 9 19 12 5 9 13 ... $ marital : Factor w/ 2 levels &quot;not married&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ gained : int [1:1000] 38 20 38 34 27 22 76 15 NA 52 ... $ weight : num [1:1000] 7.63 7.88 6.63 8 6.38 5.38 8.44 4.69 8.81 6.94 ... $ lowbirthweight: Factor w/ 2 levels &quot;low&quot;,&quot;not low&quot;: 2 2 2 2 2 1 2 1 2 2 ... $ gender : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 1 2 1 2 2 2 2 1 ... $ habit : Factor w/ 2 levels &quot;nonsmoker&quot;,&quot;smoker&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ whitemom : Factor w/ 2 levels &quot;not white&quot;,&quot;white&quot;: 1 1 2 2 1 1 1 1 2 2 ... Each row in this data frame represents an observational unit or a case. In this example, a case is a birth. Each column represents a variable. There are 13 variables in this data frame, containing information about births like father’s age, mother’s age, maturity status of the mother, etc. 9.3.3 Tidy Data In this course, we will usually interact with data frames that are tidy. Tidy data is a structured and standardized way of organizing and formatting data sets so that they are easy to work with, analyze, and visualize. The core idea behind tidy data is to make data more understandable and user-friendly by adhering to a set of principles for data organization. The key principles of tidy data are as follows: Each variable forms a column: In tidy data, each variable (e.g., a measurement, attribute, or characteristic) is represented as a separate column in the data frame or table. This makes it clear which variables are being measured or observed. Each case or observational unit forms a row: Each row in the data represents a single observation or case. Observations that share common attributes or characteristics are grouped together in rows. Each type of observational unit forms a table: Tidy data is often organized into separate tables for each type of observational unit or data source. This helps maintain clear relationships between different parts of the data. Tidy data is considered good for several reasons: Readability and understandability: Tidy data makes it easier for analysts and data scientists to understand the structure of the data and the relationships between variables. This standard format reduces ambiguity and confusion. Compatibility with data tools: Tidy data is compatible with a wide range of data manipulation and visualization tools, such as R’s tidyverse packages (e.g., dplyr and ggplot2) and Python’s pandas library. This compatibility streamlines the data analysis workflow. Facilitates data analysis and visualization: Tidy data simplifies data analysis and visualization tasks because it aligns with the principles of tidyverse and similar libraries. Analysts can write more concise and readable code when the data is organized tidily. Supports data sharing and collaboration: Tidy data is suitable for sharing and collaborating on data analysis projects because it provides a clear and consistent data structure that others can easily understand and work with. Overall, tidy data promotes good data hygiene and helps ensure that data analysis is efficient, reproducible, and less error-prone. By adhering to the principles of tidy data, analysts can spend less time wrangling data and more time gaining insights from it. 9.3.4 Types of Variables Note in the ncbirthds data frame that the variables are stored differently from one another. First consider weight. It is said to be a numerical variable (sometimes called a quantitative variable) since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. The visits variable is also numerical; it is sensible to add, subtract, or take averages with those values, although it seems to be a little different than weight. This variable represents number of hospital visits during pregnancy and can only be a whole non-negative number (\\(0\\), \\(1\\), \\(2\\), \\(...\\)). For this reason, the visits variable is said to be discrete since it can only take specific numerical values. On the other hand, the birth weight variable is said to be continuous because it can take on any value in some interval. Now technically, there are no truly continuous numerical variables since all measurements are finite up to some level of accuracy or measurement precision (e.g., we recorded birth weights to the nearest hundredth of a pound). On the other hand, consider the marital variable. This variable is not numerical, and values are stored as “married” or “not married”. This is an example of a categorical variable. It does not make sense to add, subtract or average across categorical variables. The tools we use to summarize and visualize categorical data are often different from those we use for numerical data, so it is important for us to distinguish between the two. Note that sometimes, categorical data is stored numerically in a data frame. For example, telephone area codes are three digit numbers, but we would not classify a variable reporting telephone area codes as numerical; even though area codes are made up of numerical digits, their average, sum, and difference have no clear meaning. FIGURE 9.1: Taxonomy of Variables. Solutions Manual "],["10-ODCP.html", "Chapter 10 Overview of Data Collection Principles 10.1 Overview of data collection principles 10.2 Homework Problems Solutions Manual", " Chapter 10 Overview of Data Collection Principles 10.1 Overview of data collection principles In the last chapter, we briefly introduced data and some language around it. In short, data often are presented in data frames, where cases or observational units are presented in rows, and variables are presented in columns. Too often, much attention is given to analysis of existing data, but not enough attention is given to where the data came from, why it was collected, and what research question are we addressing. Over the next two chapters, we will introduce some data collection principles and some various types of random sampling. The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that they are reliable and help achieve the research goals. 10.1.1 Populations and samples Consider the following three research questions: What is the average mercury content in swordfish in the Atlantic Ocean? Over the last 5 years, what is the average time to complete a degree for Duke undergraduate students? Does a new drug reduce the number of deaths in patients with severe heart disease? Each research question refers to a target population, the entire collection of individuals about which we want information. In the first question, the target population is all swordfish in the Atlantic Ocean, and each fish represents a case. It is usually too expensive to collect data for every case in a population. Instead, a sample is taken. A sample represents a subset of the cases and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question. Exercise: For the second and third questions above, identify the target population and what represents an individual case.10 10.1.2 Anecdotal evidence Consider the following possible responses to the three research questions: A man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high. I met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges. My friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work. Each conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence. FIGURE 10.1: In February 2010, some media pundits cited one large snow storm as evidence against global warming. As comedian Jon Stewart pointed out, ‘It’s one storm, in one region, of one country.’ Anecdotal evidence: Be careful of data collected haphazardly. Such evidence may be true and verifiable, but it may only represent extraordinary cases. Anecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead of looking at the most unusual cases, we should examine a sample of many cases that represent the population. 10.1.3 Sampling from a population We might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates. This is illustrated in Figure 10.2. FIGURE 10.2: In this graphic, five graduates are randomly selected from the population to be included in the sample. Why pick a sample randomly? Why not just pick a sample by hand? Consider the following scenario. Example: Suppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think she might collect? Do you think her sample would be representative of all graduates? 11 FIGURE 10.3: Instead of sampling from all graduates equally, a nutrition major might inadvertently pick graduates with health-related majors disproportionately often. If someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that person’s interests, which may be entirely unintentional. This introduces sampling bias (see Figure 10.3), where some individuals in the population are more likely to be sampled than others. Sampling randomly helps resolve this problem. The most basic random sample is called a simple random sample, which is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample. Sometimes a simple random sample is difficult to implement and an alternative method is helpful. One such substitute is a systematic sample, where one case is sampled after letting a fixed number of others, say 10 other cases, pass by. Since this approach uses a mechanism that is not easily subject to personal biases, it often yields a reasonably representative sample. This book will focus on simple random samples since the use of systematic samples is uncommon and requires additional considerations of the context. The act of taking a simple random sample helps minimize bias. However, bias can crop up in other ways. Even when people are picked at random, e.g. for surveys, caution must be exercised if the non-response is high. For instance, if only 30% of the people randomly sampled for a survey actually respond, and it is unclear whether the respondents are representative12 of the entire population, the survey might suffer from non-response bias13. FIGURE 10.4: Due to the possibility of non-response, surveys studies may only reach a certain group within the population. It is difficult, and often impossible, to completely fix this problem Another common pitfall is a convenience sample, where individuals who are easily accessible are more likely to be included in the sample, see Figure 10.4 . For instance, if a political survey is done by stopping people walking in the Bronx, it will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents. Exercise: We can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product?14 10.1.4 Explanatory and response variables Consider the following question for the county data set: Is federal spending, on average, higher or lower in counties with high rates of poverty? If we suspect poverty might affect spending in a county, then poverty is the explanatory variable and federal spending is the response variable in the relationship.15 If there are many variables, it may be possible to consider a number of them as explanatory variables. Explanatory and response variables To identify the explanatory variable in a pair of variables, identify which of the two variables is suspected as explaining or causing changes in the other. In data sets with more than two variables, it is possible to have multiple explanatory variables. The response variable is the outcome or result of interest. Caution: Association does not imply causation. Labeling variables as explanatory and response does not guarantee the relationship between the two is actually causal, even if there is an association identified between the two variables. We use these labels only to keep track of which variable we suspect affects the other. We also use this language to help in our use of R and the formula notation. In some cases, there is no explanatory or response variable. Consider the following question: If homeownership in a particular county is lower than the national average, will the percent of multi-unit structures in that county likely be above or below the national average? It is difficult to decide which of these variables should be considered the explanatory and response variable; i.e. the direction is ambiguous, so no explanatory or response labels are suggested here. 10.1.5 Introducing observational studies and experiments There are two primary types of data collection: observational studies and experiments. Researchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a cohort16 of many similar individuals to study why certain diseases might develop. In each of these situations, researchers merely observe what happens. In general, observational studies can provide evidence of a naturally occurring association between variables, but by themselves, they cannot show a causal connection. When researchers want to investigate the possibility of a causal connection, they conduct an experiment, a study in which the explanatory variables are assigned rather than observed. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. The individuals in each group are assigned a treatment. When individuals are randomly assigned to a treatment group, and we are comparing at least two treatments, the experiment is called a randomized comparative experiment. For example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. The case study at the beginning of the book is another example of an experiment, though that study did not employ a placebo. Math 359 is a course on the design and analysis of experimental data, DOE, at USAFA. In the Air Force, these types of experiments are an important part of test and evaluation. Many Air Force analysts are expert practitioners of DOE. In this book though, we will minimize our discussion of DOE. Association \\(\\neq\\) Causation Again, association does not imply causation. In a data analysis, association does not imply causation, and causation can only be inferred from a randomized experiment. Although, a hot field is the analysis of causal relationships in observational data. This is important because consider cigarette smoking, how do we know it causes lung cancer? We only have observational data and clearly cannot do an experiment. We think analysts will be charged in the near future with using causal reasoning on observational data. 10.2 Homework Problems Generalizability and causality. Identify the population of interest and the sample in the studies described below. These are the same studies from the previous chapter. Also comment on whether or not the results of the study can be generalized to the population and if the findings of the study can be used to establish causal relationships. Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study, air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggests that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.17 The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: patients who practiced the Buteyko method and those who did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.18 GPA and study time. A survey was conducted on 193 undergraduates who took an introductory statistics course at a private US university in 2012. This survey asked them about their GPA and the number of hours they spent studying per week. The scatterplot below displays the relationship between these two variables. What is the explanatory variable and what is the response variable? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Is this an experiment or an observational study? Can we conclude that studying longer hours leads to higher GPAs? Income and education The scatterplot below shows the relationship between per capita income (in thousands of dollars) and percent of population with a bachelor’s degree in 3,143 counties in the US in 2010. What are the explanatory and response variables? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Can we conclude that having a bachelor’s degree increases one’s income? Solutions Manual 2) Notice that the second question is only relevant to students who complete their degree; the average cannot be computed using a student who never finished her degree. Thus, only Duke undergraduate students who have graduated in the last five years represent cases in the population under consideration. Each such student would represent an individual case. 3) A person with severe heart disease represents a case. The population includes all people with severe heart disease.↩︎ Perhaps she would pick a disproportionate number of graduates from health-related fields. Or perhaps her selection would be well-representative of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if that bias is unintentional or difficult to discern.↩︎ A representative sample accurately reflects the characteristics of the population.↩︎ Non-response bias is bias that can be introduced when subjects elect not to participate in a study. Often, the individuals that do participate are systematically different from the individuals who do not.↩︎ Answers will vary. From our own anecdotal experiences, we believe people tend to rant more about products that fell below expectations than rave about those that perform as expected. For this reason, we suspect there is a negative bias in product ratings on sites like Amazon. However, since our experiences may not be representative, we also keep an open mind.↩︎ Sometimes the explanatory variable is called the independent variable and the response variable is called the dependent variable. However, this becomes confusing since a pair of variables might be independent or dependent, so be careful and consider the context when using or reading these words.↩︎ A cohort is a group of individuals who are similar in some way.↩︎ B. Ritz et al. “Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993”. In: Epidemiology 11.5 (2000), pp. 502–511.↩︎ J. McGowan. “Health Education: Does the Buteyko Institute Method make a difference?” In: Thorax 58 (2003).↩︎ "],["11-STUDY.html", "Chapter 11 Studies 11.1 Observation studies, sampling strategies, and experiments 11.2 Homework Problems Solutions Manual", " Chapter 11 Studies 11.1 Observation studies, sampling strategies, and experiments We have introduced data and some language around data collection (population and sample, bias, representative, explanatory and response variables, and observational studies and experiments). In this chapter, we focus on the actual collection of the data. What kinds of sampling can/should we conduct? What is the difference between an observational study and an experiment, and how do we effectively set up an experiment? An understanding of these concepts is important prior to exploring and visualizing data. 11.1.1 Observational studies Generally, data in observational studies are collected only by monitoring what occurs, while experiments require the primary explanatory variable in a study be assigned for each subject by the researchers. Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Thus, observational studies are generally only sufficient to show associations. Exercise: Suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen causes skin cancer?19 Some previous research20 tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, she is more likely to use sunscreen and more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple investigation. FIGURE 11.1: Sun exposure is a confounding variable because it is related to both response and explanatory variables. Sun exposure is what is called a confounding variable,21 which is a variable that is correlated with both the explanatory and response variables, see Figure 11.1 . While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured. Let’s look at an example of confounding visually. Using the SAT data from the mosaic package let’s look at expenditure per pupil versus SAT scores. Figure 11.2 is a plot of the data. Exercise: What conclusion do you reach from the plot in Figure 11.2?22 FIGURE 11.2: Average SAT score versus expenditure per pupil; reminder: each observation represents an individual state. The implication that spending less might give better results is not justified. Expenditures are confounded with the proportion of students who take the exam, and scores are higher in states where fewer students take the exam. It is interesting to look at the original plot if we place the states into two groups depending on whether more or fewer than 40% of students take the SAT. Figure 11.3 is a plot of the data broken down into the 2 groups. FIGURE 11.3: Average SAT score versus expenditure per pupil; broken down by level of participation. Once we account for the fraction of students taking the SAT, the relationship between expenditures and SAT scores changes. In the same way, the county data set is an observational study with confounding variables, and its data cannot easily be used to make causal conclusions. Exercise: Figure 11.4 shows a negative association between the homeownership rate and the percentage of multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest one or more other variables that might explain the relationship in the Figure 11.4.23 FIGURE 11.4: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties. Observational studies come in two forms: prospective and retrospective studies. A prospective study identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of similar individuals over many years to assess the possible influences of behavior on cancer risk. One example of such a study is The Nurses Health Study, started in 1976 and expanded in 1989.24 This prospective study recruits registered nurses and then collects data from them using questionnaires. Retrospective studies collect data after events have taken place; e.g. researchers may review past events in medical records. Some data sets, such as county, may contain both prospectively- and retrospectively-collected variables. Local governments prospectively collect some variables as events unfolded (e.g. retail sales) while the federal government retrospectively collected others during the 2010 census (e.g. county population). 11.1.2 Three sampling methods Almost all statistical methods are based on the notion of implied randomness. If observational data are not collected in a random framework from a population, results from these statistical methods are not reliable. Here we consider three random sampling techniques: simple, stratified, and cluster sampling. Figures 11.5 , 11.6 , and 11.7 provides a graphical representation of these techniques. FIGURE 11.5: Examples of simple random sampling. In this figure, simple random sampling was used to randomly select the 18 cases. FIGURE 11.6: In this figure, stratified sampling was used: cases were grouped into strata, and then simple random sampling was employed within each stratum. FIGURE 11.7: In this figure, cluster sampling was used, where data were binned into nine clusters, and three of the clusters were randomly selected. Simple random sampling is probably the most intuitive form of random sampling, in which each individual in the population has an equal chance of being chosen. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league’s 30 teams. To take a simple random sample of 120 baseball players and their salaries from the 2010 season, we could write the names of that season’s 828 players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as “simple random” if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included or not. Stratified sampling is a divide-and-conquer sampling strategy. The population is divided into groups called strata. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, the teams could represent the strata; some teams have a lot more money (we’re looking at you, Yankees). Then we might randomly sample 4 players from each team for a total of 120 players. Stratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. The downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. The analysis methods introduced in this book would need to be extended to analyze data collected using stratified sampling. Example: Why would it be good for cases within each stratum to be very similar?25 In cluster sampling, we group observations into clusters, then randomly sample some of the clusters. Sometimes cluster sampling can be a more economical technique than the alternatives. Also, unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don’t look very different from one another. For example, if neighborhoods represented clusters, then this sampling method works best when the neighborhoods are very diverse. A downside of cluster sampling is that more advanced analysis techniques are typically required, though the methods in this book can be extended to handle such data. Example: Suppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next. What sampling method should be employed?26 Another technique called multistage sampling is similar to cluster sampling, except that we take a simple random sample within each selected cluster. For instance, if we sampled neighborhoods using cluster sampling, we would next sample a subset of homes within each selected neighborhood if we were using multistage sampling. 11.1.3 Experiments Studies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables. 11.1.3.1 Principles of experimental design Randomized experiments are generally built on four principles. Controlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may ask all patients to drink a 12 ounce glass of water with the pill. Randomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study. Replication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. Additionally, a group of scientists may replicate an entire study to verify an earlier finding. You replicate to the level of variability you want to estimate. For example, in flight test, we can run the same flight conditions again to get a replicate; however, if the same plane and pilot are being used, the replicate is not getting the pilot-to-pilot or the plane-to-plane variability. Blocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable and then randomize cases within each block, or group, to the treatments. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in Figure 11.8. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients. FIGURE 11.8: Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly divided into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories. It is important to incorporate the first three experimental design principles into any study, and this chapter describes methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this chapter may be extended to analyze data collected using blocking. Math 359 is an entire course at USAFA devoted to the design and analysis of experiments. 11.1.3.2 Reducing bias in human experiments Randomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationships in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients.27 In particular, researchers wanted to know if the drug reduced deaths in patients. These researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug’s effect. Study volunteers28 were randomly placed into two study groups. One group, the treatment group, received the experimental treatment of interest (the new drug to treat heart attack patients). The other group, called the control group, did not receive any drug treatment. The comparison between the treatment and control groups allows researchers to determine whether the treatment really has an effect. Put yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesn’t receive the drug and sits idly, hoping her participation doesn’t increase her risk of death. These perspectives suggest there are actually two effects: the one of interest is the effectiveness of the drug, and the second is an emotional effect that is difficult to quantify. Researchers aren’t usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be blind. But there is one problem: if a patient doesn’t receive a treatment, she will know she is in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect. The patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, she might inadvertently give that patient more attention or care than a patient that she knows is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.29 Exercise: Look back to the stent study in the first chapter where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?30 11.2 Homework Problems Propose a sampling strategy. A large college class has 160 students. All 160 students attend the lectures together, but the students are divided into 4 groups, each with 40 students, for lab sections administered by different teaching assistants. The professor wants to conduct a survey about how satisfied the students are with the course, and he believes that the lab section a student is in might affect the student’s overall satisfaction with the course. What type of study is this? Suggest a sampling strategy for carrying out this study. Flawed reasoning. Identify the flaw in reasoning in the following scenarios. Explain what the individuals in the study should have done differently if they want to be able to make such strong conclusions. Students at an elementary school are given a questionnaire that they are required to return after their parents have completed it. One of the questions asked is, Do you find that your work schedule makes it difficult for you to spend time with your kids after school? Of the parents who replied, 85% said no. Based on these results, the school officials conclude that a great majority of the parents have no difficulty spending time with their kids after school. A survey is conducted on a simple random sample of 1,000 women who recently gave birth, asking them about whether or not they smoked during pregnancy. A follow-up survey asking if the children have respiratory problems is conducted 3 years later, however, only 567 of these women are reached at the same address. The researcher reports that these 567 women are representative of all mothers. Sampling strategies. A statistics student who is curious about the relationship between the amount of time students spend on social networking sites and their performance at school decides to conduct a survey. Four research strategies for collecting data are described below. In each, name the sampling method proposed and any bias you might expect. Note: Sampling methods from both Chapter 3 (Overview of Data Collection Principles) and Chapter 4 (Studies) may be used for this problem. He randomly samples 40 students from the study’s population, gives them the survey, asks them to fill it out and bring it back the next day. He gives out the survey only to his friends, and makes sure each one of them fills out the survey. He posts a link to an online survey on his Facebook wall and asks his friends to fill out the survey. He stands outside the QRC and asks every third person that walks out the door to fill out the survey. Vitamin supplements. In order to assess the effectiveness of taking large doses of vitamin C in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from staff and students at a university. A quarter of the patients were assigned a placebo, and the rest were evenly divided between 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at the onset of a cold for the following two days. All tablets had identical appearance and packaging. The nurses who handed the prescribed pills to the patients knew which patient received which treatment, but the researchers assessing the patients when they were sick did not. No significant differences were observed in any measure of cold duration or severity between the four medication groups, and the placebo group had the shortest duration of symptoms. Is this an experiment or an observational study? Why? What are the explanatory and response variables in this study? Were the patients blinded to their treatment? Was this study double-blind? Participants are ultimately able to choose whether or not to use the pills prescribed to them. We might expect that not all of them will adhere and take their pills. Does this introduce a confounding variable to the study? Explain your reasoning. Exercise and mental health. A researcher is interested in the effects of exercise on mental health and she proposes the following study: Use stratified random sampling to ensure representative proportions of 18-30, 31-40 and 41-55 year olds from the population. Next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the study, and compare the results. What type of study is this? What are the treatment and control groups in this study? Does this study make use of blocking? If so, what is the blocking variable? Does this study make use of blinding? Comment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large. Suppose you are given the task of determining if this proposed study should get funding. Would you have any reservations about the study proposal? Solutions Manual No. See the paragraph following the exercise for an explanation.↩︎ http://www.sciencedirect.com/science/article/pii/S0140673698121682 http://archderm.ama-assn.org/cgi/content/abstract/122/5/537 Study with a similar scenario to that described here: http://onlinelibrary.wiley.com/doi/10.1002/ijc.22745/full↩︎ Also called a lurking variable, confounding factor, or a confounder.↩︎ It appears that average SAT score declines as expenditures per student increases.↩︎ Answers will vary. Population density may be important. If a county is very dense, then a larger fraction of residents may live in multi-unit structures. Additionally, the high density may contribute to increases in property value, making homeownership infeasible for many residents.↩︎ http://www.channing.harvard.edu/nhs/↩︎ We might get a more stable estimate for the subpopulation in a stratum if the cases are very similar. These improved estimates for each subpopulation will help us build a reliable estimate for the full population.↩︎ A simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling seems like a very good idea. We might randomly select a small number of villages. This would probably reduce our data collection costs substantially in comparison to a simple random sample and would still give us helpful information.↩︎ Anturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256.↩︎ Human subjects are often called patients, volunteers, or study participants.↩︎ There are always some researchers in the study who do know which patients are receiving which treatment. However, they do not interact with the study’s patients and do not tell the blinded health care professionals who is receiving which treatment.↩︎ The researchers assigned the patients into their treatment groups, so this study was an experiment. However, the patients could distinguish what treatment they received, so this study was not blind. The study could not be double-blind since it was not blind.↩︎ "],["12-viz.html", "Chapter 12 Data Visualization 12.1 The grammar of graphics 12.2 Five named graphs - the 5NG 12.3 5NG#1: Scatterplots 12.4 5NG#2: Linegraphs 12.5 5NG#3: Histograms 12.6 Facets 12.7 5NG#4: Boxplots 12.8 5NG#5: Barplots 12.9 Conclusion", " Chapter 12 Data Visualization We begin the development of your data science toolbox with data visualization. By visualizing data, we gain valuable insights we couldn’t initially obtain from just looking at the raw data values. We’ll use the ggplot2 package, as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as the grammar of graphics (Wilkinson 2005), developed by Leland Wilkinson. At their most basic, graphics/plots/charts (we use these terms interchangeably in this book) provide a nice way to explore the patterns in data, such as the presence of outliers, distributions of individual variables, and relationships between groups of variables. Graphics are designed to emphasize the findings and insights you want your audience to understand. This does, however, require a balancing act. On the one hand, you want to highlight as many interesting findings as possible. On the other hand, you don’t want to include so much information that it overwhelms your audience. As we will see, plots also help us to identify patterns and outliers in our data. We’ll see that a common extension of these ideas is to compare the distribution of one numerical variable, such as what are the center and spread of the values, as we go across the levels of a different categorical variable. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Read Section ?? for information on how to install and load R packages. library(nycflights13) library(ggplot2) library(moderndive) 12.1 The grammar of graphics We start with a discussion of a theoretical framework for data visualization known as “the grammar of graphics.” This framework serves as the foundation for the ggplot2 package which we’ll use extensively in this chapter. Think of how we construct and form sentences in English by combining different elements, like nouns, verbs, articles, subjects, objects, etc. We can’t just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, “the grammar of graphics” defines a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2005) and has been implemented in a variety of data visualization software platforms like R, but also Plotly and Tableau. 12.1.1 Components of the grammar In short, the grammar tells us that: A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. Specifically, we can break a graphic into the following three essential components: data: the dataset containing the variables of interest. geom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars. aes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset. You might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example. 12.1.2 Gapminder data In February 2006, a Swedish physician and data advocate named Hans Rosling gave a TED talk titled “The best stats you’ve ever seen” where he presented global economic, health, and development data from the website gapminder.org. For example, for data on 142 countries in 2007, let’s consider only a few countries in Table 12.1 as a peek into the data. TABLE 12.1: Gapminder 2007 Data: First 3 of 142 countries Country Continent Life Expectancy Population GDP per Capita Afghanistan Asia 43.8 31889923 975 Albania Europe 76.4 3600523 5937 Algeria Africa 72.3 33333216 6223 Each row in this table corresponds to a country in 2007. For each row, we have 5 columns: Country: Name of country. Continent: Which of the five continents the country is part of. Note that “Americas” includes countries in both North and South America and that Antarctica is excluded. Life Expectancy: Life expectancy in years. Population: Number of people living in the country. GDP per Capita: Gross domestic product (in US dollars). Now consider Figure 12.1, which plots this for all 142 of the data’s countries. FIGURE 12.1: Life expectancy over GDP per capita in 2007. Let’s view this plot through the grammar of graphics: The data variable GDP per Capita gets mapped to the x-position aesthetic of the points. The data variable Life Expectancy gets mapped to the y-position aesthetic of the points. The data variable Population gets mapped to the size aesthetic of the points. The data variable Continent gets mapped to the color aesthetic of the points. We’ll see shortly that data corresponds to the particular data frame where our data is saved and that “data variables” correspond to particular columns in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. We can also use lines, bars, and other geometric objects. Let’s summarize the three essential components of the grammar in Table 12.2. TABLE 12.2: Summary of the grammar of graphics for this plot data variable aes geom GDP per Capita x point Life Expectancy y point Population size point Continent color point 12.1.3 Other components There are other components of the grammar of graphics we can control as well. As you start to delve deeper into the grammar of graphics, you’ll start to encounter these topics more frequently. In this book, we’ll keep things simple and only work with these two additional components: faceting breaks up a plot into several plots split by the values of another variable (Section 12.6) position adjustments for barplots (Section 12.8) Other more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science (Grolemund and Wickham 2017). Generally speaking, the grammar of graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them. 12.1.4 ggplot2 package In this book, we will use the ggplot2 package for data visualization, which is an implementation of the grammar of graphics for R (Wickham et al. 2023). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the grammar of graphics are specified in the ggplot() function included in the ggplot2 package. For the purposes of this book, we’ll always provide the ggplot() function with the following arguments (i.e., inputs) at a minimum: The data frame where the variables exist: the data argument. The mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved. After we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 12.6). Let’s now put the theory of the grammar of graphics into practice. 12.2 Five named graphs - the 5NG In order to keep things simple in this book, we will only focus on five different types of graphics, each with a commonly given name. We term these “five named graphs” or in abbreviated form, the 5NG: scatterplots linegraphs histograms boxplots barplots We’ll also present some variations of these plots, but with this basic repertoire of five graphics in your toolbox, you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables, while others are only appropriate for numerical variables. 12.3 5NG#1: Scatterplots The simplest of the 5NG are scatterplots, also called bivariate plots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the grammar of graphics we presented in Section 12.1. Specifically, we will visualize the relationship between the following two numerical variables in the alaska_flights data frame included in the moderndive package: dep_delay: departure delay on the horizontal “x” axis and arr_delay: arrival delay on the vertical “y” axis for Alaska Airlines flights leaving NYC in 2013. In other words, alaska_flights does not consist of all flights that left NYC in 2013, but rather only those flights where carrier is AS (which is Alaska Airlines’ carrier code). Learning check (LC2.1) Take a look at both the flights data frame from the nycflights13 package and the alaska_flights data frame from the moderndive package by running View(flights) and View(alaska_flights). In what respect do these data frames differ? For example, think about the number of rows in each dataset. 12.3.1 Scatterplots via geom_point Let’s now go over the code that will create the desired scatterplot, while keeping in mind the grammar of graphics framework we introduced in Section 12.1. Let’s take a look at the code and break it down piece-by-piece. ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point() Within the ggplot() function, we specify two of the components of the grammar of graphics as arguments (i.e., inputs): The data as the alaska_flights data frame via data = alaska_flights. The aesthetic mapping by setting mapping = aes(x = dep_delay, y = arr_delay). Specifically, the variable dep_delay maps to the x position aesthetic, while the variable arr_delay maps to the y position. We then add a layer to the ggplot() function call using the + sign. The added layer in question specifies the third component of the grammar: the geometric object. In this case, the geometric object is set to be points by specifying geom_point(). After running these two lines of code in your console, you’ll notice two outputs: a warning message and the graphic shown in Figure 12.2. Warning: Removed 5 rows containing missing values (`geom_point()`). FIGURE 12.2: Arrival delays versus departure delays for Alaska Airlines flights from NYC in 2013. Let’s first unpack the graphic in Figure 12.2. Observe that a positive relationship exists between dep_delay and arr_delay: as departure delays increase, arrival delays tend to also increase. Observe also the large mass of points clustered near (0, 0), the point indicating flights that neither departed nor arrived late. Let’s turn our attention to the warning message. R is alerting us to the fact that five rows were ignored due to them being missing. For these 5 rows, either the value for dep_delay or arr_delay or both were missing (recorded in R as NA), and thus these rows were ignored in our plot. Before we continue, let’s make a few more observations about this code that created the scatterplot. Note that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning of a line. When adding layers to a plot, you are encouraged to start a new line after the + (by pressing the Return/Enter button on your keyboard) so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code. To stress the importance of adding the layer specifying the geometric object, consider Figure 12.3 where no layers are added. Because the geometric object was not specified, we have a blank plot which is not very useful! ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) FIGURE 12.3: A plot with no layers. Learning check (LC2.2) What are some practical reasons why dep_delay and arr_delay have a positive relationship? (LC2.3) What variables in the weather data frame would you expect to have a negative correlation (i.e., a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here. Hint: Explore the weather dataset by using the View() function. (LC2.4) Why do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaska Air flights? (LC2.5) What are some other features of the plot that stand out to you? (LC2.6) Create a new scatterplot using different variables in the alaska_flights data frame by modifying the example given. 12.3.2 Overplotting The large mass of points near (0, 0) in Figure 12.2 can cause some confusion since it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to points being plotted on top of each other over and over again. When overplotting occurs, it is difficult to know the number of points being plotted. There are two methods to address the issue of overplotting. Either by Adjusting the transparency of the points or Adding a little random “jitter”, or random “nudges”, to each of the points. Method 1: Changing the transparency The first way of addressing overplotting is to change the transparency/opacity of the points by setting the alpha argument in geom_point(). We can change the alpha argument to be any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. By default, alpha is set to 1. In other words, if we don’t explicitly set an alpha value, R will use alpha = 1. Note how the following code is identical to the code in Section 12.3 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point() function: ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point(alpha = 0.2) FIGURE 12.4: Arrival vs. departure delays scatterplot with alpha = 0.2. The key feature to note in Figure 12.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line to read geom_point(aes(alpha = 0.2)). Method 2: Jittering the points The second way of addressing overplotting is by jittering all the points. This means giving each point a small “nudge” in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame with 4 identical rows of x and y values: (0,0), (0,0), (0,0), and (0,0). In Figure 12.5, we present both the regular scatterplot of these 4 points (on the left) and its jittered counterpart (on the right). FIGURE 12.5: Regular and jittered scatterplot. In the left-hand regular scatterplot, observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others. In the right-hand jittered scatterplot, it is now plainly evident that this plot involves four points since each point is given a random “nudge.” Keep in mind, however, that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged. To create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). Observe how the following code is very similar to the code that created the scatterplot with overplotting in Subsection 12.3.1, but with geom_point() replaced with geom_jitter(). ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_jitter(width = 30, height = 30) FIGURE 12.6: Arrival versus departure delays jittered scatterplot. In order to specify how much jitter to add, we adjusted the width and height arguments to geom_jitter(). This corresponds to how hard you’d like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. In this case, both axes are in minutes. How much jitter should we add using the width and height arguments? On the one hand, it is important to add just enough jitter to break any overlap in points, but on the other hand, not so much that we completely alter the original pattern in points. As can be seen in the resulting Figure 12.6, in this case jittering doesn’t really provide much new insight. In this particular case, it can be argued that changing the transparency of the points by setting alpha proved more effective. When would it be better to use a jittered scatterplot? When would it be better to alter the points’ transparency? There is no single right answer that applies to all situations. You need to make a subjective choice and own that choice. At the very least when confronted with overplotting, however, we suggest you make both types of plots and see which one better emphasizes the point you are trying to make. Learning check (LC2.7) Why is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot? (LC2.8) After viewing Figure 12.4, give an approximate range of arrival delays and departure delays that occur most frequently. How has that region changed compared to when you observed the same plot without alpha = 0.2 set in Figure 12.2? 12.3.3 Summary Scatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one numerical variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful! With medium to large datasets, you may need to play around with the different modifications to scatterplots we saw such as changing the transparency/opacity of the points or by jittering the points. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships emerge as you tinker with your plots. 12.4 5NG#2: Linegraphs The next of the five named graphs are linegraphs. Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature. In other words, there is an inherent ordering to the variable. The most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots. Let’s illustrate linegraphs using another dataset in the nycflights13 package: the weather data frame. Let’s explore the weather data frame from the nycflights13 package by running View(weather) and glimpse(weather). Furthermore let’s read the associated help file by running ?weather to bring up the help file. Observe that there is a variable called temp of hourly temperature recordings in Fahrenheit at weather stations near all three major airports in New York City: Newark (origin code EWR), John F. Kennedy International (JFK), and LaGuardia (LGA). However, instead of considering hourly temperatures for all days in 2013 for all three airports, for simplicity let’s only consider hourly temperatures at Newark airport for the first 15 days in January. This data is accessible in the early_january_weather data frame included in the moderndive package. In other words, early_january_weather contains hourly weather observations for origin equal to EWR (Newark’s airport code), month equal to 1, and day less than or equal to 15. Learning check (LC2.9) Take a look at both the weather data frame from the nycflights13 package and the early_january_weather data frame from the moderndive package by running View(weather) and View(early_january_weather). In what respect do these data frames differ? (LC2.10) View() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement, whereas the hour variable does not? 12.4.1 Linegraphs via geom_line Let’s create a time series plot of the hourly temperatures saved in the early_january_weather data frame by using geom_line() to create a linegraph, instead of using geom_point() like we used previously to create scatterplots: ggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) + geom_line() FIGURE 12.7: Hourly temperature in Newark for January 1-15, 2013. Much as with the ggplot() code that created the scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 12.2, let’s break down this code piece-by-piece in terms of the grammar of graphics: Within the ggplot() function call, we specify two of the components of the grammar of graphics as arguments: The data to be the early_january_weather data frame by setting data = early_january_weather. The aesthetic mapping by setting mapping = aes(x = time_hour, y = temp). Specifically, the variable time_hour maps to the x position aesthetic, while the variable temp maps to the y position aesthetic. We add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object in question. In this case, the geometric object is a line set by specifying geom_line(). Learning check (LC2.11) Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis? (LC2.12) Why are linegraphs frequently used when time is the explanatory variable on the x-axis? (LC2.13) Plot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013. 12.4.2 Summary Linegraphs, just like scatterplots, display the relationship between two numerical variables. However, it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e., the explanatory variable) has an inherent ordering, such as some notion of time. 12.5 5NG#3: Histograms Let’s consider the temp variable in the weather data frame once again, but unlike with the linegraphs in Section 12.4, let’s say we don’t care about its relationship with time, but rather we only care about how the values of temp distribute. In other words: What are the smallest and largest values? What is the “center” or “most typical” value? How do the values spread out? What are frequent and infrequent values? One way to visualize this distribution of this single variable temp is to plot them on a horizontal line as we do in Figure 12.8: FIGURE 12.8: Plot of hourly temperature recordings from NYC in 2013. This gives us a general idea of how the values of temp distribute: observe that temperatures vary from around 11°F (-11°C) up to 100°F (38°C). Furthermore, there appear to be more recorded temperatures between 40°F and 60°F than outside this range. However, because of the high degree of overplotting in the points, it’s hard to get a sense of exactly how many values are between say 50°F and 55°F. What is commonly produced instead of Figure 12.8 is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows: We first cut up the x-axis into a series of bins, where each bin represents a range of values. For each bin, we count the number of observations that fall in the range corresponding to that bin. Then for each bin, we draw a bar whose height marks the corresponding count. Let’s drill-down on an example of a histogram, shown in Figure 12.9. FIGURE 12.9: Example histogram. Let’s focus only on temperatures between 30°F (-1°C) and 60°F (15°C) for now. Observe that there are three bins of equal width between 30°F and 60°F. Thus we have three bins of width 10°F each: one bin for the 30-40°F range, another bin for the 40-50°F range, and another bin for the 50-60°F range. Since: The bin for the 30-40°F range has a height of around 5000. In other words, around 5000 of the hourly temperature recordings are between 30°F and 40°F. The bin for the 40-50°F range has a height of around 4300. In other words, around 4300 of the hourly temperature recordings are between 40°F and 50°F. The bin for the 50-60°F range has a height of around 3500. In other words, around 3500 of the hourly temperature recordings are between 50°F and 60°F. All nine bins spanning 10°F to 100°F on the x-axis have this interpretation. 12.5.1 Histograms via geom_histogram Let’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable temp. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram(). After running the following code, you’ll see the histogram in Figure 12.10 as well as warning messages. We’ll discuss the warning messages first. ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Warning: Removed 1 rows containing non-finite values (`stat_bin()`). FIGURE 12.10: Histogram of hourly temperatures at three NYC airports. The first message is telling us that the histogram was constructed using bins = 30 for 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We’ll see in the next section how to change the number of bins to another value than the default. The second message is telling us something similar to the warning message we received when we ran the code to create a scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 12.2: that because one row has a missing NA value for temp, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case. Now let’s unpack the resulting histogram in Figure 12.10. Observe that values less than 25°F as well as values above 80°F are rather rare. However, because of the large number of bins, it’s hard to get a sense for which range of temperatures is spanned by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram() and ignore the warning about setting the number of bins to a better value: ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(color = &quot;white&quot;) FIGURE 12.11: Histogram of hourly temperatures at three NYC airports with white borders. We now have an easier time associating ranges of temperatures to each of the bins in Figure 12.11. We can also vary the color of the bars by setting the fill argument. For example, you can set the bin colors to be “blue steel” by setting fill = \"steelblue\": ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;) If you’re curious, run colors() to see all 657 possible choice of colors in R! 12.5.2 Adjusting the bins Observe in Figure 12.11 that in the 50-75°F range there appear to be roughly 8 bins. Thus each bin has width 25 divided by 8, or 3.125°F, which is not a very easily interpretable range to work with. Let’s improve this by adjusting the number of bins in our histogram in one of two ways: By adjusting the number of bins via the bins argument to geom_histogram(). By adjusting the width of the bins via the binwidth argument to geom_histogram(). Using the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows: ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(bins = 40, color = &quot;white&quot;) Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 10°F. ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) We compare both resulting histograms side-by-side in Figure 12.12. FIGURE 12.12: Setting histogram bins in two ways. Learning check (LC2.14) What does changing the number of bins from 30 to 40 tell us about the distribution of temperatures? (LC2.15) Would you classify the distribution of temperatures as symmetric or skewed in one direction or another? (LC2.16) What would you guess is the “center” value in this distribution? Why did you make that choice? (LC2.17) Is this data spread out greatly from the center or is it close? Why? 12.5.3 Summary Histograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question. 12.6 Facets Before continuing with the next of the 5NG, let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ. For example, suppose we were interested in looking at how the histogram of hourly temperature recordings at the three NYC airports we saw in Figure 12.9 differed in each month. We could “split” this histogram by the 12 possible months in a given year. In other words, we would plot histograms of temp for each month separately. We do this by adding facet_wrap(~ month) layer. Note the ~ is a “tilde” and can generally be found on the key next to the “1” key on US keyboards. The tilde is required and you’ll receive the error Error in as.quoted(facets) : object 'month' not found if you don’t include it here. ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + facet_wrap(~ month) FIGURE 12.13: Faceted histogram of hourly temperatures by month. We can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted histogram to have 4 rows instead of 3. We simply add an nrow = 4 argument to facet_wrap(~ month). ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + facet_wrap(~ month, nrow = 4) FIGURE 12.14: Faceted histogram with 4 instead of 3 rows. Observe in both Figures 12.13 and 12.14 that as we might expect in the Northern Hemisphere, temperatures tend to be higher in the summer months, while they tend to be lower in the winter. Learning check (LC2.18) What other things do you notice about this faceted plot? How does a faceted plot help us see relationships between two variables? (LC2.19) What do the numbers 1-12 correspond to in the plot? What about 25, 50, 75, 100? (LC2.20) For which types of datasets would faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics. (LC2.21) Does the temp variable in the weather dataset have a lot of variability? Why do you say that? 12.7 5NG#4: Boxplots While faceted histograms are one type of visualization used to compare the distribution of a numerical variable split by the values of another variable, another type of visualization that achieves this same goal is a side-by-side boxplot. A boxplot is constructed from the information provided in the five-number summary of a numerical variable (see Appendix ??). To keep things simple for now, let’s only consider the 2141 hourly temperature recordings for the month of November, each represented as a jittered point in Figure 12.15. FIGURE 12.15: November temperatures represented as jittered points. These 2141 observations have the following five-number summary: Minimum: 21°F First quartile (25th percentile): 36°F Median (second quartile, 50th percentile): 45°F Third quartile (75th percentile): 52°F Maximum: 71°F In the leftmost plot of Figure 12.16, let’s mark these 5 values with dashed horizontal lines on top of the 2141 points. In the middle plot of Figure 12.16 let’s add the boxplot. In the rightmost plot of Figure 12.16, let’s remove the points and the dashed horizontal lines for clarity’s sake. FIGURE 12.16: Building up a boxplot of November temperatures. What the boxplot does is visually summarize the 2141 points by cutting the 2141 temperature recordings into quartiles at the dashed lines, where each quartile contains roughly 2141 \\(\\div\\) 4 \\(\\approx\\) 535 observations. Thus 25% of points fall below the bottom edge of the box, which is the first quartile of 36°F. In other words, 25% of observations were below 36°F. 25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 45°F. Thus, 25% of observations were between 36°F and 45°F and 50% of observations were below 45°F. 25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 52°F. It follows that 25% of observations were between 45°F and 52°F and 75% of observations were below 52°F. 25% of points fall above the top edge of the box. In other words, 25% of observations were above 52°F. The middle 50% of points lie within the interquartile range (IQR) between the first and third quartile. Thus, the IQR for this example is 52 - 36 = 16°F. The interquartile range is a measure of a numerical variable’s spread. Furthermore, in the rightmost plot of Figure 12.16, we see the whiskers of the boxplot. The whiskers stick out from either end of the box all the way to the minimum and maximum observed temperatures of 21°F and 71°F, respectively. However, the whiskers don’t always extend to the smallest and largest observed values as they do here. They in fact extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box. In this case of the November temperatures, no more than 1.5 \\(\\times\\) 16°F = 24°F from either end of the box. Any observed values outside this range get marked with points called outliers, which we’ll see in the next section. 12.7.1 Boxplots via geom_boxplot Let’s now create a side-by-side boxplot of hourly temperatures split by the 12 months as we did previously with the faceted histograms. We do this by mapping the month variable to the x-position aesthetic, the temp variable to the y-position aesthetic, and by adding a geom_boxplot() layer: ggplot(data = weather, mapping = aes(x = month, y = temp)) + geom_boxplot() FIGURE 12.17: Invalid boxplot specification. Warning messages: 1: Continuous x aesthetic -- did you forget aes(group=...)? 2: Removed 1 rows containing non-finite values (stat_boxplot). Observe in Figure 12.17 that this plot does not provide information about temperature separated by month. The first warning message clues us in as to why. It is telling us that we have a “continuous”, or numerical variable, on the x-position aesthetic. Boxplots, however, require a categorical variable to be mapped to the x-position aesthetic. The second warning message is identical to the warning message when plotting a histogram of hourly temperatures: that one of the values was recorded as NA missing. We can convert the numerical variable month into a factor categorical variable by using the factor() function. So after applying factor(month), month goes from having numerical values just the 1, 2, …, and 12 to having an associated ordering. With this ordering, ggplot() now knows how to work with this variable to produce the needed plot. ggplot(data = weather, mapping = aes(x = factor(month), y = temp)) + geom_boxplot() FIGURE 12.18: Side-by-side boxplot of temperature split by month. The resulting Figure 12.18 shows 12 separate “box and whiskers” plots similar to the rightmost plot of Figure 12.16 of only November temperatures. Thus the different boxplots are shown “side-by-side.” The “box” portions of the visualization represent the 1st quartile, the median (the 2nd quartile), and the 3rd quartile. The height of each box (the value of the 3rd quartile minus the value of the 1st quartile) is the interquartile range (IQR). It is a measure of the spread of the middle 50% of values, with longer boxes indicating more variability. The “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles, respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed temperatures. The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability. The dots representing values falling outside the whiskers are called outliers. These can be thought of as anomalous (“out-of-the-ordinary”) values. It is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long for each boxplot. Looking at this side-by-side plot we can see, as expected, that summer months (6 through 8) have higher median temperatures as evidenced by the higher solid lines in the middle of the boxes. We can easily compare temperatures across months by drawing imaginary horizontal lines across the plot. Furthermore, the heights of the 12 boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of temperatures recorded in a given month. Learning check (LC2.22) What does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point. (LC2.23) Which months have the highest variability in temperature? What reasons can you give for this? (LC2.24) We looked at the distribution of the numerical variable temp split by the numerical variable month that we converted using the factor() function in order to make a side-by-side boxplot. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative? (LC2.25) Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram? 12.7.2 Summary Side-by-side boxplots provide us with a way to compare the distribution of a numerical variable across multiple values of another variable. One can see where the median falls across the different groups by comparing the solid lines in the center of the boxes. To study the spread of a numerical variable within one of the boxes, look at both the length of the box and also how far the whiskers extend from either end of the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with distinct points. 12.8 5NG#5: Barplots Both histograms and boxplots are tools to visualize the distribution of numerical variables. Another commonly desired task is to visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the levels of the categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with barplots (also called barcharts). One complication, however, is how your data is represented. Is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges. fruits &lt;- tibble( fruit = c(&quot;apple&quot;, &quot;apple&quot;, &quot;orange&quot;, &quot;apple&quot;, &quot;orange&quot;) ) fruits_counted &lt;- tibble( fruit = c(&quot;apple&quot;, &quot;orange&quot;), number = c(3, 2) ) We see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually… # A tibble: 5 × 1 fruit &lt;chr&gt; 1 apple 2 apple 3 orange 4 apple 5 orange … fruits_counted has a variable count which represent the “pre-counted” values of each fruit. # A tibble: 2 × 2 fruit number &lt;chr&gt; &lt;dbl&gt; 1 apple 3 2 orange 2 Depending on how your categorical data is represented, you’ll need to add a different geometric layer type to your ggplot() to create a barplot, as we now explore. 12.8.1 Barplots via geom_bar or geom_col Let’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer: ggplot(data = fruits, mapping = aes(x = fruit)) + geom_bar() FIGURE 12.19: Barplot when counts are not pre-counted. However, using the fruits_counted data frame where the fruits have been “pre-counted”, we once again map the fruit variable to the x-position aesthetic, but here we also map the count variable to the y-position aesthetic, and add a geom_col() layer instead. ggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) + geom_col() FIGURE 12.20: Barplot when counts are pre-counted. Compare the barplots in Figures 12.19 and 12.20. They are identical because they reflect counts of the same five fruits. However, depending on how our categorical data is represented, either “pre-counted” or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize Is not pre-counted in your data frame, we use geom_bar(). Is pre-counted in your data frame, we use geom_col() with the y-position aesthetic mapped to the variable that has the counts. Let’s now go back to the flights data frame in the nycflights13 package and visualize the distribution of the categorical variable carrier. In other words, let’s visualize the number of domestic flights out of New York City each airline company flew in 2013. Recall from Subsection ?? when you first explored the flights data frame, you saw that each row corresponds to a flight. In other words, the flights data frame is more like the fruits data frame than the fruits_counted data frame because the flights have not been pre-counted by carrier. Thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable carrier gets mapped to the x-position. As a difference though, histograms have bars that touch whereas bar graphs have white space between the bars going from left to right. ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() FIGURE 12.21: Number of flights departing NYC in 2013 by airline using geom_bar(). Observe in Figure 12.21 that United Airlines (UA), JetBlue Airways (B6), and ExpressJet Airlines (EV) had the most flights depart NYC in 2013. If you don’t know which airlines correspond to which carrier codes, then run View(airlines) to see a directory of airlines. For example, B6 is JetBlue Airways. Alternatively, say you had a data frame where the number of flights for each carrier was pre-counted as in Table 12.3. TABLE 12.3: Number of flights pre-counted for each carrier carrier number 9E 18460 AA 32729 AS 714 B6 54635 DL 48110 EV 54173 F9 685 FL 3260 HA 342 MQ 26397 OO 32 UA 58665 US 20536 VX 5162 WN 12275 YV 601 In order to create a barplot visualizing the distribution of the categorical variable carrier in this case, we would now use geom_col() instead of geom_bar(), with an additional y = number in the aesthetic mapping on top of the x = carrier. The resulting barplot would be identical to Figure 12.21. Learning check (LC2.26) Why are histograms inappropriate for categorical variables? (LC2.27) What is the difference between histograms and barplots? (LC2.28) How many Envoy Air flights departed NYC in 2013? (LC2.29) What was the 7th highest airline for departed flights from NYC in 2013? How could we better present the table to get this answer quickly? 12.8.2 Must avoid pie charts! One of the most common plots used to visualize the distribution of categorical data is the pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book, Creating More Effective Graphs (Robbins 2013), we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another. Let’s examine the same data used in our previous barplot of the number of flights departing NYC by airline in Figure 12.21, but this time we will use a pie chart in Figure 12.22. Try to answer the following questions: How much larger is the portion of the pie for ExpressJet Airlines (EV) compared to US Airways (US)? What is the third largest carrier in terms of departing flights? How many carriers have fewer flights than United Airlines (UA)? FIGURE 12.22: The dreaded pie chart. While it is quite difficult to answer these questions when looking at the pie chart in Figure 12.22, we can much more easily answer these questions using the barchart in Figure 12.21. This is true since barplots present the information in a way such that comparisons between categories can be made with single horizontal lines, whereas pie charts present the information in a way such that comparisons must be made by comparing angles. Learning check (LC2.30) Why should pie charts be avoided and replaced by barplots? (LC2.31) Why do you think people continue to use pie charts? 12.8.3 Two categorical variables Barplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of outgoing domestic flights from NYC by carrier as well as origin. In other words, the number of flights for each carrier and origin combination. For example, the number of WestJet flights from JFK, the number of WestJet flights from LGA, the number of WestJet flights from EWR, the number of American Airlines flights from JFK, and so on. Recall the ggplot() code that created the barplot of carrier frequency in Figure 12.21: ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() We can now map the additional variable origin by adding a fill = origin inside the aes() aesthetic mapping. ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) + geom_bar() FIGURE 12.23: Stacked barplot of flight amount by carrier and origin. Figure 12.23 is an example of a stacked barplot. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of flights from each origin airport between the carriers. Before we continue, let’s address some common points of confusion among new R users. First, the fill aesthetic corresponds to the color used to fill the bars, while the color aesthetic corresponds to the color of the outline of the bars. This is identical to how we added color to our histogram in Subsection 12.5.1: we set the outline of the bars to white by setting color = \"white\" and the colors of the bars to blue steel by setting fill = \"steelblue\". Observe in Figure 12.24 that mapping origin to color and not fill yields grey bars with different colored outlines. ggplot(data = flights, mapping = aes(x = carrier, color = origin)) + geom_bar() FIGURE 12.24: Stacked barplot with color aesthetic used instead of fill. Second, note that fill is another aesthetic mapping much like x-position; thus we were careful to include it within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will yield an error. This is a fairly common error that new ggplot users make: ggplot(data = flights, mapping = aes(x = carrier), fill = origin) + geom_bar() An alternative to stacked barplots are side-by-side barplots, also known as dodged barplots, as seen in Figure 12.25. The code to create a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = \"dodge\" argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot instead. ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) + geom_bar(position = &quot;dodge&quot;) FIGURE 12.25: Side-by-side barplot comparing number of flights by carrier and origin. Note the width of the bars for AS, F9, FL, HA and YV is different than the others. We can make one tweak to the position argument to get them to be the same size in terms of width as the other bars by using the more robust position_dodge() function. ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) + geom_bar(position = position_dodge(preserve = &quot;single&quot;)) FIGURE 12.26: Side-by-side barplot comparing number of flights by carrier and origin (with formatting tweak). Lastly, another type of barplot is a faceted barplot. Recall in Section 12.6 we visualized the distribution of hourly temperatures at the 3 NYC airports split by month using facets. We apply the same principle to our barplot visualizing the frequency of carrier split by origin: instead of mapping origin to fill we include it as the variable to create small multiples of the plot across the levels of origin. ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() + facet_wrap(~ origin, ncol = 1) FIGURE 12.27: Faceted barplot comparing the number of flights by carrier and origin. Learning check (LC2.32) What kinds of questions are not easily answered by looking at Figure 12.23? (LC2.33) What can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights? (LC2.34) Why might the side-by-side barplot be preferable to a stacked barplot in this case? (LC2.35) What are the disadvantages of using a dodged barplot, in general? (LC2.36) Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case? (LC2.37) What information about the different carriers at different airports is more easily seen in the faceted barplot? 12.8.4 Summary Barplots are a common way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories (also called levels) occur. They are easy to understand and make it easy to make comparisons across levels. Furthermore, when trying to visualize the relationship of two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the relationship you are trying to emphasize, you will need to make a choice between these three types of barplots and own that choice. 12.9 Conclusion 12.9.1 Summary table Let’s recap all five of the five named graphs (5NG) in Table 12.4 summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package. TABLE 12.4: Summary of Five Named Graphs Named graph Shows Geometric object Notes 1 Scatterplot Relationship between 2 numerical variables geom_point() 2 Linegraph Relationship between 2 numerical variables geom_line() Used when there is a sequential order to x-variable, e.g., time 3 Histogram Distribution of 1 numerical variable geom_histogram() Facetted histograms show the distribution of 1 numerical variable split by the values of another variable 4 Boxplot Distribution of 1 numerical variable split by the values of another variable geom_boxplot() 5 Barplot Distribution of 1 categorical variable geom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted Stacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables 12.9.2 Function argument specification Let’s go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code: # Segment 1: ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() # Segment 2: ggplot(flights, aes(x = carrier)) + geom_bar() You’ll notice that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() function by default assumes that the data argument comes first and the mapping argument comes second. As long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. Going forward for the rest of this book, all ggplot() code will be like the second segment: with the data = and mapping = explicit naming of the argument omitted with the default ordering of arguments respected. We’ll do this for brevity’s sake; it’s common to see this style when reviewing other R users’ code. 12.9.3 Additional resources An R script file of all R code used in this chapter is available here. If you want to further unlock the power of the ggplot2 package for data visualization, we suggest that you check out RStudio’s “Data Visualization with ggplot2” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter. In particular, it presents many more than the 5 geometric objects we covered in this chapter while providing quick and easy to read visual descriptions. For all the geometric objects, it also lists all the possible aesthetic attributes one can tweak. In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Data Visualization with ggplot2.” You can see a preview in the figure below. Alternatively, you can preview the cheat sheet by going to the ggplot2 Github page with this link. FIGURE 12.28: Data Visualization with ggplot2 cheatsheet. 12.9.4 What’s to come Recall in Figure 12.2 in Section 12.3 we visualized the relationship between departure delay and arrival delay for Alaska Airlines flights only, rather than all flights. This data is saved in the alaska_flights data frame from the moderndive package. In reality, the alaska_flights data frame is merely a subset of the flights data frame from the nycflights13 package consisting of all flights that left NYC in 2013. We created alaska_flights using the following code that uses the dplyr package for data wrangling: library(dplyr) alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point() This code takes the flights data frame and filter() it to only return the 714 rows where carrier is equal to \"AS\", Alaska Airlines’ carrier code. (Recall from Section ?? that testing for equality is specified with == and not =.) The code then cycles back to save the output in a new data frame called alaska_flights using the &lt;- assignment operator. Similarly, recall in Figure 12.7 in Section 12.4 we visualized hourly temperature recordings at Newark airport only for the first 15 days of January 2013. This data is saved in the early_january_weather data frame from the moderndive package. In reality, the early_january_weather data frame is merely a subset of the weather data frame from the nycflights13 package consisting of all hourly weather observations in 2013 for all three NYC airports. We created early_january_weather using the following dplyr code: early_january_weather &lt;- weather %&gt;% filter(origin == &quot;EWR&quot; &amp; month == 1 &amp; day &lt;= 15) ggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) + geom_line() This code pares down the weather data frame to a new data frame early_january_weather consisting of hourly temperature recordings only for origin == \"EWR\", month == 1, and day less than or equal to 15. These two code segments are a preview of Chapter 13 on data wrangling using the dplyr package. Data wrangling is the process of transforming and modifying existing data with the intent of making it more appropriate for analysis purposes. For example, these two code segments used the filter() function to create new data frames (alaska_flights and early_january_weather) by choosing only a subset of rows of existing data frames (flights and weather). In the next chapter, we’ll formally introduce the filter() and other data wrangling functions as well as the pipe operator %&gt;% which allows you to combine multiple data wrangling actions into a single sequential chain of actions. On to Chapter 13 on data wrangling! References "],["13-wrangling.html", "Chapter 13 Data Wrangling 13.1 The pipe operator: %&gt;% 13.2 filter rows 13.3 summarize variables 13.4 group_by rows 13.5 mutate existing variables 13.6 arrange and sort rows 13.7 join data frames 13.8 Other verbs 13.9 Conclusion", " Chapter 13 Data Wrangling So far in our journey, we’ve seen how to look at data saved in data frames using the glimpse() and View() functions in Chapter ??, and how to create data visualizations using the ggplot2 package in Chapter 12. In particular we studied what we term the “five named graphs” (5NG): scatterplots via geom_point() linegraphs via geom_line() boxplots via geom_boxplot() histograms via geom_histogram() barplots via geom_bar() or geom_col() We created these visualizations using the grammar of graphics, which maps variables in a data frame to the aesthetic attributes of one of the 5 geometric objects. We can also control other aesthetic attributes of the geometric objects such as the size and color as seen in the Gapminder data example in Figure 12.1. In this chapter, we’ll introduce a series of functions from the dplyr package for data wrangling that will allow you to take a data frame and “wrangle” it (transform it) to suit your needs. Such functions include: filter() a data frame’s existing rows to only pick out a subset of them. For example, the alaska_flights data frame. summarize() one or more of its columns/variables with a summary statistic. Examples of summary statistics include the median and interquartile range of temperatures as we saw in Section 12.7 on boxplots. group_by() its rows. In other words, assign different rows to be part of the same group. We can then combine group_by() with summarize() to report summary statistics for each group separately. For example, say you don’t want a single overall average departure delay dep_delay for all three origin airports combined, but rather three separate average departure delays, one computed for each of the three origin airports. mutate() its existing columns/variables to create new ones. For example, convert hourly temperature recordings from degrees Fahrenheit to degrees Celsius. arrange() its rows. For example, sort the rows of weather in ascending or descending order of temp. join() it with another data frame by matching along a “key” variable. In other words, merge these two data frames together. Notice how we used computer_code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling has intuitively verb-named functions that are easy to remember. There is a further benefit to learning to use the dplyr package for data wrangling: its similarity to the database querying language SQL (pronounced “sequel” or spelled out as “S”, “Q”, “L”). SQL (which stands for “Structured Query Language”) is used to manage large databases quickly and efficiently and is widely used by many institutions with a lot of data. While SQL is a topic left for a book or a course on database management, keep in mind that once you learn dplyr, you can learn SQL easily. We’ll talk more about their similarities in Subsection 13.7.4. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section ?? for information on how to install and load R packages. library(dplyr) library(ggplot2) library(nycflights13) 13.1 The pipe operator: %&gt;% Before we start data wrangling, let’s first introduce a nifty tool that gets loaded with the dplyr package: the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Let’s start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of f(x) as an input to a function g() then Use the output of g(f(x)) as an input to a function h() One way to achieve this sequence of operations is by using nesting parentheses as follows: h(g(f(x))) This code isn’t so hard to read since we are applying only three functions: f(), then g(), then h() and each of the functions is short in its name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively harder to read as the number of functions applied in your sequence increases and the arguments in each function increase as well. This is where the pipe operator %&gt;% comes in handy. %&gt;% takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then” or “and then.” For example, you can obtain the same output as the hypothetical sequence of functions as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So while both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling: The starting value x will be a data frame. For example, the flights data frame we explored in Section ??. The sequence of functions, here f(), g(), and h(), will mostly be a sequence of any number of the six data wrangling verb-named functions we listed in the introduction to this chapter. For example, the filter(carrier == \"AS\") function and argument specified we previewed earlier. The result will be the transformed/modified data frame that you want. In our example, we’ll save the result in a new data frame by using the &lt;- assignment operator with the name alaska_flights via alaska_flights &lt;-. alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) Much like when adding layers to a ggplot() using the + sign, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence using the pipe operator %&gt;%. Furthermore, much like how the + sign has to come at the end of lines when constructing plots, the pipe operator %&gt;% has to come at the end of lines as well. Keep in mind, there are many more advanced data wrangling functions than just the six listed in the introduction to this chapter; you’ll see some examples of these in Section 13.8. However, just with these six verb-named functions you’ll be able to perform a broad array of data wrangling tasks for the rest of this book. 13.2 filter rows FIGURE 13.1: Diagram of filter() rows operation. The filter() function here works much like the “Filter” option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only the rows that match that criteria. We begin by focusing only on flights from New York City to Portland, Oregon. The dest destination code (or airport code) for Portland, Oregon is \"PDX\". Run the following and look at the results in RStudio’s spreadsheet viewer to ensure that only flights heading to Portland are chosen here: portland_flights &lt;- flights %&gt;% filter(dest == &quot;PDX&quot;) View(portland_flights) Note the order of the code. First, take the flights data frame flights then filter() the data frame so that only those where the dest equals \"PDX\" are included. We test for equality using the double equal sign == and not a single equal sign =. In other words filter(dest = \"PDX\") will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it. You can use other operators beyond just the == operator that tests for equality: &gt; corresponds to “greater than” &lt; corresponds to “less than” &gt;= corresponds to “greater than or equal to” &lt;= corresponds to “less than or equal to” != corresponds to “not equal to.” The ! is used in many programming languages to indicate “not.” Furthermore, you can combine multiple criteria using operators that make comparisons: | corresponds to “or” &amp; corresponds to “and” To see many of these in action, let’s filter flights for all rows that departed from JFK and were heading to Burlington, Vermont (\"BTV\") or Seattle, Washington (\"SEA\") and departed in the months of October, November, or December. Run the following: btv_sea_flights_fall &lt;- flights %&gt;% filter(origin == &quot;JFK&quot; &amp; (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;) &amp; month &gt;= 10) View(btv_sea_flights_fall) Note that even though colloquially speaking one might say “all flights leaving Burlington, Vermont and Seattle, Washington,” in terms of computer operations, we really mean “all flights leaving Burlington, Vermont or leaving Seattle, Washington.” For a given row in the data, dest can be \"BTV\", or \"SEA\", or something else, but not both \"BTV\" and \"SEA\" at the same time. Furthermore, note the careful use of parentheses around dest == \"BTV\" | dest == \"SEA\". We can often skip the use of &amp; and just separate our conditions with a comma. The previous code will return the identical output btv_sea_flights_fall as the following code: btv_sea_flights_fall &lt;- flights %&gt;% filter(origin == &quot;JFK&quot;, (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;), month &gt;= 10) View(btv_sea_flights_fall) Let’s present another example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to flights that didn’t go to Burlington, VT or Seattle, WA. not_BTV_SEA &lt;- flights %&gt;% filter(!(dest == &quot;BTV&quot; | dest == &quot;SEA&quot;)) View(not_BTV_SEA) Again, note the careful use of parentheses around the (dest == \"BTV\" | dest == \"SEA\"). If we didn’t use parentheses as follows: flights %&gt;% filter(!dest == &quot;BTV&quot; | dest == &quot;SEA&quot;) We would be returning all flights not headed to \"BTV\" or those headed to \"SEA\", which is an entirely different resulting data frame. Now say we have a larger number of airports we want to filter for, say \"SEA\", \"SFO\", \"PDX\", \"BTV\", and \"BDL\". We could continue to use the | (or) operator: many_airports &lt;- flights %&gt;% filter(dest == &quot;SEA&quot; | dest == &quot;SFO&quot; | dest == &quot;PDX&quot; | dest == &quot;BTV&quot; | dest == &quot;BDL&quot;) but as we progressively include more airports, this will get unwieldy to write. A slightly shorter approach uses the %in% operator along with the c() function. Recall from Subsection ?? that the c() function “combines” or “concatenates” values into a single vector of values. many_airports &lt;- flights %&gt;% filter(dest %in% c(&quot;SEA&quot;, &quot;SFO&quot;, &quot;PDX&quot;, &quot;BTV&quot;, &quot;BDL&quot;)) View(many_airports) What this code is doing is filtering flights for all flights where dest is in the vector of airports c(\"BTV\", \"SEA\", \"PDX\", \"SFO\", \"BDL\"). Both outputs of many_airports are the same, but as you can see the latter takes much less energy to code. The %in% operator is useful for looking for matches commonly in one vector/variable compared to another. As a final note, we recommend that filter() should often be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations you care about. Learning check (LC3.1) What’s another way of using the “not” operator ! to filter only the rows that are not going to Burlington, VT nor Seattle, WA in the flights data frame? Test this out using the previous code. 13.3 summarize variables The next common task when working with data frames is to compute summary statistics. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (also called the average) and the median (the middle value). Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value also called the minimum, the largest value also called the maximum, and the standard deviation. See Appendix ?? for a glossary of such summary statistics. Let’s calculate two summary statistics of the temp temperature variable in the weather data frame: the mean and standard deviation (recall from Section ?? that the weather data frame is included in the nycflights13 package). To compute these summary statistics, we need the mean() and sd() summary functions in R. Summary functions in R take in many values and return a single value, as illustrated in Figure 13.2. FIGURE 13.2: Diagram illustrating a summary function in R. More precisely, we’ll use the mean() and sd() summary functions within the summarize() function from the dplyr package. Note you can also use the British English spelling of summarise(). As shown in Figure 13.3, the summarize() function takes in a data frame and returns a data frame with only one row corresponding to the summary statistics. FIGURE 13.3: Diagram of summarize() rows. We’ll save the results in a new data frame called summary_temp that will have two columns/variables: the mean and the std_dev: summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp), std_dev = sd(temp)) summary_temp # A tibble: 1 × 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 NA NA Why are the values returned NA? As we saw in Subsection 12.3.1 when creating the scatterplot of departure and arrival delays for alaska_flights, NA is how R encodes missing values where NA indicates “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it? Perhaps the data was not collected at all because it was too difficult to do so? Perhaps there was an erroneous value that someone entered that has been corrected to read as missing? You’ll often encounter issues with missing values when working with real data. Going back to our summary_temp output, by default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, NA is returned. To work around this fact, you can set the na.rm argument to TRUE, where rm is short for “remove”; this will ignore any NA missing values and only return the summary value for all non-missing values. The code that follows computes the mean and standard deviation of all non-missing values of temp: summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE)) summary_temp # A tibble: 1 × 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 55.3 17.8 Notice how the na.rm = TRUE are used as arguments to the mean() and sd() summary functions individually, and not to the summarize() function. However, one needs to be cautious whenever ignoring missing values as we’ve just done. In the upcoming Learning checks questions, we’ll consider the possible ramifications of blindly sweeping rows with missing values “under the rug.” This is in fact why the na.rm argument to any summary statistic function in R is set to FALSE by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis. What are other summary functions we can use inside the summarize() verb to compute summary statistics? As seen in the diagram in Figure 13.2, you can use any function in R that takes many values and returns just one. Here are just a few: mean(): the average sd(): the standard deviation, which is a measure of spread min() and max(): the minimum and maximum values, respectively IQR(): interquartile range sum(): the total amount when adding multiple numbers n(): a count of the number of rows in each group. This particular summary function will make more sense when group_by() is covered in Section 13.4. Learning check (LC3.2) Say a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five-year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach? (LC3.3) Modify the earlier summarize() function code that creates the summary_temp data frame to also use the n() summary function: summarize(... , count = n()). What does the returned value correspond to? (LC3.4) Why doesn’t the following code work? Run the code line-by-line instead of all at once, and then look at the data. In other words, run summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE)) first. summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE)) %&gt;% summarize(std_dev = sd(temp, na.rm = TRUE)) 13.4 group_by rows FIGURE 13.4: Diagram of group_by() and summarize(). Say instead of a single mean temperature for the whole year, you would like 12 mean temperatures, one for each of the 12 months separately. In other words, we would like to compute the mean temperature split by month. We can do this by “grouping” temperature observations by the values of another variable, in this case by the 12 values of the variable month. Run the following code: summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE)) summary_monthly_temp # A tibble: 12 × 3 month mean std_dev &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 35.6 10.2 2 2 34.3 6.98 3 3 39.9 6.25 4 4 51.7 8.79 5 5 61.8 9.68 6 6 72.2 7.55 7 7 80.1 7.12 8 8 74.5 5.19 9 9 67.4 8.47 10 10 60.1 8.85 11 11 45.0 10.4 12 12 38.4 9.98 This code is identical to the previous code that created summary_temp, but with an extra group_by(month) added before the summarize(). Grouping the weather dataset by month and then applying the summarize() functions yields a data frame that displays the mean and standard deviation temperature split by the 12 months of the year. It is important to note that the group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the grouping structure. It is only after we apply the summarize() function that the data frame changes. For example, let’s consider the diamonds data frame included in the ggplot2 package. Run this code: diamonds # A tibble: 53,940 × 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ℹ 53,930 more rows Observe that the first line of the output reads # A tibble: 53,940 x 10. This is an example of meta-data, in this case the number of observations/rows and variables/columns in diamonds. The actual data itself are the subsequent table of values. Now let’s pipe the diamonds data frame into group_by(cut): diamonds %&gt;% group_by(cut) # A tibble: 53,940 × 10 # Groups: cut [5] carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ℹ 53,930 more rows Observe that now there is additional meta-data: # Groups: cut [5] indicating that the grouping structure meta-data has been set based on the 5 possible levels of the categorical variable cut: \"Fair\", \"Good\", \"Very Good\", \"Premium\", and \"Ideal\". On the other hand, observe that the data has not changed: it is still a table of 53,940 \\(\\times\\) 10 values. Only by combining a group_by() with another data wrangling operation, in this case summarize(), will the data actually be transformed. diamonds %&gt;% group_by(cut) %&gt;% summarize(avg_price = mean(price)) # A tibble: 5 × 2 cut avg_price &lt;ord&gt; &lt;dbl&gt; 1 Fair 4359. 2 Good 3929. 3 Very Good 3982. 4 Premium 4584. 5 Ideal 3458. If you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the ungroup() function: diamonds %&gt;% group_by(cut) %&gt;% ungroup() # A tibble: 53,940 × 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ℹ 53,930 more rows Observe how the # Groups: cut [5] meta-data is no longer present. Let’s now revisit the n() counting summary function we briefly introduced previously. Recall that the n() function counts rows. This is opposed to the sum() summary function that returns the sum of a numerical variable. For example, suppose we’d like to count how many flights departed each of the three airports in New York City: by_origin &lt;- flights %&gt;% group_by(origin) %&gt;% summarize(count = n()) by_origin # A tibble: 3 × 2 origin count &lt;chr&gt; &lt;int&gt; 1 EWR 120835 2 JFK 111279 3 LGA 104662 We see that Newark (\"EWR\") had the most flights departing in 2013 followed by \"JFK\" and lastly by LaGuardia (\"LGA\"). Note there is a subtle but important difference between sum() and n(); while sum() returns the sum of a numerical variable, n() returns a count of the number of rows/observations. 13.4.1 Grouping by more than one variable You are not limited to grouping by one variable. Say you want to know the number of flights leaving each of the three New York City airports for each month. We can also group by a second variable month using group_by(origin, month): by_origin_monthly &lt;- flights %&gt;% group_by(origin, month) %&gt;% summarize(count = n()) by_origin_monthly # A tibble: 36 × 3 # Groups: origin [3] origin month count &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 EWR 1 9893 2 EWR 2 9107 3 EWR 3 10420 4 EWR 4 10531 5 EWR 5 10592 6 EWR 6 10175 7 EWR 7 10475 8 EWR 8 10359 9 EWR 9 9550 10 EWR 10 10104 # ℹ 26 more rows Observe that there are 36 rows to by_origin_monthly because there are 12 months for 3 airports (EWR, JFK, and LGA). Why do we group_by(origin, month) and not group_by(origin) and then group_by(month)? Let’s investigate: by_origin_monthly_incorrect &lt;- flights %&gt;% group_by(origin) %&gt;% group_by(month) %&gt;% summarize(count = n()) by_origin_monthly_incorrect # A tibble: 12 × 2 month count &lt;int&gt; &lt;int&gt; 1 1 27004 2 2 24951 3 3 28834 4 4 28330 5 5 28796 6 6 28243 7 7 29425 8 8 29327 9 9 27574 10 10 28889 11 11 27268 12 12 28135 What happened here is that the second group_by(month) overwrote the grouping structure meta-data of the earlier group_by(origin), so that in the end we are only grouping by month. The lesson here is if you want to group_by() two or more variables, you should include all the variables at the same time in the same group_by() adding a comma between the variable names. Learning check (LC3.5) Recall from Chapter 12 when we looked at temperatures by months in NYC. What does the standard deviation column in the summary_monthly_temp data frame tell us about temperatures in NYC throughout the year? (LC3.6) What code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC? (LC3.7) Recreate by_monthly_origin, but instead of grouping via group_by(origin, month), group variables in a different order group_by(month, origin). What differs in the resulting dataset? (LC3.8) How could we identify how many flights left each of the three airports for each carrier? (LC3.9) How does the filter() operation differ from a group_by() followed by a summarize()? 13.5 mutate existing variables FIGURE 13.5: Diagram of mutate() columns. Another common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of temperature in degrees Celsius (°C) instead of degrees Fahrenheit (°F). The formula to convert temperatures from °F to °C is \\[ \\text{temp in C} = \\frac{\\text{temp in F} - 32}{1.8} \\] We can apply this formula to the temp variable using the mutate() function from the dplyr package, which takes existing variables and mutates them to create new ones. weather &lt;- weather %&gt;% mutate(temp_in_C = (temp - 32) / 1.8) In this code, we mutate() the weather data frame by creating a new variable temp_in_C = (temp - 32) / 1.8 and then overwrite the original weather data frame. Why did we overwrite the data frame weather, instead of assigning the result to a new data frame like weather_new? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable temp, but instead created a new variable called temp_in_C? Because if we did this, we would have erased the original information contained in temp of temperatures in Fahrenheit that may still be valuable to us. Let’s now compute monthly average temperatures in both °F and °C using the group_by() and summarize() code we saw in Section 13.4: summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean_temp_in_F = mean(temp, na.rm = TRUE), mean_temp_in_C = mean(temp_in_C, na.rm = TRUE)) summary_monthly_temp # A tibble: 12 × 3 month mean_temp_in_F mean_temp_in_C &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 35.6 2.02 2 2 34.3 1.26 3 3 39.9 4.38 4 4 51.7 11.0 5 5 61.8 16.6 6 6 72.2 22.3 7 7 80.1 26.7 8 8 74.5 23.6 9 9 67.4 19.7 10 10 60.1 15.6 11 11 45.0 7.22 12 12 38.4 3.58 Let’s consider another example. Passengers are often frustrated when their flight departs late, but aren’t as annoyed if, in the end, pilots can make up some time during the flight. This is known in the airline industry as gain, and we will create this variable using the mutate() function: flights &lt;- flights %&gt;% mutate(gain = dep_delay - arr_delay) Let’s take a look at only the dep_delay, arr_delay, and the resulting gain variables for the first 5 rows in our updated flights data frame in Table 13.1. TABLE 13.1: First five rows of departure/arrival delay and gain variables dep_delay arr_delay gain 2 11 -9 4 20 -16 2 33 -31 -1 -18 17 -6 -25 19 The flight in the first row departed 2 minutes late but arrived 11 minutes late, so its “gained time in the air” is a loss of 9 minutes, hence its gain is 2 - 11 = -9. On the other hand, the flight in the fourth row departed a minute early (dep_delay of -1) but arrived 18 minutes early (arr_delay of -18), so its “gained time in the air” is \\(-1 - (-18) = -1 + 18 = 17\\) minutes, hence its gain is +17. Let’s look at some summary statistics of the gain variable by considering multiple summary functions at once in the same summarize() code: gain_summary &lt;- flights %&gt;% summarize( min = min(gain, na.rm = TRUE), q1 = quantile(gain, 0.25, na.rm = TRUE), median = quantile(gain, 0.5, na.rm = TRUE), q3 = quantile(gain, 0.75, na.rm = TRUE), max = max(gain, na.rm = TRUE), mean = mean(gain, na.rm = TRUE), sd = sd(gain, na.rm = TRUE), missing = sum(is.na(gain)) ) gain_summary # A tibble: 1 × 8 min q1 median q3 max mean sd missing &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 -196 -3 7 17 109 5.66 18.0 9430 We see for example that the average gain is +5 minutes, while the largest is +109 minutes! However, this code would take some time to type out in practice. We’ll see later on in Subsection 17.1.1 that there is a much more succinct way to compute a variety of common summary statistics: using the skim() function from the skimr package. Recall from Section 12.5 that since gain is a numerical variable, we can visualize its distribution using a histogram. ggplot(data = flights, mapping = aes(x = gain)) + geom_histogram(color = &quot;white&quot;, bins = 20) FIGURE 13.6: Histogram of gain variable. The resulting histogram in Figure 13.6 provides a different perspective on the gain variable than the summary statistics we computed earlier. For example, note that most values of gain are right around 0. To close out our discussion on the mutate() function to create new variables, note that we can create multiple new variables at once in the same mutate() code. Furthermore, within the same mutate() code we can refer to new variables we just created. As an example, consider the mutate() code Hadley Wickham and Garrett Grolemund show in Chapter 5 of R for Data Science (Grolemund and Wickham 2017): flights &lt;- flights %&gt;% mutate( gain = dep_delay - arr_delay, hours = air_time / 60, gain_per_hour = gain / hours ) Learning check (LC3.10) What do positive values of the gain variable in flights correspond to? What about negative values? And what about a zero value? (LC3.11) Could we create the dep_delay and arr_delay columns by simply subtracting dep_time from sched_dep_time and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in flights. (LC3.12) What can we say about the distribution of gain? Describe it in a few sentences using the plot and the gain_summary data frame values. 13.6 arrange and sort rows One of the most commonly performed data wrangling tasks is to sort a data frame’s rows in the alphanumeric order of one of the variables. The dplyr package’s arrange() function allows us to sort/reorder a data frame’s rows according to the values of the specified variable. Suppose we are interested in determining the most frequent destination airports for all domestic flights departing from New York City in 2013: freq_dest &lt;- flights %&gt;% group_by(dest) %&gt;% summarize(num_flights = n()) freq_dest # A tibble: 105 × 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 ABQ 254 2 ACK 265 3 ALB 439 4 ANC 8 5 ATL 17215 6 AUS 2439 7 AVL 275 8 BDL 443 9 BGR 375 10 BHM 297 # ℹ 95 more rows Observe that by default the rows of the resulting freq_dest data frame are sorted in alphabetical order of destination. Say instead we would like to see the same data, but sorted from the most to the least number of flights (num_flights) instead: freq_dest %&gt;% arrange(num_flights) # A tibble: 105 × 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 LEX 1 2 LGA 1 3 ANC 8 4 SBN 10 5 HDN 15 6 MTJ 15 7 EYW 17 8 PSP 19 9 JAC 25 10 BZN 36 # ℹ 95 more rows This is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because arrange() always returns rows sorted in ascending order by default. To switch the ordering to be in “descending” order instead, we use the desc() function as so: freq_dest %&gt;% arrange(desc(num_flights)) # A tibble: 105 × 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 ORD 17283 2 ATL 17215 3 LAX 16174 4 BOS 15508 5 MCO 14082 6 CLT 14064 7 SFO 13331 8 FLL 12055 9 MIA 11728 10 DCA 9705 # ℹ 95 more rows 13.7 join data frames Another common data transformation task is “joining” or “merging” two different datasets. For example, in the flights data frame, the variable carrier lists the carrier code for the different flights. While the corresponding airline names for \"UA\" and \"AA\" might be somewhat easy to guess (United and American Airlines), what airlines have codes \"VX\", \"HA\", and \"B6\"? This information is provided in a separate data frame airlines. View(airlines) We see that in airlines, carrier is the carrier code, while name is the full name of the airline company. Using this table, we can see that \"VX\", \"HA\", and \"B6\" correspond to Virgin America, Hawaiian Airlines, and JetBlue, respectively. However, wouldn’t it be nice to have all this information in a single data frame instead of two separate data frames? We can do this by “joining” the flights and airlines data frames. Note that the values in the variable carrier in the flights data frame match the values in the variable carrier in the airlines data frame. In this case, we can use the variable carrier as a key variable to match the rows of the two data frames. Key variables are almost always identification variables that uniquely identify the observational units as we saw in Subsection ??. This ensures that rows in both data frames are appropriately matched during the join. Hadley and Garrett (Grolemund and Wickham 2017) created the diagram shown in Figure 13.7 to help us understand how the different data frames in the nycflights13 package are linked by various key variables: FIGURE 13.7: Data relationships in nycflights13 from R for Data Science. 13.7.1 Matching “key” variable names In both the flights and airlines data frames, the key variable we want to join/merge/match the rows by has the same name: carrier. Let’s use the inner_join() function to join the two data frames, where the rows will be matched by the variable carrier, and then compare the resulting data frames: flights_joined &lt;- flights %&gt;% inner_join(airlines, by = &quot;carrier&quot;) View(flights) View(flights_joined) Observe that the flights and flights_joined data frames are identical except that flights_joined has an additional variable name. The values of name correspond to the airline companies’ names as indicated in the airlines data frame. A visual representation of the inner_join() is shown in Figure 13.8 (Grolemund and Wickham 2017). There are other types of joins available (such as left_join(), right_join(), outer_join(), and anti_join()), but the inner_join() will solve nearly all of the problems you’ll encounter in this book. FIGURE 13.8: Diagram of inner join from R for Data Science. 13.7.2 Different “key” variable names Say instead you are interested in the destinations of all domestic flights departing NYC in 2013, and you ask yourself questions like: “What cities are these airports in?”, or “Is \"ORD\" Orlando?”, or “Where is \"FLL\"?”. The airports data frame contains the airport codes for each airport: View(airports) However, if you look at both the airports and flights data frames, you’ll find that the airport codes are in variables that have different names. In airports the airport code is in faa, whereas in flights the airport codes are in origin and dest. This fact is further highlighted in the visual representation of the relationships between these data frames in Figure 13.7. In order to join these two data frames by airport code, our inner_join() operation will use the by = c(\"dest\" = \"faa\") argument with modified code syntax allowing us to join two data frames where the key variable has a different name: flights_with_airport_names &lt;- flights %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) View(flights_with_airport_names) Let’s construct the chain of pipe operators %&gt;% that computes the number of flights from NYC to each destination, but also includes information about each destination airport: named_dests &lt;- flights %&gt;% group_by(dest) %&gt;% summarize(num_flights = n()) %&gt;% arrange(desc(num_flights)) %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% rename(airport_name = name) named_dests # A tibble: 101 × 9 dest num_flights airport_name lat lon alt tz dst tzone &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 ORD 17283 Chicago Ohare Intl 42.0 -87.9 668 -6 A Amer… 2 ATL 17215 Hartsfield Jackson At… 33.6 -84.4 1026 -5 A Amer… 3 LAX 16174 Los Angeles Intl 33.9 -118. 126 -8 A Amer… 4 BOS 15508 General Edward Lawren… 42.4 -71.0 19 -5 A Amer… 5 MCO 14082 Orlando Intl 28.4 -81.3 96 -5 A Amer… 6 CLT 14064 Charlotte Douglas Intl 35.2 -80.9 748 -5 A Amer… 7 SFO 13331 San Francisco Intl 37.6 -122. 13 -8 A Amer… 8 FLL 12055 Fort Lauderdale Holly… 26.1 -80.2 9 -5 A Amer… 9 MIA 11728 Miami Intl 25.8 -80.3 8 -5 A Amer… 10 DCA 9705 Ronald Reagan Washing… 38.9 -77.0 15 -5 A Amer… # ℹ 91 more rows In case you didn’t know, \"ORD\" is the airport code of Chicago O’Hare airport and \"FLL\" is the main airport in Fort Lauderdale, Florida, which can be seen in the airport_name variable. 13.7.3 Multiple “key” variables Say instead we want to join two data frames by multiple key variables. For example, in Figure 13.7, we see that in order to join the flights and weather data frames, we need more than one key variable: year, month, day, hour, and origin. This is because the combination of these 5 variables act to uniquely identify each observational unit in the weather data frame: hourly weather recordings at each of the 3 NYC airports. We achieve this by specifying a vector of key variables to join by using the c() function. Recall from Subsection ?? that c() is short for “combine” or “concatenate.” flights_weather_joined &lt;- flights %&gt;% inner_join(weather, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;origin&quot;)) View(flights_weather_joined) Learning check (LC3.13) Looking at Figure 13.7, when joining flights and weather (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of year, month, day, hour, and origin, and not just hour? (LC3.14) What surprises you about the top 10 destinations from NYC in 2013? 13.7.4 Normal forms The data frames included in the nycflights13 package are in a form that minimizes redundancy of data. For example, the flights data frame only saves the carrier code of the airline company; it does not include the actual name of the airline. For example, the first row of flights has carrier equal to UA, but it does not include the airline name of “United Air Lines Inc.” The names of the airline companies are included in the name variable of the airlines data frame. In order to have the airline company name included in flights, we could join these two data frames as follows: joined_flights &lt;- flights %&gt;% inner_join(airlines, by = &quot;carrier&quot;) View(joined_flights) We are capable of performing this join because each of the data frames have keys in common to relate one to another: the carrier variable in both the flights and airlines data frames. The key variable(s) that we base our joins on are often identification variables as we mentioned previously. This is an important property of what’s known as normal forms of data. The process of decomposing data frames into less redundant tables without losing information is called normalization. More information is available on Wikipedia. Both dplyr and SQL we mentioned in the introduction of this chapter use such normal forms. Given that they share such commonalities, once you learn either of these two tools, you can learn the other very easily. Learning check (LC3.15) What are some advantages of data in normal forms? What are some disadvantages? 13.8 Other verbs Here are some other useful data wrangling verbs: select() only a subset of variables/columns. rename() variables/columns to have new names. Return only the top_n() values of a variable. 13.8.1 select variables FIGURE 13.9: Diagram of select() columns. We’ve seen that the flights data frame in the nycflights13 package contains 19 different variables. You can identify the names of these 19 variables by running the glimpse() function from the dplyr package: glimpse(flights) However, say you only need two of these 19 variables, say carrier and flight. You can select() these two variables: flights %&gt;% select(carrier, flight) This function makes it easier to explore large datasets since it allows us to limit the scope to only those variables we care most about. For example, if we select() only a smaller number of variables as is shown in Figure 13.9, it will make viewing the dataset in RStudio’s spreadsheet viewer more digestible. Let’s say instead you want to drop, or de-select, certain variables. For example, consider the variable year in the flights data frame. This variable isn’t quite a “variable” because it is always 2013 and hence doesn’t change. Say you want to remove this variable from the data frame. We can deselect year by using the - sign: flights_no_year &lt;- flights %&gt;% select(-year) Another way of selecting columns/variables is by specifying a range of columns: flight_arr_times &lt;- flights %&gt;% select(month:day, arr_time:sched_arr_time) flight_arr_times This will select() all columns between month and day, as well as between arr_time and sched_arr_time, and drop the rest. The select() function can also be used to reorder columns when used with the everything() helper function. For example, suppose we want the hour, minute, and time_hour variables to appear immediately after the year, month, and day variables, while not discarding the rest of the variables. In the following code, everything() will pick up all remaining variables: flights_reorder &lt;- flights %&gt;% select(year, month, day, hour, minute, time_hour, everything()) glimpse(flights_reorder) Lastly, the helper functions starts_with(), ends_with(), and contains() can be used to select variables/columns that match those conditions. As examples, flights %&gt;% select(starts_with(&quot;a&quot;)) flights %&gt;% select(ends_with(&quot;delay&quot;)) flights %&gt;% select(contains(&quot;time&quot;)) 13.8.2 rename variables Another useful function is rename(), which as you may have guessed changes the name of variables. Suppose we want to only focus on dep_time and arr_time and change dep_time and arr_time to be departure_time and arrival_time instead in the flights_time data frame: flights_time_new &lt;- flights %&gt;% select(dep_time, arr_time) %&gt;% rename(departure_time = dep_time, arrival_time = arr_time) glimpse(flights_time_new) Note that in this case we used a single = sign within the rename(). For example, departure_time = dep_time renames the dep_time variable to have the new name departure_time. This is because we are not testing for equality like we would using ==. Instead we want to assign a new variable departure_time to have the same values as dep_time and then delete the variable dep_time. Note that new dplyr users often forget that the new variable name comes before the equal sign. 13.8.3 top_n values of a variable We can also return the top n values of a variable using the top_n() function. For example, we can return a data frame of the top 10 destination airports using the example from Subsection 13.7.2. Observe that we set the number of values to return to n = 10 and wt = num_flights to indicate that we want the rows corresponding to the top 10 values of num_flights. See the help file for top_n() by running ?top_n for more information. named_dests %&gt;% top_n(n = 10, wt = num_flights) Let’s further arrange() these results in descending order of num_flights: named_dests %&gt;% top_n(n = 10, wt = num_flights) %&gt;% arrange(desc(num_flights)) Learning check (LC3.16) What are some ways to select all three of the dest, air_time, and distance variables from flights? Give the code showing how to do this in at least three different ways. (LC3.17) How could one use starts_with(), ends_with(), and contains() to select columns from the flights data frame? Provide three different examples in total: one for starts_with(), one for ends_with(), and one for contains(). (LC3.18) Why might we want to use the select function on a data frame? (LC3.19) Create a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013. 13.9 Conclusion 13.9.1 Summary table Let’s recap our data wrangling verbs in Table 13.2. Using these verbs and the pipe %&gt;% operator from Section 13.1, you’ll be able to write easily legible code to perform almost all the data wrangling and data transformation necessary for the rest of this book. TABLE 13.2: Summary of data wrangling verbs Verb Data wrangling operation filter() Pick out a subset of rows summarize() Summarize many values to one using a summary statistic function like mean(), median(), etc. group_by() Add grouping structure to rows in data frame. Note this does not change values in data frame, rather only the meta-data mutate() Create new variables by mutating existing ones arrange() Arrange rows of a data variable in ascending (default) or descending order inner_join() Join/merge two data frames, matching rows by a key variable Learning check (LC3.20) Let’s now put your newly acquired data wrangling skills to the test! An airline industry measure of a passenger airline’s capacity is the available seat miles, which is equal to the number of seats available multiplied by the number of miles or kilometers flown summed over all flights. For example, let’s consider the scenario in Figure 13.10. Since the airplane has 4 seats and it travels 200 miles, the available seat miles are \\(4 \\times 200 = 800\\). FIGURE 13.10: Example of available seat miles for one flight. Extending this idea, let’s say an airline had 2 flights using a plane with 10 seats that flew 500 miles and 3 flights using a plane with 20 seats that flew 1000 miles, the available seat miles would be \\(2 \\times 10 \\times 500 + 3 \\times 20 \\times 1000 = 70,000\\) seat miles. Using the datasets included in the nycflights13 package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints: Crucial: Unless you are very confident in what you are doing, it is worthwhile not starting to code right away. Rather, first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level pseudocode that is informal yet detailed enough to articulate what you are doing. This way you won’t confuse what you are trying to do (the algorithm) with how you are going to do it (writing dplyr code). Take a close look at all the datasets using the View() function: flights, weather, planes, airports, and airlines to identify which variables are necessary to compute available seat miles. Figure 13.7 showing how the various datasets can be joined will also be useful. Consider the data wrangling verbs in Table 13.2 as your toolbox! 13.9.2 Additional resources An R script file of all R code used in this chapter is available here. In the online Appendix C, we provide a page of data wrangling “tips and tricks” consisting of the most common data wrangling questions we’ve encountered in student projects (shout out to Dr. Jenny Smetzer for her work setting this up!): Dealing with missing values Reordering bars in a barplot Showing money on an axis Changing values inside cells Converting a numerical variable to a categorical one Computing proportions Dealing with %, commas, and $ However to provide a tips and tricks page covering all possible data wrangling questions would be too long to be useful! If you want to further unlock the power of the dplyr package for data wrangling, we suggest that you check out RStudio’s “Data Transformation with dplyr” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular more intermediate level and advanced data wrangling functions, while providing quick and easy-to-read visual descriptions. In fact, many of the diagrams illustrating data wrangling operations in this chapter, such as Figure 13.1 on filter(), originate from this cheatsheet. In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Data Transformation with dplyr.” You can see a preview in the figure below. FIGURE 13.11: Data Transformation with dplyr cheatsheet. On top of the data wrangling verbs and examples we presented in this section, if you’d like to see more examples of using the dplyr package for data wrangling, check out Chapter 5 of R for Data Science (Grolemund and Wickham 2017). 13.9.3 What’s to come? So far in this book, we’ve explored, visualized, and wrangled data saved in data frames. These data frames were saved in a spreadsheet-like format: in a rectangular shape with a certain number of rows corresponding to observations and a certain number of columns corresponding to variables describing these observations. We’ll see in the upcoming Chapter ?? that there are actually two ways to represent data in spreadsheet-type rectangular format: (1) “wide” format and (2) “tall/narrow” format. The tall/narrow format is also known as “tidy” format in R user circles. While the distinction between “tidy” and non-“tidy” formatted data is subtle, it has immense implications for our data science work. This is because almost all the packages used in this book, including the ggplot2 package for data visualization and the dplyr package for data wrangling, all assume that all data frames are in “tidy” format. Furthermore, up until now we’ve only explored, visualized, and wrangled data saved within R packages. But what if you want to analyze data that you have saved in a Microsoft Excel, a Google Sheets, or a “Comma-Separated Values” (CSV) file? In Section ??, we’ll show you how to import this data into R using the readr package. References "],["14-sampling.html", "Chapter 14 Sampling 14.1 Sampling bowl activity 14.2 Virtual sampling 14.3 Sampling framework 14.4 Case study: Polls 14.5 Central Limit Theorem 14.6 Conclusion", " Chapter 14 Sampling In this chapter, we kick off the third portion of this book on statistical inference by learning about sampling. The concepts behind sampling form the basis of confidence intervals and hypothesis testing, which we’ll cover in Chapters 15 and 16. We will see that the tools that you learned in the data science portion of this book, in particular data visualization and data wrangling, will also play an important role in the development of your understanding. As mentioned before, the concepts throughout this text all build into a culmination allowing you to “tell your story with data.” Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section ?? that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section ?? for information on how to install and load R packages. library(tidyverse) library(moderndive) 14.1 Sampling bowl activity Let’s start with a hands-on activity. 14.1.1 What proportion of this bowl’s balls are red? Take a look at the bowl in Figure 14.1. It has a certain number of red and a certain number of white balls all of equal size. Furthermore, it appears the bowl has been mixed beforehand, as there does not seem to be any coherent pattern to the spatial distribution of the red and white balls. Let’s now ask ourselves, what proportion of this bowl’s balls are red? FIGURE 14.1: A bowl with red and white balls. One way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However, this would be a long and tedious process. 14.1.2 Using the shovel once Instead of performing an exhaustive count, let’s insert a shovel into the bowl as seen in Figure 14.2. Using the shovel, let’s remove \\(5 \\cdot 10 = 50\\) balls, as seen in Figure 14.3. FIGURE 14.2: Inserting a shovel into the bowl. FIGURE 14.3: Removing 50 balls from the bowl. Observe that 17 of the balls are red and thus 0.34 = 34% of the shovel’s balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count of all the balls in the bowl, our guess of 34% took much less time and energy to make. However, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% again? Maybe? What if we repeated this activity several times following the process shown in Figure 14.4? Would we obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% every time? Surely not. Let’s repeat this exercise several times with the help of 33 groups of friends to understand how the value differs with repetition. 14.1.3 Using the shovel 33 times Each of our 33 groups of friends will do the following: Use the shovel to remove 50 balls each. Count the number of red balls and thus compute the proportion of the 50 balls that are red. Return the balls into the bowl. Mix the contents of the bowl a little to not let a previous group’s results influence the next group’s. FIGURE 14.4: Repeating sampling activity 33 times. Each of our 33 groups of friends make note of their proportion of red balls from their sample collected. Each group then marks their proportion of their 50 balls that were red in the appropriate bin in a hand-drawn histogram as seen in Figure 14.5. FIGURE 14.5: Constructing a histogram of proportions. Recall from Section 12.5 that histograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in Figure 14.6. FIGURE 14.6: Hand-drawn histogram of first 10 out of 33 proportions. Observe the following in the histogram in Figure 14.6: At the low end, one group removed 50 balls from the bowl with proportion red between 0.20 and 0.25. At the high end, another group removed 50 balls from the bowl with proportion between 0.45 and 0.5 red. However, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution. The shape of this distribution is somewhat bell-shaped. Let’s construct this same hand-drawn histogram in R using your data visualization skills that you honed in Chapter 12. We saved our 33 groups of friends’ results in the tactile_prop_red data frame included in the moderndive package. Run the following to display the first 10 of 33 rows: tactile_prop_red # A tibble: 33 × 4 group replicate red_balls prop_red &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 Ilyas, Yohan 1 21 0.42 2 Morgan, Terrance 2 17 0.34 3 Martin, Thomas 3 21 0.42 4 Clark, Frank 4 21 0.42 5 Riddhi, Karina 5 18 0.36 6 Andrew, Tyler 6 19 0.38 7 Julia 7 19 0.38 8 Rachel, Lauren 8 11 0.22 9 Daniel, Caroline 9 15 0.3 10 Josh, Maeve 10 17 0.34 # ℹ 23 more rows Observe for each group that we have their names, the number of red_balls they obtained, and the corresponding proportion out of 50 balls that were red named prop_red. We also have a replicate variable enumerating each of the 33 groups. We chose this name because each row can be viewed as one instance of a replicated (in other words repeated) activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red. Let’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05 in Figure 14.7. This is a computerized and complete version of the partially completed hand-drawn histogram you saw in Figure 14.6. Note that setting boundary = 0.4 indicates that we want a binning scheme such that one of the bins’ boundary is at 0.4. This helps us to more closely align this histogram with the hand-drawn histogram in Figure 14.6. ggplot(tactile_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 33 proportions red&quot;) FIGURE 14.7: Distribution of 33 proportions based on 33 samples of size 50. 14.1.4 What did we just do? What we just demonstrated in this activity is the statistical concept of sampling. We would like to know the proportion of the bowl’s balls that are red. Because the bowl has a large number of balls, performing an exhaustive count of the red and white balls would be time-consuming. We thus extracted a sample of 50 balls using the shovel to make an estimate. Using this sample of 50 balls, we estimated the proportion of the bowl’s balls that are red to be 34%. Moreover, because we mixed the balls before each use of the shovel, the samples were randomly drawn. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Figure 14.7. This is known as the concept of sampling variation. The purpose of this sampling activity was to develop an understanding of two key concepts relating to sampling: Understanding the effect of sampling variation. Understanding the effect of sample size on sampling variation. In Section 14.2, we’ll mimic the hands-on sampling activity we just performed on a computer. This will allow us not only to repeat the sampling exercise much more than 33 times, but it will also allow us to use shovels with different numbers of slots than just 50. Afterwards, we’ll present you with definitions, terminology, and notation related to sampling in Section 14.3. As in many disciplines, such necessary background knowledge may seem inaccessible and even confusing at first. However, as with many difficult topics, if you truly understand the underlying concepts and practice, practice, practice, you’ll be able to master them. To tie the contents of this chapter to the real world, we’ll present an example of one of the most recognizable uses of sampling: polls. In Section 14.4 we’ll look at a particular case study: a 2013 poll on then U.S. President Barack Obama’s popularity among young Americans, conducted by Kennedy School’s Institute of Politics at Harvard University. To close this chapter, we’ll generalize the “sampling from a bowl” exercise to other sampling scenarios and present a theoretical result known as the Central Limit Theorem. Learning check (LC7.1) Why was it important to mix the bowl before we sampled the balls? (LC7.2) Why is it that our 33 groups of friends did not all have the same numbers of balls that were red out of 50, and hence different proportions red? 14.2 Virtual sampling In the previous Section 14.1, we performed a tactile sampling activity by hand. In other words, we used a physical bowl of balls and a physical shovel. We performed this sampling activity by hand first so that we could develop a firm understanding of the root ideas behind sampling. In this section, we’ll mimic this tactile sampling activity with a virtual sampling activity using a computer. In other words, we’ll use a virtual analog to the bowl of balls and a virtual analog to the shovel. 14.2.1 Using the virtual shovel once Let’s start by performing the virtual analog of the tactile sampling exercise we performed in Section 14.1. We first need a virtual analog of the bowl seen in Figure 14.1. To this end, we included a data frame named bowl in the moderndive package. The rows of bowl correspond exactly with the contents of the actual bowl. bowl # A tibble: 2,400 × 2 ball_ID color &lt;int&gt; &lt;chr&gt; 1 1 white 2 2 white 3 3 white 4 4 red 5 5 white 6 6 white 7 7 red 8 8 white 9 9 red 10 10 white # ℹ 2,390 more rows Observe that bowl has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable ball_ID is used as an identification variable as discussed in Subsection ??; none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white. View the contents of the bowl in RStudio’s data viewer and scroll through the contents to convince yourself that bowl is indeed a virtual analog of the actual bowl in Figure 14.1. Now that we have a virtual analog of our bowl, we next need a virtual analog to the shovel seen in Figure 14.2 to generate virtual samples of 50 balls. We’re going to use the rep_sample_n() function included in the moderndive package. This function allows us to take repeated, or replicated, samples of size n. virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) virtual_shovel # A tibble: 50 × 3 # Groups: replicate [1] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1970 white 2 1 842 red 3 1 2287 white 4 1 599 white 5 1 108 white 6 1 846 red 7 1 390 red 8 1 344 white 9 1 910 white 10 1 1485 white # ℹ 40 more rows Observe that virtual_shovel has 50 rows corresponding to our virtual sample of size 50. The ball_ID variable identifies which of the 2400 balls from bowl are included in our sample of 50 balls while color denotes its color. However, what does the replicate variable indicate? In virtual_shovel’s case, replicate is equal to 1 for all 50 rows. This is telling us that these 50 rows correspond to the first repeated/replicated use of the shovel, in our case our first sample. We’ll see shortly that when we “virtually” take 33 samples, replicate will take values between 1 and 33. Let’s compute the proportion of balls in our virtual sample that are red using the dplyr data wrangling verbs you learned in Chapter 13. First, for each of our 50 sampled balls, let’s identify if it is red or not using a test for equality with ==. Let’s create a new Boolean variable is_red using the mutate() function from Section 13.5: virtual_shovel %&gt;% mutate(is_red = (color == &quot;red&quot;)) # A tibble: 50 × 4 # Groups: replicate [1] replicate ball_ID color is_red &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; 1 1 1970 white FALSE 2 1 842 red TRUE 3 1 2287 white FALSE 4 1 599 white FALSE 5 1 108 white FALSE 6 1 846 red TRUE 7 1 390 red TRUE 8 1 344 white FALSE 9 1 910 white FALSE 10 1 1485 white FALSE # ℹ 40 more rows Observe that for every row where color == \"red\", the Boolean (logical) value TRUE is returned and for every row where color is not equal to \"red\", the Boolean FALSE is returned. Second, let’s compute the number of balls out of 50 that are red using the summarize() function. Recall from Section 13.3 that summarize() takes a data frame with many rows and returns a data frame with a single row containing summary statistics, like the mean() or median(). In this case, we use the sum(): virtual_shovel %&gt;% mutate(is_red = (color == &quot;red&quot;)) %&gt;% summarize(num_red = sum(is_red)) # A tibble: 1 × 2 replicate num_red &lt;int&gt; &lt;int&gt; 1 1 12 Why does this work? Because R treats TRUE like the number 1 and FALSE like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of balls where color is red. In our case, 12 of the 50 balls were red. However, you might have gotten a different number red because of the randomness of the virtual sampling. Third and lastly, let’s compute the proportion of the 50 sampled balls that are red by dividing num_red by 50: virtual_shovel %&gt;% mutate(is_red = color == &quot;red&quot;) %&gt;% summarize(num_red = sum(is_red)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 × 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 12 0.24 In other words, 24% of this virtual sample’s balls were red. Let’s make this code a little more compact and succinct by combining the first mutate() and the summarize() as follows: virtual_shovel %&gt;% summarize(num_red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 × 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 12 0.24 Great! 24% of virtual_shovel’s 50 balls were red! So based on this particular sample of 50 balls, our guess at the proportion of the bowl’s balls that are red is 24%. But remember from our earlier tactile sampling activity that if we repeat this sampling, we will not necessarily obtain the same value of 24% again. There will likely be some variation. In fact, our 33 groups of friends computed 33 such proportions whose distribution we visualized in Figure 14.6. We saw that these estimates varied. Let’s now perform the virtual analog of having 33 groups of students use the sampling shovel! 14.2.2 Using the virtual shovel 33 times Recall that in our tactile sampling exercise in Section 14.1, we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls. We then used these 33 samples to compute 33 proportions. In other words, we repeated/replicated using the shovel 33 times. We can perform this repeated/replicated sampling virtually by once again using our virtual shovel function rep_sample_n(), but by adding the reps = 33 argument. This is telling R that we want to repeat the sampling 33 times. We’ll save these results in a data frame called virtual_samples. While we provide a preview of the first 10 rows of virtual_samples in what follows, we highly suggest you scroll through its contents using RStudio’s spreadsheet viewer by running View(virtual_samples). virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 33) virtual_samples # A tibble: 1,650 × 3 # Groups: replicate [33] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 875 white 2 1 1851 red 3 1 1548 red 4 1 1975 white 5 1 835 white 6 1 16 white 7 1 327 white 8 1 1803 red 9 1 740 red 10 1 179 red # ℹ 1,640 more rows Observe in the spreadsheet viewer that the first 50 rows of replicate are equal to 1 while the next 50 rows of replicate are equal to 2. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 rows correspond to the second sample of 50 balls. This pattern continues for all reps = 33 replicates and thus virtual_samples has 33 \\(\\cdot\\) 50 = 1650 rows. Let’s now take virtual_samples and compute the resulting 33 proportions red. We’ll use the same dplyr verbs as before, but this time with an additional group_by() of the replicate variable. Recall from Section 13.4 that by assigning the grouping variable “meta-data” before we summarize(), we’ll obtain 33 different proportions red. We display a preview of the first 10 out of 33 rows: virtual_prop_red &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) virtual_prop_red # A tibble: 33 × 3 replicate red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 23 0.46 2 2 19 0.38 3 3 18 0.36 4 4 19 0.38 5 5 15 0.3 6 6 21 0.42 7 7 21 0.42 8 8 16 0.32 9 9 24 0.48 10 10 14 0.28 # ℹ 23 more rows As with our 33 groups of friends’ tactile samples, there is variation in the resulting 33 virtual proportions red. Let’s visualize this variation in a histogram in Figure 14.8. Note that we add binwidth = 0.05 and boundary = 0.4 arguments as well. Recall that setting boundary = 0.4 ensures a binning scheme with one of the bins’ boundaries at 0.4. Since the binwidth = 0.05 is also set, this will create bins with boundaries at 0.30, 0.35, 0.45, 0.5, etc. as well. ggplot(virtual_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 33 proportions red&quot;) FIGURE 14.8: Distribution of 33 proportions based on 33 samples of size 50. Observe that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 40% (for 11 out of 33 samples). Why do we have these differences in proportions red? Because of sampling variation. Let’s now compare our virtual results with our tactile results from the previous section in Figure 14.9. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped. FIGURE 14.9: Comparing 33 virtual and 33 tactile proportions red. Learning check (LC7.3) Why couldn’t we study the effects of sampling variation when we used the virtual shovel only once? Why did we need to take more than one virtual sample (in our case 33 virtual samples)? 14.2.3 Using the virtual shovel 1000 times Now say we want to study the effects of sampling variation not for 33 samples, but rather for a larger number of samples, say 1000. We have two choices at this point. We could have our groups of friends manually take 1000 samples of 50 balls and compute the corresponding 1000 proportions. However, this would be a tedious and time-consuming task. This is where computers excel: automating long and repetitive tasks while performing them quite quickly. Thus, at this point we will abandon tactile sampling in favor of only virtual sampling. Let’s once again use the rep_sample_n() function with sample size set to be 50 once again, but this time with the number of replicates reps set to 1000. Be sure to scroll through the contents of virtual_samples in RStudio’s viewer. virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) virtual_samples # A tibble: 50,000 × 3 # Groups: replicate [1,000] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1236 red 2 1 1944 red 3 1 1939 white 4 1 780 white 5 1 1956 white 6 1 1003 white 7 1 2113 white 8 1 2213 white 9 1 782 white 10 1 898 white # ℹ 49,990 more rows Observe that now virtual_samples has 1000 \\(\\cdot\\) 50 = 50,000 rows, instead of the 33 \\(\\cdot\\) 50 = 1650 rows from earlier. Using the same data wrangling code as earlier, let’s take the data frame virtual_samples with 1000 \\(\\cdot\\) 50 = 50,000 rows and compute the resulting 1000 proportions of red balls. virtual_prop_red &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) virtual_prop_red # A tibble: 1,000 × 3 replicate red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 18 0.36 2 2 19 0.38 3 3 20 0.4 4 4 15 0.3 5 5 17 0.34 6 6 16 0.32 7 7 23 0.46 8 8 23 0.46 9 9 15 0.3 10 10 18 0.36 # ℹ 990 more rows Observe that we now have 1000 replicates of prop_red, the proportion of 50 balls that are red. Using the same code as earlier, let’s now visualize the distribution of these 1000 replicates of prop_red in a histogram in Figure 14.10. ggplot(virtual_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 1000 proportions red&quot;) FIGURE 14.10: Distribution of 1000 proportions based on 1000 samples of size 50. Once again, the most frequently occurring proportions of red balls occur between 35% and 40%. Every now and then, we obtain proportions as low as between 20% and 25%, and others as high as between 55% and 60%. These are rare, however. Furthermore, observe that we now have a much more symmetric and smoother bell-shaped distribution. This distribution is, in fact, approximated well by a normal distribution. At this point we recommend you read the “Normal distribution” section (Appendix ??) for a brief discussion on the properties of the normal distribution. Learning check (LC7.4) Why did we not take 1000 “tactile” samples of 50 balls by hand? (LC7.5) Looking at Figure 14.10, would you say that sampling 50 balls where 30% of them were red is likely or not? What about sampling 50 balls where 10% of them were red? 14.2.4 Using different shovels Now say instead of just one shovel, you have three choices of shovels to extract a sample of balls with: shovels of size 25, 50, and 100. FIGURE 14.11: Three shovels to extract three different sample sizes. If your goal is still to estimate the proportion of the bowl’s balls that are red, which shovel would you choose? In our experience, most people would choose the largest shovel with 100 slots because it would yield the “best” guess of the proportion of the bowl’s balls that are red. Let’s define some criteria for “best” in this subsection. Using our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes! In other words, let’s use rep_sample_n() with size set to 25, 50, and 100, respectively, while keeping the number of repeated/replicated samples at 1000: Virtually use the appropriate shovel to generate 1000 samples with size balls. Compute the resulting 1000 replicates of the proportion of the shovel’s balls that are red. Visualize the distribution of these 1000 proportions red using a histogram. Run each of the following code segments individually and then compare the three resulting histograms. # Segment 1: sample size = 25 ------------------------------ # 1.a) Virtually use shovel 1000 times virtual_samples_25 &lt;- bowl %&gt;% rep_sample_n(size = 25, reps = 1000) # 1.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_25 &lt;- virtual_samples_25 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 25) # 1.c) Plot distribution via a histogram ggplot(virtual_prop_red_25, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 25 balls that were red&quot;, title = &quot;25&quot;) # Segment 2: sample size = 50 ------------------------------ # 2.a) Virtually use shovel 1000 times virtual_samples_50 &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) # 2.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_50 &lt;- virtual_samples_50 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) # 2.c) Plot distribution via a histogram ggplot(virtual_prop_red_50, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;50&quot;) # Segment 3: sample size = 100 ------------------------------ # 3.a) Virtually using shovel with 100 slots 1000 times virtual_samples_100 &lt;- bowl %&gt;% rep_sample_n(size = 100, reps = 1000) # 3.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_100 &lt;- virtual_samples_100 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 100) # 3.c) Plot distribution via a histogram ggplot(virtual_prop_red_100, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 100 balls that were red&quot;, title = &quot;100&quot;) For easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure 14.12. FIGURE 14.12: Comparing the distributions of proportion red for different sample sizes. Observe that as the sample size increases, the variation of the 1000 replicates of the proportion of red decreases. In other words, as the sample size increases, there are fewer differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure 14.12, all three histograms appear to center around roughly 40%. We can be numerically explicit about the amount of variation in our three sets of 1000 values of prop_red using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable (see Appendix ?? for a brief discussion on the properties of the standard deviation). For all three sample sizes, let’s compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the sd() summary function. # n = 25 virtual_prop_red_25 %&gt;% summarize(sd = sd(prop_red)) # n = 50 virtual_prop_red_50 %&gt;% summarize(sd = sd(prop_red)) # n = 100 virtual_prop_red_100 %&gt;% summarize(sd = sd(prop_red)) Let’s compare these three measures of distributional variation in Table 14.1. TABLE 14.1: Comparing standard deviations of proportions red for three different shovels Number of slots in shovel Standard deviation of proportions red 25 0.094 50 0.069 100 0.045 As we observed in Figure 14.12, as the sample size increases, the variation decreases. In other words, there is less variation in the 1000 values of the proportion red. So as the sample size increases, our guesses at the true proportion of the bowl’s balls that are red get more precise. Learning check (LC7.6) In Figure 14.12, we used shovels to take 1000 samples each, computed the resulting 1000 proportions of the shovel’s balls that were red, and then visualized the distribution of these 1000 proportions in a histogram. We did this for shovels with 25, 50, and 100 slots in them. As the size of the shovels increased, the histograms got narrower. In other words, as the size of the shovels increased from 25 to 50 to 100, did the 1000 proportions A. vary less, B. vary by the same amount, or C. vary more? (LC7.7) What summary statistic did we use to quantify how much the 1000 proportions red varied? A. The interquartile range B. The standard deviation C. The range: the largest value minus the smallest. 14.3 Sampling framework In both our tactile and our virtual sampling activities, we used sampling for the purpose of estimation. We extracted samples in order to estimate the proportion of the bowl’s balls that are red. We used sampling as a less time-consuming approach than performing an exhaustive count of all the balls. Our virtual sampling activity built up to the results shown in Figure 14.12 and Table 14.1: comparing 1000 proportions red based on samples of size 25, 50, and 100. This was our first attempt at understanding two key concepts relating to sampling for estimation: The effect of sampling variation on our estimates. The effect of sample size on sampling variation. Now that you have built some intuition relating to sampling, let’s now attach words and labels to the various concepts we’ve explored so far. Specifically in the next section, we’ll introduce terminology and notation as well as statistical definitions related to sampling. This will allow us to succinctly summarize and refer to the ideas behind sampling for the rest of this book. 14.3.1 Terminology and notation Let’s now attach words and labels to the various sampling concepts we’ve seen so far by introducing some terminology and mathematical notation. While they may seem daunting at first, we’ll make sure to tie each of them to sampling bowl activities you performed earlier. Furthermore, throughout this book we’ll give you plenty of opportunity for practice, as the best method for mastering these terms is repetition. The first set of terms and notation relate to populations: A population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case \\(N\\). A population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Canadians, the population parameter of interest is the population mean. A census is an exhaustive enumeration or counting of all \\(N\\) individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number \\(N\\) of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money). So in our sampling activities, the population is the collection of \\(N\\) = 2400 identically sized red and white balls in the bowl shown in Figure 14.1. Recall that we also represented the bowl “virtually” in the data frame bowl: bowl # A tibble: 2,400 × 2 ball_ID color &lt;int&gt; &lt;chr&gt; 1 1 white 2 2 white 3 3 white 4 4 red 5 5 white 6 6 white 7 7 red 8 8 white 9 9 red 10 10 white # ℹ 2,390 more rows The population parameter here is the proportion of the bowl’s balls that are red. Whenever we’re interested in a proportion of some value in a population, the population parameter has a specific name: the population proportion. We denote population proportions with the letter \\(p\\). We’ll see later on in Table 14.5 that we can also consider other types of population parameters, like population means and population regression slopes. In order to compute this population proportion \\(p\\) exactly, we need to first conduct a census by going through all \\(N\\) = 2400 and counting the number that are red. We then divide this count by 2400 to obtain the proportion red. You might be now asking yourself: “Wait. I understand that performing a census on the actual bowl would take a long time. But can’t we conduct a ‘virtual’ census using the virtual bowl?” You are absolutely correct! In fact when the authors of this book created the bowl data frame, they made its contents match the contents of actual bowl not by doing a census, but by reading the contents written on the box the bowl came in! Let’s conduct this “virtual” census by using the same dplyr verbs you used earlier to count the number of balls that are red: bowl %&gt;% summarize(red = sum(color == &quot;red&quot;)) # A tibble: 1 × 1 red &lt;int&gt; 1 900 Since 900 of the 2400 are red, the proportion is 900/2400 = 0.375 = 37.5%. So we know the value of the population parameter: in our case, the population proportion \\(p\\) is equal to 0.375. At this point, you might be further asking yourself: “If we had a way of knowing that the proportion of the balls that are red is 37.5%, then why did we do any sampling?” Great question! Normally, you wouldn’t do any sampling! However, the sampling activities we did this chapter are merely simulations of how sampling is done in real-life! We perform these simulations in order to study: The effect of sampling variation on our estimates. The effect of sample size on sampling variation. As we’ll see in Section 14.4 on polls, in real-life sampling not only will the population size \\(N\\) be very large making a census expensive, but sometimes we won’t even know how big the population is! For now however, we press on with our next set of terms and notation. The second set of terms and notation relate to samples: Sampling is the act of collecting a sample from the population, which we generally only do when we can’t perform a census. We mathematically denote the sample size using lower case \\(n\\), as opposed to upper case \\(N\\) which denotes the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). Thus sampling is a much cheaper alternative than performing a census. A point estimate, also known as a sample statistic, is a summary statistic computed from a sample that estimates the unknown population parameter. So previously we conducted sampling using a shovel with 50 slots to extract samples of size \\(n\\) = 50. To perform the virtual analog of this sampling, recall that we used the rep_sample_n() function as follows: virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) virtual_shovel # A tibble: 50 × 3 # Groups: replicate [1] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1970 white 2 1 842 red 3 1 2287 white 4 1 599 white 5 1 108 white 6 1 846 red 7 1 390 red 8 1 344 white 9 1 910 white 10 1 1485 white # ℹ 40 more rows Using the sample of 50 balls contained in virtual_shovel, we generated an estimate of the proportion of the bowl’s balls that are red prop_red virtual_shovel %&gt;% summarize(num_red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 × 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 12 0.24 So in our case, the value of prop_red is the point estimate of the population proportion \\(p\\) since it estimates the latter’s value. Furthermore, this point estimate has a specific name when considering proportions: the sample proportion. It is denoted using \\(\\widehat{p}\\) because it is a common convention in statistics to use a “hat” symbol to denote point estimates. The third set of terms relate to sampling methodology: the method used to collect samples. You’ll see here and throughout the rest of your book that the way you collect samples directly influences their quality. A sample is said to be representative if it roughly “looks like” the population. In other words, if the sample’s characteristics are a “good” representation of the population’s characteristics. We say a sample is generalizable if any results based on the sample can generalize to the population. In other words, if we can make “good” guesses about the population using the sample. We say a sampling procedure is biased if certain individuals in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every individual in a population has an equal chance of being sampled. We say a sample of \\(n\\) balls extracted using our shovel is representative of the population if it’s contents “roughly resemble” the contents of the bowl. If so, then the proportion of the shovel’s balls that are red can generalize to the proportion of the bowl’s \\(N\\) = 2400 balls that are red. Or expressed differently, \\(\\widehat{p}\\) is a “good guess” of \\(p\\). Now say we cheated when using the shovel and removed a number of white balls in favor of red balls. Then this sample would be biased towards red balls, and thus the sample would no longer be representative of the bowl. The fourth and final set of terms and notation relate to the goal of sampling: One way to ensure that a sample is unbiased and representative of the population is by using random sampling Inference is the act of “making a guess” about some unknown. Statistical inference is the act of making a guess about a population using a sample. In our case, since the rep_sample_n() function uses your computer’s random number generator, we were in fact performing random sampling. Let’s now put all four sets of terms and notation together, keeping our sampling activities in mind: Since we extracted a sample of \\(n\\) = 50 balls at random, we mixed all of the equally sized balls before using the shovel, then the contents of the shovel are unbiased and representative of the contents of the bowl, thus any result based on the shovel can generalize to the bowl, thus the sample proportion \\(\\widehat{p}\\) of the \\(n\\) = 50 balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the bowl’s \\(N\\) = 2400 balls that are red, thus instead of conducting a census of the 2400 balls in the bowl, we can infer about the bowl using the sample from the shovel. What you have been performing is statistical inference. This is one of the most important concepts in all of statistics. So much so, we included this term in the title of our book: “Statistical Inference via Data Science”. More generally speaking, If the sampling of a sample of size \\(n\\) is done at random, then the sample is unbiased and representative of the population of size \\(N\\), thus any result based on the sample can generalize to the population, thus the point estimate is a “good guess” of the unknown population parameter, thus instead of performing a census, we can infer about the population using sampling. In the upcoming Chapter 15 on confidence intervals, we’ll introduce the infer package, which makes statistical inference “tidy” and transparent. It is why this third portion of the book is called “Statistical inference via infer.” Learning check (LC7.8) In the case of our bowl activity, what is the population parameter? Do we know its value? (LC7.9) What would performing a census in our bowl activity correspond to? Why did we not perform a census? (LC7.10) What purpose do point estimates serve in general? What is the name of the point estimate specific to our bowl activity? What is its mathematical notation? (LC7.11) How did we ensure that our tactile samples using the shovel were random? (LC7.12) Why is it important that sampling be done at random? (LC7.13) What are we inferring about the bowl based on the samples using the shovel? 14.3.2 Statistical definitions To further attach words and labels to the various sampling concepts we’ve seen so far, we also introduce some important statistical definitions related to sampling. As a refresher of our 1000 repeated/replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section 14.2, let’s display Figure 14.12 again as Figure 14.13. FIGURE 14.13: Previously seen three distributions of the sample proportion \\(\\widehat{p}\\). These types of distributions have a special name: sampling distributions of point estimates. Their visualization displays the effect of sampling variation on the distribution of any point estimate, in this case, the sample proportion \\(\\widehat{p}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we can typically expect. Unfortunately, the term sampling distribution is often confused with a sample’s distribution which is merely the distribution of the values in a single sample. For example, observe the centers of all three sampling distributions: they are all roughly centered around 0.4 = 40%. Furthermore, observe that while we are somewhat likely to observe sample proportions of red balls of 0.2 = 20% when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions in Table 14.1, which we display again as Table 14.2: TABLE 14.2: Previously seen comparing standard deviations of proportions red for three different shovels Number of slots in shovel Standard deviation of proportions red 25 0.094 50 0.069 100 0.045 So as the sample size increases, the standard deviation of the proportion of red balls decreases. This type of standard deviation has another special name: standard error of a point estimate. Standard errors quantify the effect of sampling variation induced on our estimates. In other words, they quantify how much we can expect different proportions of a shovel’s balls that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases. Similarly to confusion between sampling distributions with a sample’s distribution, people often confuse the standard error with the standard deviation. This is especially the case since a standard error is itself a kind of standard deviation. The best advice we can give is that a standard error is merely a kind of standard deviation: the standard deviation of any point estimate from sampling. In other words, all standard errors are standard deviations, but not every standard deviation is necessarily a standard error. To help reinforce these concepts, let’s re-display Figure 14.12 but using our new terminology, notation, and definitions relating to sampling in Figure 14.14. FIGURE 14.14: Three sampling distributions of the sample proportion \\(\\widehat{p}\\). Furthermore, let’s re-display Table 14.1 but using our new terminology, notation, and definitions relating to sampling in Table 14.3. TABLE 14.3: Standard errors of the sample proportion based on sample sizes of 25, 50, and 100 Sample size (n) Standard error of \\(\\widehat{p}\\) n = 25 0.094 n = 50 0.069 n = 100 0.045 Remember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error. In fact, in Subsection 14.6.2 we’ll see that the standard error for the sample proportion \\(\\widehat{p}\\) can also be approximated via a mathematical theory-based formula, a formula that has \\(n\\) in the denominator. Learning check (LC7.14) What purpose did the sampling distributions serve? (LC7.15) What does the standard error of the sample proportion \\(\\widehat{p}\\) quantify? 14.3.3 The moral of the story Let’s recap this section so far. We’ve seen that if a sample is generated at random, then the resulting point estimate is a “good guess” of the true unknown population parameter. In our sampling activities, since we made sure to mix the balls first before extracting a sample with the shovel, the resulting sample proportion \\(\\widehat{p}\\) of the shovel’s balls that were red was a “good guess” of the population proportion \\(p\\) of the bowl’s balls that were red. However, what do we mean by our point estimate being a “good guess”? Sometimes, we’ll get an estimate that is less than the true value of the population parameter, while at other times we’ll get an estimate that is greater. This is due to sampling variation. However, despite this sampling variation, our estimates will “on average” be correct and thus will be centered at the true value. This is because our sampling was done at random and thus in an unbiased fashion. In our sampling activities, sometimes our sample proportion \\(\\widehat{p}\\) was less than the true population proportion \\(p\\), while at other times it was greater. This was due to the sampling variability. However, despite this sampling variation, our sample proportions \\(\\widehat{p}\\) were “on average” correct and thus were centered at the true value of the population proportion \\(p\\). This is because we mixed our bowl before taking samples and thus the sampling was done at random and thus in an unbiased fashion. This is also known as having an accurate estimate. Recall from earlier that the value of the population proportion \\(p\\) of the \\(N\\) = 2400 balls in the bowl was 900/2400 = 0.375 = 37.5%. We computed this value by performing a virtual census of bowl. Let’s re-display our sampling distributions from Figures 14.12 and 14.14, but now with a vertical red line marking the true population proportion \\(p\\) of balls that are red = 37.5% in Figure 14.15. We see that while there is a certain amount of error in the sample proportions \\(\\widehat{p}\\) for all three sampling distributions, on average the \\(\\widehat{p}\\) are centered at the true population proportion red \\(p\\). FIGURE 14.15: Three sampling distributions with population proportion \\(p\\) marked by vertical line. We also saw in this section that as your sample size \\(n\\) increases, your point estimates will vary less and less and be more and more concentrated around the true population parameter. This variation is quantified by the decreasing standard error. In other words, the typical error of your point estimates will decrease. In our sampling exercise, as the sample size increased, the variation of our sample proportions \\(\\widehat{p}\\) decreased. You can observe this behavior in Figure 14.15. This is also known as having a precise estimate. So random sampling ensures our point estimates are accurate, while on the other hand having a large sample size ensures our point estimates are precise. While the terms “accuracy” and “precision” may sound like they mean the same thing, there is a subtle difference. Accuracy describes how “on target” our estimates are, whereas precision describes how “consistent” our estimates are. Figure 14.16 illustrates the difference. FIGURE 14.16: Comparing accuracy and precision. At this point, you might be asking yourself: “Why did we take 1000 repeated samples of size n = 25, 50, and 100? Shouldn’t we be taking only one sample that’s as large as possible?”. If you did ask yourself these questions, your suspicion is correct! Recall from earlier when we asked ourselves “If we had a way of knowing that the proportion of the balls that are red is 37.5%, then why did we do any sampling?” Similarly, we took 1000 repeated samples as a simulation of how sampling is done in real-life! We used these simulations to study: The effect of sampling variation on our estimates. The effect of sample size on sampling variation. This is not how sampling is done in real life! In a real-life scenario, we wouldn’t take 1000 repeated/replicated samples, but rather a single sample that’s as large as we can afford. In Section 14.4, we’re going to study a real-life example of sampling: polls. Learning check (LC7.16) The table that follows is a version of Table 14.3 matching sample sizes \\(n\\) to different standard errors of the sample proportion \\(\\widehat{p}\\), but with the rows randomly re-ordered and the sample sizes removed. Fill in the table by matching the correct sample sizes to the correct standard errors. TABLE 14.4: Standard errors of \\(\\widehat{p}\\) based on n = 25, 50, 100 Sample size Standard error of \\(\\widehat{p}\\) n = 0.094 n = 0.045 n = 0.069 For the following four Learning checks, let the estimate be the sample proportion \\(\\widehat{p}\\): the proportion of a shovel’s balls that were red. It estimates the population proportion \\(p\\): the proportion of the bowl’s balls that were red. (LC7.17) What is the difference between an accurate and a precise estimate? (LC7.18) How do we ensure that an estimate is accurate? How do we ensure that an estimate is precise? (LC7.19) In a real-life situation, we would not take 1000 different samples to infer about a population, but rather only one. Then, what was the purpose of our exercises where we took 1000 different samples? (LC7.20) Figure 14.16 with the targets shows four combinations of “accurate versus precise” estimates. Draw four corresponding sampling distributions of the sample proportion \\(\\widehat{p}\\), like the one in the leftmost plot in Figure 14.15. 14.4 Case study: Polls Let’s now switch gears to a more realistic sampling scenario than our bowl activity: a poll. In practice, pollsters do not take 1000 repeated samples as we did in our previous sampling activities, but rather take only a single sample that’s as large as possible. On December 4, 2013, National Public Radio in the US reported on a poll of President Obama’s approval rating among young Americans aged 18-29 in an article, “Poll: Support For Obama Among Young Americans Eroding.” The poll was conducted by the Kennedy School’s Institute of Politics at Harvard University. A quote from the article: After voting for him in large numbers in 2008 and 2012, young Americans are souring on President Obama. According to a new Harvard University Institute of Politics poll, just 41 percent of millennials — adults ages 18-29 — approve of Obama’s job performance, his lowest-ever standing among the group and an 11-point drop from April. Let’s tie elements of the real-life poll in this news article with our “tactile” and “virtual” bowl activity from Sections 14.1 and 14.2 using the terminology, notations, and definitions we learned in Section 14.3. You’ll see that our sampling activity with the bowl is an idealized version of what pollsters are trying to do in real life. First, who is the (study) population of \\(N\\) individuals or observations of interest? Bowl: \\(N\\) = 2400 identically sized red and white balls Obama poll: \\(N\\) = ? young Americans aged 18-29 Second, what is the population parameter? Bowl: The population proportion \\(p\\) of all the balls in the bowl that are red. Obama poll: The population proportion \\(p\\) of all young Americans who approve of Obama’s job performance. Third, what would a census look like? Bowl: Manually going over all \\(N\\) = 2400 balls and exactly computing the population proportion \\(p\\) of the balls that are red. Obama poll: Locating all \\(N\\) young Americans and asking them all if they approve of Obama’s job performance. In this case, we don’t even know what the population size \\(N\\) is! Fourth, how do you perform sampling to obtain a sample of size \\(n\\)? Bowl: Using a shovel with \\(n\\) slots. Obama poll: One method is to get a list of phone numbers of all young Americans and pick out \\(n\\) phone numbers. In this poll’s case, the sample size of this poll was \\(n = 2089\\) young Americans. Fifth, what is your point estimate also known as the sample statistic of the unknown population parameter? Bowl: The sample proportion \\(\\widehat{p}\\) of the balls in the shovel that were red. Obama poll: The sample proportion \\(\\widehat{p}\\) of young Americans in the sample that approve of Obama’s job performance. In this poll’s case, \\(\\widehat{p} = 0.41 = 41\\%\\), the quoted percentage in the second paragraph of the article. Sixth, is the sampling procedure representative? Bowl: Are the contents of the shovel representative of the contents of the bowl? Because we mixed the bowl before sampling, we can feel confident that they are. Obama poll: Is the sample of \\(n = 2089\\) young Americans representative of all young Americans aged 18-29? This depends on whether the sampling was random. Seventh, are the samples generalizable to the greater population? Bowl: Is the sample proportion \\(\\widehat{p}\\) of the shovel’s balls that are red a “good guess” of the population proportion \\(p\\) of the bowl’s balls that are red? Given that the sample was representative, the answer is yes. Obama poll: Is the sample proportion \\(\\widehat{p}\\) = 0.41 of the sample of young Americans who supported Obama a “good guess” of the population proportion \\(p\\) of all young Americans who supported Obama at this time in 2013? In other words, can we confidently say that roughly 41% of all young Americans approved of Obama at the time of the poll? Again, this depends on whether the sampling was random. Eighth, is the sampling procedure unbiased? In other words, do all observations have an equal chance of being included in the sample? Bowl: Since each ball was equally sized and we mixed the bowl before using the shovel, each ball had an equal chance of being included in a sample and hence the sampling was unbiased. Obama poll: Did all young Americans have an equal chance at being represented in this poll? Again, this depends on whether the sampling was random. Ninth and lastly, was the sampling done at random? Bowl: As long as you mixed the bowl sufficiently before sampling, your samples would be random. Obama poll: Was the sample conducted at random? We can’t answer this question without knowing about the sampling methodology used by Kennedy School’s Institute of Politics at Harvard University. We’ll discuss this more at the end of this section. In other words, the poll by Kennedy School’s Institute of Politics at Harvard University can be thought of as an instance of using the shovel to sample balls from the bowl. Furthermore, if another polling company conducted a similar poll of young Americans at roughly the same time, they would likely get a different estimate than 41%. This is due to sampling variation. Let’s now revisit the sampling paradigm from Subsection 14.3.1: In general: If the sampling of a sample of size \\(n\\) is done at random, then the sample is unbiased and representative of the population of size \\(N\\), thus any result based on the sample can generalize to the population, thus the point estimate is a “good guess” of the unknown population parameter, thus instead of performing a census, we can infer about the population using sampling. Specific to the bowl: Since we extracted a sample of \\(n\\) = 50 balls at random, in other words we mixed all of the equally sized balls before using the shovel, then the contents of the shovel are unbiased and representative of the contents of the bowl, thus any result based on the shovel can generalize to the bowl, thus the sample proportion \\(\\widehat{p}\\) of the \\(n\\) = 50 balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the bowl’s \\(N\\) = 2400 balls that are red, thus instead of conducting a census of the 2400 balls in the bowl, we can infer about the bowl using the sample from the shovel. Specific to the Obama poll: If we had a way of contacting a randomly chosen sample of 2089 young Americans and polling their approval of President Obama in 2013, then these 2089 young Americans would be an unbiased and representative sample of all young Americans in 2013, thus any results based on this sample of 2089 young Americans can generalize to the entire population of all young Americans in 2013, thus the reported sample approval rating of 41% of these 2089 young Americans is a good guess of the true approval rating among all young Americans in 2013, thus instead of performing an expensive census of all young Americans in 2013, we can infer about all young Americans in 2013 using polling. So as you can see, it was critical for the sample obtained by Kennedy School’s Institute of Politics at Harvard University to be truly random in order to infer about all young Americans’ opinions about Obama. Was their sample truly random? It’s hard to answer such questions without knowing about the sampling methodology they used. For example, if this poll was conducted using only mobile phone numbers, people without mobile phones would be left out and therefore not represented in the sample. What about if Kennedy School’s Institute of Politics at Harvard University conducted this poll on an internet news site? Then people who don’t read this particular internet news site would be left out. Ensuring that our samples were random was easy to do in our sampling bowl exercises; however, in a real-life situation like the Obama poll, this is much harder to do. Learning check Comment on the representativeness of the following sampling methodologies: (LC7.21) The Royal Air Force wants to study how resistant all their airplanes are to bullets. They study the bullet holes on all the airplanes on the tarmac after an air battle against the Luftwaffe (German Air Force). (LC7.22) Imagine it is 1993, a time when almost all households had landlines. You want to know the average number of people in each household in your city. You randomly pick out 500 phone numbers from the phone book and conduct a phone survey. (LC7.23) You want to know the prevalence of illegal downloading of TV shows among students at a local college. You get the emails of 100 randomly chosen students and ask them, “How many times did you download a pirated TV show last week?”. (LC7.24) A local college administrator wants to know the average income of all graduates in the last 10 years. So they get the records of five randomly chosen graduates, contact them, and obtain their answers. 14.5 Central Limit Theorem This chapter began with (virtual) access to a large bowl of balls (our population) and a desire to figure out the proportion of red balls. Despite having access to this population, in reality, you almost never will have access to the population, either because the population is too large, ever changing, or too expensive to take a census of. Accepting this reality means accepting that we need to use statistical inference. In Section 14.3.1, we stated that “statistical inference is the act of making a guess about a population using a sample.” But how do we do this inference? In the previous section, we defined the sampling framework only to state that in reality we take one large sample, instead of many samples as done in the sampling framework (which we modeled physically by taking many samples from the bowl). In reality, we take only one sample and use that one sample to make statements about the population parameter. This ability of making statements about the population is allowable by a famous theorem, or mathematically proven truth, called the Central Limit Theorem. What you visualized in Figures 14.12 and 14.14 and summarized in Tables 14.1 and 14.3 was a demonstration of this theorem. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow. In other words, as our sample size gets larger (1) the sampling distribution of a point estimate (like a sample proportion) increasingly follows a normal distribution and (2) the variation of these sampling distributions gets smaller, as quantified by their standard errors. We discuss the properties of the normal distribution in Appendix ??. Shuyi Chiou, Casey Dunn, and Pathikrit Bhattacharyya created a 3-minute and 38-second video at https://youtu.be/jvoxEYmQHNM explaining this crucial statistical theorem using the average weight of wild bunny rabbits and the average wingspan of dragons as examples. Figure 14.17 shows a preview of this video. FIGURE 14.17: Preview of Central Limit Theorem video. Here’s what is so surprising about the Central Limit Theorem: regardless of the shape of the underlying population distribution, the sampling distribution of means (such as the sample mean of bunny weights or the sample mean of the length of dragon wings) and proportions (such as the sample proportion red in our shovels) will be normal. Normal distributions are defined by where they are centered and how wide they are, and the Central Limit Theorem gives us both: The sampling distribution of the point estimate is centered at the true population parameter We have an estimate for how wide the sampling distribution of the point estimate is, given by the standard error (which we will discuss further in Chapter 15) What the Central Limit Theorem creates for us is a ladder between a single sample and the population. By the Central Limit Theorem, we can say that (1) our sample’s point estimate is drawn from a normal distribution centered at the true population parameter and (2)that the width of that normal distribution is governed by the standard error of our point estimate. Relating this to our bowl, if we pull one sample and get the sample proportion of red balls \\(\\widehat{p}\\), this value of \\(\\widehat{p}\\) is drawn from the normal curve centered at the true population proportion of red balls \\(p\\) with the computed standard error. 14.6 Conclusion 14.6.1 Sampling scenarios In this chapter, we performed both tactile and virtual sampling exercises to infer about an unknown proportion. We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion \\(\\widehat{p}\\) to estimate the population proportion \\(p\\). However, we are not just limited to scenarios related to proportions. In other words, we can use sampling to estimate other population parameters using other point estimates as well. We present four more such scenarios in Table 14.5. TABLE 14.5: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) or \\(\\widehat{\\mu}_1 - \\widehat{\\mu}_2\\) 5 Population regression slope \\(\\beta_1\\) Fitted regression slope \\(b_1\\) or \\(\\widehat{\\beta}_1\\) In the rest of this book, we’ll cover all the remaining scenarios as follows: In Chapter 15, we’ll cover examples of statistical inference for Scenario 2: The mean age \\(\\mu\\) of all pennies in circulation in the US. Scenario 3: The difference \\(p_1 - p_2\\) in the proportion of people who yawn when seeing someone else yawn first minus the proportion of people who yawn without seeing someone else yawn first. This is an example of two-sample inference. In Chapter 16, we’ll cover an example of statistical inference for Scenario 4: The difference \\(\\mu_1 - \\mu_2\\) in mean IMDb ratings for action and romance movies. This is another example of two-sample inference. In Chapter 18, we’ll cover an example of statistical inference for regression by revisiting the regression models for teaching score as a function of various instructor demographic variables you saw in Chapters 17 and 19. Scenario 5: The slope \\(\\beta_1\\) of the population regression line. 14.6.2 Theory-based standard-errors There exists in many cases a formula that approximates the standard error! In the case of our bowl where we used the sample proportion red \\(\\widehat{p}\\) to estimate the proportion of the bowl’s balls that are red, the formula that approximates the standard error for the sample proportion \\(\\widehat{p}\\) is: \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] For example, say you sampled \\(n = 50\\) balls and observed 21 red balls. This equals a sample proportion \\(\\widehat{p}\\) of 21/50 = 0.42. So, using the formula, an approximation of the standard error of \\(\\widehat{p}\\) is \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{50}} = \\sqrt{0.004872} = 0.0698 \\approx 0.070\\] Say instead you sampled \\(n = 100\\) balls and observed 42 red balls. This once again equals a sample proportion \\(\\widehat{p}\\) of 42/100 = 0.42. However using the formula, an approximation of the standard error of \\(\\widehat{p}\\) is now \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{100}} = \\sqrt{0.002436} = 0.0494\\] Observe that the standard error has gone down from 0.0698 to 0.0494. In other words, the “typical” error of our estimates using \\(n\\) = 100 will go down relative to \\(n\\) = 50 and hence be more precise. Recall that we illustrated the difference between accuracy and precision of estimates in Figure 14.16. The key observation to make in the formula is that there is an \\(n\\) in the denominator. As the sample size \\(n\\) increases, the standard error decreases. We’ve demonstrated this fact using our virtual shovels in Subsection 14.3.3. Furthermore, this is one of the key messages of the Central Limit Theorem we saw in Subsection 14.5: as the sample size \\(n\\) increases, the distribution of averages gets narrower as quantified by the standard deviation of the sampling distribution of the sample mean. This standard deviation of the sampling distribution of the sample means in turn has a special name: the standard error of the sample mean. Why is this formula true? Unfortunately, we don’t have the tools at this point to prove this; you’ll need to take a more advanced course in probability and statistics. (It is related to the concepts of Bernoulli and Binomial Distributions. You can read more about its derivation here if you like.) 14.6.3 Additional resources An R script file of all R code used in this chapter is available here. 14.6.4 What’s to come? Recall in our Obama poll case study in Section 14.4 that based on this particular sample, the best guess by Kennedy School’s Institute of Politics at Harvard University of the U.S. President Obama’s approval rating among all young Americans was 41%. However, this isn’t the end of the story. If you read the article further, it states: The online survey of 2,089 adults was conducted from Oct. 30 to Nov. 11, just weeks after the federal government shutdown ended and the problems surrounding the implementation of the Affordable Care Act began to take center stage. The poll’s margin of error was plus or minus 2.1 percentage points. Note the term margin of error, which here is “plus or minus 2.1 percentage points.” Most polls won’t produce an estimate that’s perfectly right; there will always be a certain amount of error caused by sampling variation. The margin of error of plus or minus 2.1 percentage points is saying that a typical range of errors for polls of this type is about \\(\\pm\\) 2.1%, in words from about 2.1% too small to about 2.1% too big. We can restate this as the interval of \\([41\\% - 2.1\\%, 41\\% + 2.1\\%] = [37.9\\%, 43.1\\%]\\) (this notation indicates the interval contains all values between 37.9% and 43.1%, including the end points of 37.9% and 43.1%). We’ll see in the next chapter that such intervals are known as confidence intervals. "],["15-confidence-intervals.html", "Chapter 15 Bootstrapping and Confidence Intervals 15.1 Pennies activity 15.2 Computer simulation of resampling 15.3 Understanding confidence intervals 15.4 Constructing confidence intervals 15.5 Interpreting confidence intervals 15.6 Case study: Is yawning contagious? 15.7 Conclusion", " Chapter 15 Bootstrapping and Confidence Intervals In Chapter 14, we studied sampling. We started with a “tactile” exercise where we wanted to know the proportion of balls in the sampling bowl in Figure 14.1 that are red. While we could have performed an exhaustive count, this would have been a tedious process. So instead, we used a shovel to extract a sample of 50 balls and used the resulting proportion that were red as an estimate. Furthermore, we made sure to mix the bowl’s contents before every use of the shovel. Because of the randomness created by the mixing, different uses of the shovel yielded different proportions red and hence different estimates of the proportion of the bowl’s balls that are red. We then mimicked this “tactile” sampling exercise with an equivalent “virtual” sampling exercise performed on the computer. Using our computer’s random number generator, we quickly mimicked the above sampling procedure a large number of times. In Subsection 14.2, we quickly repeated this sampling procedure 1000 times, using three different “virtual” shovels with 25, 50, and 100 slots. We visualized these three sets of 1000 estimates in Figure 14.15 and saw that as the sample size increased, the variation in the estimates decreased. In doing so, what we did was construct sampling distributions. The motivation for taking 1000 repeated samples and visualizing the resulting estimates was to study how these estimates varied from one sample to another; in other words, we wanted to study the effect of sampling variation. We quantified the variation of these estimates using their standard deviation, which has a special name: the standard error. In particular, we saw that as the sample size increased from 25 to 50 to 100, the standard error decreased and thus the sampling distributions narrowed. Larger sample sizes led to more precise estimates that varied less around the center. We then tied these sampling exercises to terminology and mathematical notation related to sampling in Subsection 14.3.1. Our study population was the large bowl with \\(N\\) = 2400 balls, while the population parameter, the unknown quantity of interest, was the population proportion \\(p\\) of the bowl’s balls that were red. Since performing a census would be expensive in terms of time and energy, we instead extracted a sample of size \\(n\\) = 50. The point estimate, also known as a sample statistic, used to estimate \\(p\\) was the sample proportion \\(\\widehat{p}\\) of these 50 sampled balls that were red. Furthermore, since the sample was obtained at random, it can be considered as unbiased and representative of the population. Thus any results based on the sample could be generalized to the population. Therefore, the proportion of the shovel’s balls that were red was a “good guess” of the proportion of the bowl’s balls that are red. In other words, we used the sample to infer about the population. However, as described in Section 14.2, both the tactile and virtual sampling exercises are not what one would do in real life; this was merely an activity used to study the effects of sampling variation. In a real-life situation, we would not take 1000 samples of size \\(n\\), but rather take a single representative sample that’s as large as possible. Additionally, we knew that the true proportion of the bowl’s balls that were red was 37.5%. In a real-life situation, we will not know what this value is. Because if we did, then why would we take a sample to estimate it? An example of a realistic sampling situation would be a poll, like the Obama poll you saw in Section 14.4. Pollsters did not know the true proportion of all young Americans who supported President Obama in 2013, and thus they took a single sample of size \\(n\\) = 2089 young Americans to estimate this value. So how does one quantify the effects of sampling variation when you only have a single sample to work with? You cannot directly study the effects of sampling variation when you only have one sample. One common method to study this is bootstrapping resampling, which will be the focus of the earlier sections of this chapter. Furthermore, what if we would like not only a single estimate of the unknown population parameter, but also a range of highly plausible values? Going back to the Obama poll article, it stated that the pollsters’ estimate of the proportion of all young Americans who supported President Obama was 41%. But in addition it stated that the poll’s “margin of error was plus or minus 2.1 percentage points.” This “plausible range” was [41% - 2.1%, 41% + 2.1%] = [38.9%, 43.1%]. This range of plausible values is what’s known as a confidence interval, which will be the focus of the later sections of this chapter. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section ?? that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to tidy format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section ?? for information on how to install and load R packages. library(tidyverse) library(moderndive) library(infer) 15.1 Pennies activity As we did in Chapter 14, we’ll begin with a hands-on tactile activity. 15.1.1 What is the average year on US pennies in 2019? Try to imagine all the pennies being used in the United States in 2019. That’s a lot of pennies! Now say we’re interested in the average year of minting of all these pennies. One way to compute this value would be to gather up all pennies being used in the US, record the year, and compute the average. However, this would be near impossible! So instead, let’s collect a sample of 50 pennies from a local bank in downtown Northampton, Massachusetts, USA as seen in Figure 15.1. FIGURE 15.1: Collecting a sample of 50 US pennies from a local bank. An image of these 50 pennies can be seen in Figure 15.2. For each of the 50 pennies starting in the top left, progressing row-by-row, and ending in the bottom right, we assigned an “ID” identification variable and marked the year of minting. FIGURE 15.2: 50 US pennies labelled. The moderndive package contains this data on our 50 sampled pennies in the pennies_sample data frame: pennies_sample # A tibble: 50 × 2 ID year &lt;int&gt; &lt;dbl&gt; 1 1 2002 2 2 1986 3 3 2017 4 4 1988 5 5 2008 6 6 1983 7 7 2008 8 8 1996 9 9 2004 10 10 2000 # ℹ 40 more rows The pennies_sample data frame has 50 rows corresponding to each penny with two variables. The first variable ID corresponds to the ID labels in Figure 15.2, whereas the second variable year corresponds to the year of minting saved as a numeric variable, also known as a double (dbl). Based on these 50 sampled pennies, what can we say about all US pennies in 2019? Let’s study some properties of our sample by performing an exploratory data analysis. Let’s first visualize the distribution of the year of these 50 pennies using our data visualization tools from Chapter 12. Since year is a numerical variable, we use a histogram in Figure 15.3 to visualize its distribution. ggplot(pennies_sample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) FIGURE 15.3: Distribution of year on 50 US pennies. Observe a slightly left-skewed distribution, since most pennies fall between 1980 and 2010 with only a few pennies older than 1970. What is the average year for the 50 sampled pennies? Eyeballing the histogram it appears to be around 1990. Let’s now compute this value exactly using our data wrangling tools from Chapter 13. x_bar &lt;- pennies_sample %&gt;% summarize(mean_year = mean(year)) x_bar # A tibble: 1 × 1 mean_year &lt;dbl&gt; 1 1995.44 Thus, if we’re willing to assume that pennies_sample is a representative sample from all US pennies, a “good guess” of the average year of minting of all US pennies would be 1995.44. In other words, around 1995. This should all start sounding similar to what we did previously in Chapter 14! In Chapter 14, our study population was the bowl of \\(N\\) = 2400 balls. Our population parameter was the population proportion of these balls that were red, denoted by \\(p\\). In order to estimate \\(p\\), we extracted a sample of 50 balls using the shovel. We then computed the relevant point estimate: the sample proportion of these 50 balls that were red, denoted mathematically by \\(\\widehat{p}\\). Here our population is \\(N\\) = whatever the number of pennies are being used in the US, a value which we don’t know and probably never will. The population parameter of interest is now the population mean year of all these pennies, a value denoted mathematically by the Greek letter \\(\\mu\\) (pronounced “mu”). In order to estimate \\(\\mu\\), we went to the bank and obtained a sample of 50 pennies and computed the relevant point estimate: the sample mean year of these 50 pennies, denoted mathematically by \\(\\overline{x}\\) (pronounced “x-bar”). An alternative and more intuitive notation for the sample mean is \\(\\widehat{\\mu}\\). However, this is unfortunately not as commonly used, so in this book we’ll stick with convention and always denote the sample mean as \\(\\overline{x}\\). We summarize the correspondence between the sampling bowl exercise in Chapter 14 and our pennies exercise in Table 15.1, which are the first two rows of the previously seen Table 14.5. TABLE 15.1: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) Going back to our 50 sampled pennies in Figure 15.2, the point estimate of interest is the sample mean \\(\\overline{x}\\) of 1995.44. This quantity is an estimate of the population mean year of all US pennies \\(\\mu\\). Recall that we also saw in Chapter 14 that such estimates are prone to sampling variation. For example, in this particular sample in Figure 15.2, we observed three pennies with the year 1999. If we sampled another 50 pennies, would we observe exactly three pennies with the year 1999 again? More than likely not. We might observe none, one, two, or maybe even all 50! The same can be said for the other 26 unique years that are represented in our sample of 50 pennies. To study the effects of sampling variation in Chapter 14, we took many samples, something we could easily do with our shovel. In our case with pennies, however, how would we obtain another sample? By going to the bank and getting another roll of 50 pennies. Say we’re feeling lazy, however, and don’t want to go back to the bank. How can we study the effects of sampling variation using our single sample? We will do so using a technique known as bootstrap resampling with replacement, which we now illustrate. 15.1.2 Resampling once Step 1: Let’s print out identically sized slips of paper representing our 50 pennies as seen in Figure 15.4. FIGURE 15.4: Step 1: 50 slips of paper representing 50 US pennies. Step 2: Put the 50 slips of paper into a hat or tuque as seen in Figure 15.5. FIGURE 15.5: Step 2: Putting 50 slips of paper in a hat. Step 3: Mix the hat’s contents and draw one slip of paper at random as seen in Figure 15.6. Record the year. FIGURE 15.6: Step 3: Drawing one slip of paper at random. Step 4: Put the slip of paper back in the hat! In other words, replace it as seen in Figure 15.7. FIGURE 15.7: Step 4: Replacing slip of paper. Step 5: Repeat Steps 3 and 4 a total of 49 more times, resulting in 50 recorded years. What we just performed was a resampling of the original sample of 50 pennies. We are not sampling 50 pennies from the population of all US pennies as we did in our trip to the bank. Instead, we are mimicking this act by resampling 50 pennies from our original sample of 50 pennies. Now ask yourselves, why did we replace our resampled slip of paper back into the hat in Step 4? Because if we left the slip of paper out of the hat each time we performed Step 4, we would end up with the same 50 original pennies! In other words, replacing the slips of paper induces sampling variation. Being more precise with our terminology, we just performed a resampling with replacement from the original sample of 50 pennies. Had we left the slip of paper out of the hat each time we performed Step 4, this would be resampling without replacement. Let’s study our 50 resampled pennies via an exploratory data analysis. First, let’s load the data into R by manually creating a data frame pennies_resample of our 50 resampled values. We’ll do this using the tibble() command from the dplyr package. Note that the 50 values you resample will almost certainly not be the same as ours given the inherent randomness. pennies_resample &lt;- tibble( year = c(1976, 1962, 1976, 1983, 2017, 2015, 2015, 1962, 2016, 1976, 2006, 1997, 1988, 2015, 2015, 1988, 2016, 1978, 1979, 1997, 1974, 2013, 1978, 2015, 2008, 1982, 1986, 1979, 1981, 2004, 2000, 1995, 1999, 2006, 1979, 2015, 1979, 1998, 1981, 2015, 2000, 1999, 1988, 2017, 1992, 1997, 1990, 1988, 2006, 2000) ) The 50 values of year in pennies_resample represent a resample of size 50 from the original sample of 50 pennies. We display the 50 resampled pennies in Figure 15.8. FIGURE 15.8: 50 resampled US pennies labelled. Let’s compare the distribution of the numerical variable year of our 50 resampled pennies with the distribution of the numerical variable year of our original sample of 50 pennies in Figure 15.9. ggplot(pennies_resample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) + labs(title = &quot;Resample of 50 pennies&quot;) ggplot(pennies_sample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) + labs(title = &quot;Original sample of 50 pennies&quot;) FIGURE 15.9: Comparing year in the resampled pennies_resample with the original sample pennies_sample. Observe in Figure 15.9 that while the general shapes of both distributions of year are roughly similar, they are not identical. Recall from the previous section that the sample mean of the original sample of 50 pennies from the bank was 1995.44. What about for our resample? Any guesses? Let’s have dplyr help us out as before: pennies_resample %&gt;% summarize(mean_year = mean(year)) # A tibble: 1 × 1 mean_year &lt;dbl&gt; 1 1996 We obtained a different mean year of 1996. This variation is induced by the resampling with replacement we performed earlier. What if we repeated this resampling exercise many times? Would we obtain the same mean year each time? In other words, would our guess at the mean year of all pennies in the US in 2019 be exactly 1996 every time? Just as we did in Chapter 14, let’s perform this resampling activity with the help of some of our friends: 35 friends in total. 15.1.3 Resampling 35 times Each of our 35 friends will repeat the same five steps: Start with 50 identically sized slips of paper representing the 50 pennies. Put the 50 small pieces of paper into a hat or beanie cap. Mix the hat’s contents and draw one slip of paper at random. Record the year in a spreadsheet. Replace the slip of paper back in the hat! Repeat Steps 3 and 4 a total of 49 more times, resulting in 50 recorded years. Since we had 35 of our friends perform this task, we ended up with \\(35 \\cdot 50 = 1750\\) values. We recorded these values in a shared spreadsheet with 50 rows (plus a header row) and 35 columns. We display a snapshot of the first 10 rows and five columns of this shared spreadsheet in Figure 15.10. FIGURE 15.10: Snapshot of shared spreadsheet of resampled pennies. For your convenience, we’ve taken these 35 \\(\\cdot\\) 50 = 1750 values and saved them in pennies_resamples, a “tidy” data frame included in the moderndive package. We saw what it means for a data frame to be “tidy” in Subsection ??. pennies_resamples # A tibble: 1,750 × 3 # Groups: name [35] replicate name year &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Arianna 1988 2 1 Arianna 2002 3 1 Arianna 2015 4 1 Arianna 1998 5 1 Arianna 1979 6 1 Arianna 1971 7 1 Arianna 1971 8 1 Arianna 2015 9 1 Arianna 1988 10 1 Arianna 1979 # ℹ 1,740 more rows What did each of our 35 friends obtain as the mean year? Once again, dplyr to the rescue! After grouping the rows by name, we summarize each group of 50 rows by their mean year: resampled_means &lt;- pennies_resamples %&gt;% group_by(name) %&gt;% summarize(mean_year = mean(year)) resampled_means # A tibble: 35 × 2 name mean_year &lt;chr&gt; &lt;dbl&gt; 1 Arianna 1992.5 2 Artemis 1996.42 3 Bea 1996.32 4 Camryn 1996.9 5 Cassandra 1991.22 6 Cindy 1995.48 7 Claire 1995.52 8 Dahlia 1998.48 9 Dan 1993.86 10 Eindra 1993.56 # ℹ 25 more rows Observe that resampled_means has 35 rows corresponding to the 35 means based on the 35 resamples. Furthermore, observe the variation in the 35 values in the variable mean_year. Let’s visualize this variation using a histogram in Figure 15.11. Recall that adding the argument boundary = 1990 to the geom_histogram() sets the binning structure so that one of the bin boundaries is at 1990 exactly. ggplot(resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;Sampled mean year&quot;) FIGURE 15.11: Distribution of 35 sample means from 35 resamples. Observe in Figure 15.11 that the distribution looks roughly normal and that we rarely observe sample mean years less than 1992 or greater than 2000. Also observe how the distribution is roughly centered at 1995, which is close to the sample mean of 1995.44 of the original sample of 50 pennies from the bank. 15.1.4 What did we just do? What we just demonstrated in this activity is the statistical procedure known as bootstrap resampling with replacement. We used resampling to mimic the sampling variation we studied in Chapter 14 on sampling. However, in this case, we did so using only a single sample from the population. In fact, the histogram of sample means from 35 resamples in Figure 15.11 is called the bootstrap distribution. It is an approximation to the sampling distribution of the sample mean, in the sense that both distributions will have a similar shape and similar spread. In fact in the upcoming Section 15.7, we’ll show you that this is the case. Using this bootstrap distribution, we can study the effect of sampling variation on our estimates. In particular, we’ll study the typical “error” of our estimates, known as the standard error. In Section 15.2 we’ll mimic our tactile resampling activity virtually on the computer, allowing us to quickly perform the resampling many more than 35 times. In Section 15.3 we’ll define the statistical concept of a confidence interval, which builds off the concept of bootstrap distributions. In Section 15.4, we’ll construct confidence intervals using the dplyr package, as well as a new package: the infer package for “tidy” and transparent statistical inference. We’ll introduce the “tidy” statistical inference framework that was the motivation for the infer package pipeline. The infer package will be the driving package throughout the rest of this book. As we did in Chapter 14, we’ll tie all these ideas together with a real-life case study in Section 15.6. This time we’ll look at data from an experiment about yawning from the US television show Mythbusters. 15.2 Computer simulation of resampling Let’s now mimic our tactile resampling activity virtually with a computer. 15.2.1 Virtually resampling once First, let’s perform the virtual analog of resampling once. Recall that the pennies_sample data frame included in the moderndive package contains the years of our original sample of 50 pennies from the bank. Furthermore, recall in Chapter 14 on sampling that we used the rep_sample_n() function as a virtual shovel to sample balls from our virtual bowl of 2400 balls as follows: virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) Let’s modify this code to perform the resampling with replacement of the 50 slips of paper representing our original sample 50 pennies: virtual_resample &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE) Observe how we explicitly set the replace argument to TRUE in order to tell rep_sample_n() that we would like to sample pennies with replacement. Had we not set replace = TRUE, the function would’ve assumed the default value of FALSE and hence done resampling without replacement. Additionally, since we didn’t specify the number of replicates via the reps argument, the function assumes the default of one replicate reps = 1. Lastly, observe also that the size argument is set to match the original sample size of 50 pennies. Let’s look at only the first 10 out of 50 rows of virtual_resample: virtual_resample # A tibble: 50 × 3 # Groups: replicate [1] replicate ID year &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 37 1962 2 1 1 2002 3 1 45 1997 4 1 28 2006 5 1 50 2017 6 1 10 2000 7 1 16 2015 8 1 47 1982 9 1 23 1998 10 1 44 2015 # ℹ 40 more rows The replicate variable only takes on the value of 1 corresponding to us only having reps = 1, the ID variable indicates which of the 50 pennies from pennies_sample was resampled, and year denotes the year of minting. Let’s now compute the mean year in our virtual resample of size 50 using data wrangling functions included in the dplyr package: virtual_resample %&gt;% summarize(resample_mean = mean(year)) # A tibble: 1 × 2 replicate resample_mean &lt;int&gt; &lt;dbl&gt; 1 1 1996 As we saw when we did our tactile resampling exercise, the resulting mean year is different than the mean year of our 50 originally sampled pennies of 1995.44. 15.2.2 Virtually resampling 35 times Let’s now perform the virtual analog of our 35 friends’ resampling. Using these results, we’ll be able to study the variability in the sample means from 35 resamples of size 50. Let’s first add a reps = 35 argument to rep_sample_n() to indicate we would like 35 replicates. Thus, we want to repeat the resampling with the replacement of 50 pennies 35 times. virtual_resamples &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 35) virtual_resamples # A tibble: 1,750 × 3 # Groups: replicate [35] replicate ID year &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 21 1981 2 1 34 1985 3 1 4 1988 4 1 11 1994 5 1 26 1979 6 1 8 1996 7 1 19 1983 8 1 21 1981 9 1 49 2006 10 1 2 1986 # ℹ 1,740 more rows The resulting virtual_resamples data frame has 35 \\(\\cdot\\) 50 = 1750 rows corresponding to 35 resamples of 50 pennies. Let’s now compute the resulting 35 sample means using the same dplyr code as we did in the previous section, but this time adding a group_by(replicate): virtual_resampled_means &lt;- virtual_resamples %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) virtual_resampled_means # A tibble: 35 × 2 replicate mean_year &lt;int&gt; &lt;dbl&gt; 1 1 1995.58 2 2 1999.74 3 3 1993.7 4 4 1997.1 5 5 1999.42 6 6 1995.12 7 7 1994.94 8 8 1997.78 9 9 1991.26 10 10 1996.88 # ℹ 25 more rows Observe that virtual_resampled_means has 35 rows, corresponding to the 35 resampled means. Furthermore, observe that the values of mean_year vary. Let’s visualize this variation using a histogram in Figure 15.12. ggplot(virtual_resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;Resample mean year&quot;) FIGURE 15.12: Distribution of 35 sample means from 35 resamples. Let’s compare our virtually constructed bootstrap distribution with the one our 35 friends constructed via our tactile resampling exercise in Figure 15.13. Observe how they are somewhat similar, but not identical. FIGURE 15.13: Comparing distributions of means from resamples. Recall that in the “resampling with replacement” scenario we are illustrating here, both of these histograms have a special name: the bootstrap distribution of the sample mean. Furthermore, recall they are an approximation to the sampling distribution of the sample mean, a concept you saw in Chapter 14 on sampling. These distributions allow us to study the effect of sampling variation on our estimates of the true population mean, in this case the true mean year for all US pennies. However, unlike in Chapter 14 where we took multiple samples (something one would never do in practice), bootstrap distributions are constructed by taking multiple resamples from a single sample: in this case, the 50 original pennies from the bank. 15.2.3 Virtually resampling 1000 times Remember that one of the goals of resampling with replacement is to construct the bootstrap distribution, which is an approximation of the sampling distribution. However, the bootstrap distribution in Figure 15.12 is based only on 35 resamples and hence looks a little coarse. Let’s increase the number of resamples to 1000, so that we can hopefully better see the shape and the variability between different resamples. # Repeat resampling 1000 times virtual_resamples &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) # Compute 1000 sample means virtual_resampled_means &lt;- virtual_resamples %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) However, in the interest of brevity, going forward let’s combine these two operations into a single chain of pipe (%&gt;%) operators: virtual_resampled_means &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) virtual_resampled_means # A tibble: 1,000 × 2 replicate mean_year &lt;int&gt; &lt;dbl&gt; 1 1 1992.6 2 2 1994.78 3 3 1994.74 4 4 1997.88 5 5 1990 6 6 1999.48 7 7 1990.26 8 8 1993.2 9 9 1994.88 10 10 1996.3 # ℹ 990 more rows In Figure 15.14 let’s visualize the bootstrap distribution of these 1000 means based on 1000 virtual resamples: ggplot(virtual_resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;sample mean&quot;) FIGURE 15.14: Bootstrap resampling distribution based on 1000 resamples. Note here that the bell shape is starting to become much more apparent. We now have a general sense for the range of values that the sample mean may take on. But where is this histogram centered? Let’s compute the mean of the 1000 resample means: virtual_resampled_means %&gt;% summarize(mean_of_means = mean(mean_year)) # A tibble: 1 × 1 mean_of_means &lt;dbl&gt; 1 1995.36 The mean of these 1000 means is 1995.36, which is quite close to the mean of our original sample of 50 pennies of 1995.44. This is the case since each of the 1000 resamples is based on the original sample of 50 pennies. Congratulations! You’ve just constructed your first bootstrap distribution! In the next section, you’ll see how to use this bootstrap distribution to construct confidence intervals. Learning check (LC8.1) What is the chief difference between a bootstrap distribution and a sampling distribution? (LC8.2) Looking at the bootstrap distribution for the sample mean in Figure 15.14, between what two values would you say most values lie? 15.3 Understanding confidence intervals Let’s start this section with an analogy involving fishing. Say you are trying to catch a fish. On the one hand, you could use a spear, while on the other you could use a net. Using the net will probably allow you to catch more fish! Now think back to our pennies exercise where you are trying to estimate the true population mean year \\(\\mu\\) of all US pennies. Think of the value of \\(\\mu\\) as a fish. On the one hand, we could use the appropriate point estimate/sample statistic to estimate \\(\\mu\\), which we saw in Table 15.1 is the sample mean \\(\\overline{x}\\). Based on our sample of 50 pennies from the bank, the sample mean was 1995.44. Think of using this value as “fishing with a spear.” What would “fishing with a net” correspond to? Look at the bootstrap distribution in Figure 15.14 once more. Between which two years would you say that “most” sample means lie? While this question is somewhat subjective, saying that most sample means lie between 1992 and 2000 would not be unreasonable. Think of this interval as the “net.” What we’ve just illustrated is the concept of a confidence interval, which we’ll abbreviate with “CI” throughout this book. As opposed to a point estimate/sample statistic that estimates the value of an unknown population parameter with a single value, a confidence interval gives what can be interpreted as a range of plausible values. Going back to our analogy, point estimates/sample statistics can be thought of as spears, whereas confidence intervals can be thought of as nets. FIGURE 15.15: Analogy of difference between point estimates and confidence intervals. Our proposed interval of 1992 to 2000 was constructed by eye and was thus somewhat subjective. We now introduce two methods for constructing such intervals in a more exact fashion: the percentile method and the standard error method. Both methods for confidence interval construction share some commonalities. First, they are both constructed from a bootstrap distribution, as you constructed in Subsection 15.2.3 and visualized in Figure 15.14. Second, they both require you to specify the confidence level. Commonly used confidence levels include 90%, 95%, and 99%. All other things being equal, higher confidence levels correspond to wider confidence intervals, and lower confidence levels correspond to narrower confidence intervals. In this book, we’ll be mostly using 95% and hence constructing “95% confidence intervals for \\(\\mu\\)” for our pennies activity. 15.3.1 Percentile method One method to construct a confidence interval is to use the middle 95% of values of the bootstrap distribution. We can do this by computing the 2.5th and 97.5th percentiles, which are 1991.059 and 1999.283, respectively. This is known as the percentile method for constructing confidence intervals. For now, let’s focus only on the concepts behind a percentile method constructed confidence interval; we’ll show you the code that computes these values in the next section. Let’s mark these percentiles on the bootstrap distribution with vertical lines in Figure 15.16. About 95% of the mean_year variable values in virtual_resampled_means fall between 1991.059 and 1999.283, with 2.5% to the left of the leftmost line and 2.5% to the right of the rightmost line. FIGURE 15.16: Percentile method 95% confidence interval. Interval endpoints marked by vertical lines. 15.3.2 Standard error method Recall in Appendix ??, we saw that if a numerical variable follows a normal distribution, or, in other words, the histogram of this variable is bell-shaped, then roughly 95% of values fall between \\(\\pm\\) 1.96 standard deviations of the mean. Given that our bootstrap distribution based on 1000 resamples with replacement in Figure 15.14 is normally shaped, let’s use this fact about normal distributions to construct a confidence interval in a different way. First, recall the bootstrap distribution has a mean equal to 1995.36. This value almost coincides exactly with the value of the sample mean \\(\\overline{x}\\) of our original 50 pennies of 1995.44. Second, let’s compute the standard deviation of the bootstrap distribution using the values of mean_year in the virtual_resampled_means data frame: virtual_resampled_means %&gt;% summarize(SE = sd(mean_year)) # A tibble: 1 × 1 SE &lt;dbl&gt; 1 2.15466 What is this value? Recall that the bootstrap distribution is an approximation to the sampling distribution. Recall also that the standard deviation of a sampling distribution has a special name: the standard error. Putting these two facts together, we can say that 2.155 is an approximation of the standard error of \\(\\overline{x}\\). Thus, using our 95% rule of thumb about normal distributions from Appendix ??, we can use the following formula to determine the lower and upper endpoints of a 95% confidence interval for \\(\\mu\\): \\[ \\begin{aligned} \\overline{x} \\pm 1.96 \\cdot SE &amp;= (\\overline{x} - 1.96 \\cdot SE, \\overline{x} + 1.96 \\cdot SE)\\\\ &amp;= (1995.44 - 1.96 \\cdot 2.15, 1995.44 + 1.96 \\cdot 2.15)\\\\ &amp;= (1991.15, 1999.73) \\end{aligned} \\] Let’s now add the SE method confidence interval with dashed lines in Figure 15.17. FIGURE 15.17: Comparing two 95% confidence interval methods. We see that both methods produce nearly identical 95% confidence intervals for \\(\\mu\\) with the percentile method yielding \\((1991.06, 1999.28)\\) while the standard error method produces \\((1991.22, 1999.66)\\). However, recall that we can only use the standard error rule when the bootstrap distribution is roughly normally shaped. Now that we’ve introduced the concept of confidence intervals and laid out the intuition behind two methods for constructing them, let’s explore the code that allows us to construct them. Learning check (LC8.3) What condition about the bootstrap distribution must be met for us to be able to construct confidence intervals using the standard error method? (LC8.4) Say we wanted to construct a 68% confidence interval instead of a 95% confidence interval for \\(\\mu\\). Describe what changes are needed to make this happen. Hint: we suggest you look at Appendix ?? on the normal distribution. 15.4 Constructing confidence intervals Recall that the process of resampling with replacement we performed by hand in Section 15.1 and virtually in Section 15.2 is known as bootstrapping. The term bootstrapping originates in the expression of “pulling oneself up by their bootstraps,” meaning to “succeed only by one’s own efforts or abilities.” From a statistical perspective, bootstrapping alludes to succeeding in being able to study the effects of sampling variation on estimates from the “effort” of a single sample. Or more precisely, it refers to constructing an approximation to the sampling distribution using only one sample. To perform this resampling with replacement virtually in Section 15.2, we used the rep_sample_n() function, making sure that the size of the resamples matched the original sample size of 50. In this section, we’ll build off these ideas to construct confidence intervals using a new package: the infer package for “tidy” and transparent statistical inference. 15.4.1 Original workflow Recall that in Section 15.2, we virtually performed bootstrap resampling with replacement to construct bootstrap distributions. Such distributions are approximations to the sampling distributions we saw in Chapter 14, but are constructed using only a single sample. Let’s revisit the original workflow using the %&gt;% pipe operator. First, we used the rep_sample_n() function to resample size = 50 pennies with replacement from the original sample of 50 pennies in pennies_sample by setting replace = TRUE. Furthermore, we repeated this resampling 1000 times by setting reps = 1000: pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) Second, since for each of our 1000 resamples of size 50, we wanted to compute a separate sample mean, we used the dplyr verb group_by() to group observations/rows together by the replicate variable… pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) … followed by using summarize() to compute the sample mean() year for each replicate group: pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) For this simple case, we can get by with using the rep_sample_n() function and a couple of dplyr verbs to construct the bootstrap distribution. However, using only dplyr verbs only provides us with a limited set of tools. For more complicated situations, we’ll need a little more firepower. Let’s repeat this using the infer package. 15.4.2 infer package workflow The infer package is an R package for statistical inference. It makes efficient use of the %&gt;% pipe operator we introduced in Section 13.1 to spell out the sequence of steps necessary to perform statistical inference in a “tidy” and transparent fashion. Furthermore, just as the dplyr package provides functions with verb-like names to perform data wrangling, the infer package provides functions with intuitive verb-like names to perform statistical inference. Let’s go back to our pennies. Previously, we computed the value of the sample mean \\(\\overline{x}\\) using the dplyr function summarize(): pennies_sample %&gt;% summarize(stat = mean(year)) We’ll see that we can also do this using infer functions specify() and calculate(): pennies_sample %&gt;% specify(response = year) %&gt;% calculate(stat = &quot;mean&quot;) You might be asking yourself: “Isn’t the infer code longer? Why would I use that code?”. While not immediately apparent, you’ll see that there are three chief benefits to the infer workflow as opposed to the dplyr workflow. First, the infer verb names better align with the overall resampling framework you need to understand to construct confidence intervals and to conduct hypothesis tests (in Chapter 16). We’ll see flowchart diagrams of this framework in the upcoming Figure 15.23 and in Chapter 16 with Figure 16.14. Second, you can jump back and forth seamlessly between confidence intervals and hypothesis testing with minimal changes to your code. This will become apparent in Subsection 16.3.2 when we’ll compare the infer code for both of these inferential methods. Third, the infer workflow is much simpler for conducting inference when you have more than one variable. We’ll see two such situations. We’ll first see situations of two-sample inference where the sample data is collected from two groups, such as in Section 15.6 where we study the contagiousness of yawning and in Section 16.1 where we compare promotion rates of two groups at banks in the 1970s. Then in Section 18.4, we’ll see situations of inference for regression using the regression models you fit in Chapter 17. Let’s now illustrate the sequence of verbs necessary to construct a confidence interval for \\(\\mu\\), the population mean year of minting of all US pennies in 2019. 1. specify variables FIGURE 15.18: Diagram of the specify() verb. As shown in Figure 15.18, the specify() function is used to choose which variables in a data frame will be the focus of our statistical inference. We do this by specifying the response argument. For example, in our pennies_sample data frame of the 50 pennies sampled from the bank, the variable of interest is year: pennies_sample %&gt;% specify(response = year) Response: year (numeric) # A tibble: 50 × 1 year &lt;dbl&gt; 1 2002 2 1986 3 2017 4 1988 5 2008 6 1983 7 2008 8 1996 9 2004 10 2000 # ℹ 40 more rows Notice how the data itself doesn’t change, but the Response: year (numeric) meta-data does. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Section 13.4. We can also specify which variables will be the focus of our statistical inference using a formula = y ~ x. This is the same formula notation you saw in Chapters 17 and 19 on regression models: the response variable y is separated from the explanatory variable x by a ~ (“tilde”). The following use of specify() with the formula argument yields the same result seen previously: pennies_sample %&gt;% specify(formula = year ~ NULL) Since in the case of pennies we only have a response variable and no explanatory variable of interest, we set the x on the right-hand side of the ~ to be NULL. While in the case of the pennies either specification works just fine, we’ll see examples later on where the formula specification is simpler. In particular, this comes up in the upcoming Section 15.6 on comparing two proportions and Section 18.4 on inference for regression. 2. generate replicates FIGURE 15.19: Diagram of generate() replicates. After we specify() the variables of interest, we pipe the results into the generate() function to generate replicates. Figure 15.19 shows how this is combined with specify() to start the pipeline. In other words, repeat the resampling process a large number of times. Recall in Sections 15.2.2 and 15.2.3 we did this 35 and 1000 times. The generate() function’s first argument is reps, which sets the number of replicates we would like to generate. Since we want to resample the 50 pennies in pennies_sample with replacement 1000 times, we set reps = 1000. The second argument type determines the type of computer simulation we’d like to perform. We set this to type = \"bootstrap\" indicating that we want to perform bootstrap resampling. You’ll see different options for type in Chapter 16. pennies_sample %&gt;% specify(response = year) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: year (numeric) # A tibble: 50,000 × 2 # Groups: replicate [1,000] replicate year &lt;int&gt; &lt;dbl&gt; 1 1 1981 2 1 1988 3 1 2006 4 1 2016 5 1 2002 6 1 1985 7 1 1979 8 1 2000 9 1 2006 10 1 2016 # ℹ 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 pennies with replacement 1000 times and 50,000 = 50 \\(\\cdot\\) 1000. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. The default value of the type argument is \"bootstrap\" in this scenario, so if the last line was written as generate(reps = 1000), we’d obtain the same results. Comparing with original workflow: Note that the steps of the infer workflow so far produce the same results as the original workflow using the rep_sample_n() function we saw earlier. In other words, the following two code chunks produce similar results: # infer workflow: # Original workflow: pennies_sample %&gt;% pennies_sample %&gt;% specify(response = year) %&gt;% rep_sample_n(size = 50, replace = TRUE, generate(reps = 1000) reps = 1000) 3. calculate summary statistics FIGURE 15.20: Diagram of calculate() summary statistics. After we generate() many replicates of bootstrap resampling with replacement, we next want to summarize each of the 1000 resamples of size 50 to a single sample statistic value. As seen in the diagram, the calculate() function does this. In our case, we want to calculate the mean year for each bootstrap resample of size 50. To do so, we set the stat argument to \"mean\". You can also set the stat argument to a variety of other common summary statistics, like \"median\", \"sum\", \"sd\" (standard deviation), and \"prop\" (proportion). To see a list of all possible summary statistics you can use, type ?calculate and read the help file. Let’s save the result in a data frame called bootstrap_distribution and explore its contents: bootstrap_distribution &lt;- pennies_sample %&gt;% specify(response = year) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = &quot;mean&quot;) bootstrap_distribution # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 1995.7 2 2 1994.04 3 3 1993.62 4 4 1994.5 5 5 1994.08 6 6 1993.6 7 7 1995.26 8 8 1996.64 9 9 1994.3 10 10 1995.94 # ℹ 990 more rows Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 replicate values. It also has the mean year for each bootstrap resample saved in the variable stat. Comparing with original workflow: You may have recognized at this point that the calculate() step in the infer workflow produces the same output as the group_by() %&gt;% summarize() steps in the original workflow. # infer workflow: # Original workflow: pennies_sample %&gt;% pennies_sample %&gt;% specify(response = year) %&gt;% rep_sample_n(size = 50, replace = TRUE, generate(reps = 1000) %&gt;% reps = 1000) %&gt;% calculate(stat = &quot;mean&quot;) group_by(replicate) %&gt;% summarize(stat = mean(year)) 4. visualize the results FIGURE 15.21: Diagram of visualize() results. The visualize() verb provides a quick way to visualize the bootstrap distribution as a histogram of the numerical stat variable’s values. The pipeline of the main infer verbs used for exploring bootstrap distribution results is shown in Figure 15.21. visualize(bootstrap_distribution) FIGURE 15.22: Bootstrap distribution. Comparing with original workflow: In fact, visualize() is a wrapper function for the ggplot() function that uses a geom_histogram() layer. Recall that we illustrated the concept of a wrapper function in Figure 17.5 in Subsection 17.1.2. # infer workflow: # Original workflow: visualize(bootstrap_distribution) ggplot(bootstrap_distribution, aes(x = stat)) + geom_histogram() The visualize() function can take many other arguments which we’ll see momentarily to customize the plot further. It also works with helper functions to do the shading of the histogram values corresponding to the confidence interval values. Let’s recap the steps of the infer workflow for constructing a bootstrap distribution and then visualizing it in Figure 15.23. FIGURE 15.23: infer package workflow for confidence intervals. Recall how we introduced two different methods for constructing 95% confidence intervals for an unknown population parameter in Section 15.3: the percentile method and the standard error method. Let’s now check out the infer package code that explicitly constructs these. There are also some additional neat functions to visualize the resulting confidence intervals built-in to the infer package! 15.4.3 Percentile method with infer Recall the percentile method for constructing 95% confidence intervals we introduced in Subsection 15.3.1. This method sets the lower endpoint of the confidence interval at the 2.5th percentile of the bootstrap distribution and similarly sets the upper endpoint at the 97.5th percentile. The resulting interval captures the middle 95% of the values of the sample mean in the bootstrap distribution. We can compute the 95% confidence interval by piping bootstrap_distribution into the get_confidence_interval() function from the infer package, with the confidence level set to 0.95 and the confidence interval type to be \"percentile\". Let’s save the results in percentile_ci. percentile_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 1991.24 1999.42 Alternatively, we can visualize the interval (1991.24, 1999.42) by piping the bootstrap_distribution data frame into the visualize() function and adding a shade_confidence_interval() layer. We set the endpoints argument to be percentile_ci. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = percentile_ci) FIGURE 15.24: Percentile method 95% confidence interval shaded corresponding to potential values. Observe in Figure 15.24 that 95% of the sample means stored in the stat variable in bootstrap_distribution fall between the two endpoints marked with the darker lines, with 2.5% of the sample means to the left of the shaded area and 2.5% of the sample means to the right. You also have the option to change the colors of the shading using the color and fill arguments. You can also use the shorter named function shade_ci() and the results will be the same. This is for folks who don’t want to type out all of confidence_interval and prefer to type out ci instead. Try out the following code! visualize(bootstrap_distribution) + shade_ci(endpoints = percentile_ci, color = &quot;hotpink&quot;, fill = &quot;khaki&quot;) 15.4.4 Standard error method with infer Recall the standard error method for constructing 95% confidence intervals we introduced in Subsection 15.3.2. For any distribution that is normally shaped, roughly 95% of the values lie within two standard deviations of the mean. In the case of the bootstrap distribution, the standard deviation has a special name: the standard error. So in our case, 95% of values of the bootstrap distribution will lie within \\(\\pm 1.96\\) standard errors of \\(\\overline{x}\\). Thus, a 95% confidence interval is \\[\\overline{x} \\pm 1.96 \\cdot SE = (\\overline{x} - 1.96 \\cdot SE, \\, \\overline{x} + 1.96 \\cdot SE).\\] Computation of the 95% confidence interval can once again be done by piping the bootstrap_distribution data frame we created into the get_confidence_interval() function. However, this time we set the first type argument to be \"se\". Second, we must specify the point_estimate argument in order to set the center of the confidence interval. We set this to be the sample mean of the original sample of 50 pennies of 1995.44 we saved in x_bar earlier. standard_error_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(type = &quot;se&quot;, point_estimate = x_bar) Using `level = 0.95` to compute confidence interval. standard_error_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 1991.35 1999.53 If we would like to visualize the interval (1991.35, 1999.53), we can once again pipe the bootstrap_distribution data frame into the visualize() function and add a shade_confidence_interval() layer to our plot. We set the endpoints argument to be standard_error_ci. The resulting standard-error method based on a 95% confidence interval for \\(\\mu\\) can be seen in Figure 15.25. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = standard_error_ci) FIGURE 15.25: Standard-error-method 95% confidence interval. As noted in Section 15.3, both methods produce similar confidence intervals: Percentile method: (1991.24, 1999.42) Standard error method: (1991.35, 1999.53) Learning check (LC8.5) Construct a 95% confidence interval for the median year of minting of all US pennies. Use the percentile method and, if appropriate, then use the standard-error method. 15.5 Interpreting confidence intervals Now that we’ve shown you how to construct confidence intervals using a sample drawn from a population, let’s now focus on how to interpret their effectiveness. The effectiveness of a confidence interval is judged by whether or not it contains the true value of the population parameter. Going back to our fishing analogy in Section 15.3, this is like asking, “Did our net capture the fish?”. So, for example, does our percentile-based confidence interval of (1991.24, 1999.42) “capture” the true mean year \\(\\mu\\) of all US pennies? Alas, we’ll never know, because we don’t know what the true value of \\(\\mu\\) is. After all, we’re sampling to estimate it! In order to interpret a confidence interval’s effectiveness, we need to know what the value of the population parameter is. That way we can say whether or not a confidence interval “captured” this value. Let’s revisit our sampling bowl from Chapter 14. What proportion of the bowl’s 2400 balls are red? Let’s compute this: bowl %&gt;% summarize(p_red = mean(color == &quot;red&quot;)) # A tibble: 1 × 1 p_red &lt;dbl&gt; 1 0.375 In this case, we know what the value of the population parameter is: we know that the population proportion \\(p\\) is 0.375. In other words, we know that 37.5% of the bowl’s balls are red. As we stated in Subsection 14.3.3, the sampling bowl exercise doesn’t really reflect how sampling is done in real life, but rather was an idealized activity. In real life, we won’t know what the true value of the population parameter is, hence the need for estimation. Let’s now construct confidence intervals for \\(p\\) using our 33 groups of friends’ samples from the bowl in Chapter 14. We’ll then see if the confidence intervals “captured” the true value of \\(p\\), which we know to be 37.5%. That is to say, “Did the net capture the fish?”. 15.5.1 Did the net capture the fish? Recall that we had 33 groups of friends each take samples of size 50 from the bowl and then compute the sample proportion of red balls \\(\\widehat{p}\\). This resulted in 33 such estimates of \\(p\\). Let’s focus on Ilyas and Yohan’s sample, which is saved in the bowl_sample_1 data frame in the moderndive package: bowl_sample_1 # A tibble: 50 × 1 color &lt;chr&gt; 1 white 2 white 3 red 4 red 5 white 6 white 7 red 8 white 9 white 10 white # ℹ 40 more rows They observed 21 red balls out of 50 and thus their sample proportion \\(\\widehat{p}\\) was 21/50 = 0.42 = 42%. Think of this as the “spear” from our fishing analogy. Let’s now follow the infer package workflow from Subsection 15.4.2 to create a percentile-method-based 95% confidence interval for \\(p\\) using Ilyas and Yohan’s sample. Think of this as the “net.” 1. specify variables First, we specify() the response variable of interest color: bowl_sample_1 %&gt;% specify(response = color) Error: A level of the response variable `color` needs to be specified for the `success` argument in `specify()`. Whoops! We need to define which event is of interest! red or white balls? Since we are interested in the proportion red, let’s set success to be \"red\": bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) Response: color (factor) # A tibble: 50 × 1 color &lt;fct&gt; 1 white 2 white 3 red 4 red 5 white 6 white 7 red 8 white 9 white 10 white # ℹ 40 more rows 2. generate replicates Second, we generate() 1000 replicates of bootstrap resampling with replacement from bowl_sample_1 by setting reps = 1000 and type = \"bootstrap\". bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: color (factor) # A tibble: 50,000 × 2 # Groups: replicate [1,000] replicate color &lt;int&gt; &lt;fct&gt; 1 1 white 2 1 white 3 1 white 4 1 white 5 1 red 6 1 white 7 1 white 8 1 white 9 1 white 10 1 red # ℹ 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 balls with replacement 1000 times and thus 50,000 = 50 \\(\\cdot\\) 1000. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. 3. calculate summary statistics Third, we summarize each of the 1000 resamples of size 50 with the proportion of successes. In other words, the proportion of the balls that are \"red\". We can set the summary statistic to be calculated as the proportion by setting the stat argument to be \"prop\". Let’s save the result as sample_1_bootstrap: sample_1_bootstrap &lt;- bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) sample_1_bootstrap # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.32 2 2 0.42 3 3 0.44 4 4 0.4 5 5 0.44 6 6 0.52 7 7 0.38 8 8 0.44 9 9 0.34 10 10 0.42 # ℹ 990 more rows Observe there are 1000 rows in this data frame and thus 1000 values of the variable stat. These 1000 values of stat represent our 1000 replicated values of the proportion, each based on a different resample. 4. visualize the results Fourth and lastly, let’s compute the resulting 95% confidence interval. percentile_ci_1 &lt;- sample_1_bootstrap %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci_1 # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.3 0.56 Let’s visualize the bootstrap distribution along with the percentile_ci_1 percentile-based 95% confidence interval for \\(p\\) in Figure 15.26. We’ll adjust the number of bins to better see the resulting shape. Furthermore, we’ll add a dashed vertical line at Ilyas and Yohan’s observed \\(\\widehat{p}\\) = 21/50 = 0.42 = 42% using geom_vline(). sample_1_bootstrap %&gt;% visualize(bins = 15) + shade_confidence_interval(endpoints = percentile_ci_1) + geom_vline(xintercept = 0.42, linetype = &quot;dashed&quot;) FIGURE 15.26: Bootstrap distribution. Did Ilyas and Yohan’s net capture the fish? Did their 95% confidence interval for \\(p\\) based on their sample contain the true value of \\(p\\) of 0.375? Yes! 0.375 is between the endpoints of their confidence interval (0.3, 0.56). However, will every 95% confidence interval for \\(p\\) capture this value? In other words, if we had a different sample of 50 balls and constructed a different confidence interval, would it necessarily contain \\(p\\) = 0.375 as well? Let’s see! Let’s first take a different sample from the bowl, this time using the computer as we did in Chapter 14: bowl_sample_2 &lt;- bowl %&gt;% rep_sample_n(size = 50) bowl_sample_2 # A tibble: 50 × 3 # Groups: replicate [1] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1665 red 2 1 1312 red 3 1 2105 red 4 1 810 white 5 1 189 white 6 1 1429 white 7 1 2294 red 8 1 1233 white 9 1 1951 white 10 1 2061 white # ℹ 40 more rows Let’s reapply the same infer functions on bowl_sample_2 to generate a different 95% confidence interval for \\(p\\). First, we create the new bootstrap distribution and save the results in sample_2_bootstrap: sample_2_bootstrap &lt;- bowl_sample_2 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) sample_2_bootstrap # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.48 2 2 0.38 3 3 0.32 4 4 0.32 5 5 0.34 6 6 0.26 7 7 0.3 8 8 0.36 9 9 0.44 10 10 0.36 # ℹ 990 more rows We once again compute a percentile-based 95% confidence interval for \\(p\\): percentile_ci_2 &lt;- sample_2_bootstrap %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci_2 # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.2 0.48 Does this new net capture the fish? In other words, does the 95% confidence interval for \\(p\\) based on the new sample contain the true value of \\(p\\) of 0.375? Yes again! 0.375 is between the endpoints of our confidence interval (0.2, 0.48). Let’s now repeat this process 100 more times: we take 100 virtual samples from the bowl and construct 100 95% confidence intervals. Let’s visualize the results in Figure 15.27 where: We mark the true value of \\(p = 0.375\\) with a vertical line. We mark each of the 100 95% confidence intervals with horizontal lines. These are the “nets.” The horizontal line is colored grey if the confidence interval “captures” the true value of \\(p\\) marked with the vertical line. The horizontal line is colored black otherwise. FIGURE 15.27: 100 percentile-based 95% confidence intervals for \\(p\\). Of the 100 95% confidence intervals, 95 of them captured the true value \\(p = 0.375\\), whereas 5 of them didn’t. In other words, 95 of our nets caught the fish, whereas 5 of our nets didn’t. This is where the “95% confidence level” we defined in Section 15.3 comes into play: for every 100 95% confidence intervals, we expect that 95 of them will capture \\(p\\) and that five of them won’t. Note that “expect” is a probabilistic statement referring to a long-run average. In other words, for every 100 confidence intervals, we will observe about 95 confidence intervals that capture \\(p\\), but not necessarily exactly 95. In Figure 15.27 for example, 95 of the confidence intervals capture \\(p\\). To further accentuate our point about confidence levels, let’s generate a figure similar to Figure 15.27, but this time constructing 80% standard-error method based confidence intervals instead. Let’s visualize the results in Figure 15.28 with the scale on the x-axis being the same as in Figure 15.27 to make comparison easy. Furthermore, since all standard-error method confidence intervals for \\(p\\) are centered at their respective point estimates \\(\\widehat{p}\\), we mark this value on each line with dots. FIGURE 15.28: 100 SE-based 80% confidence intervals for \\(p\\) with point estimate center marked with dots. Observe how the 80% confidence intervals are narrower than the 95% confidence intervals, reflecting our lower degree of confidence. Think of this as using a smaller “net.” We’ll explore other determinants of confidence interval width in the upcoming Subsection 15.5.3. Furthermore, observe that of the 100 80% confidence intervals, 82 of them captured the population proportion \\(p\\) = 0.375, whereas 18 of them did not. Since we lowered the confidence level from 95% to 80%, we now have a much larger number of confidence intervals that failed to “catch the fish.” 15.5.2 Precise and shorthand interpretation Let’s return our attention to 95% confidence intervals. The precise and mathematically correct interpretation of a 95% confidence interval is a little long-winded: Precise interpretation: If we repeated our sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population parameter. This is what we observed in Figure 15.27. Our confidence interval construction procedure is 95% reliable. That is to say, we can expect our confidence intervals to include the true population parameter about 95% of the time. A common but incorrect interpretation is: “There is a 95% probability that the confidence interval contains \\(p\\).” Looking at Figure 15.27, each of the confidence intervals either does or doesn’t contain \\(p\\). In other words, the probability is either a 1 or a 0. So if the 95% confidence level only relates to the reliability of the confidence interval construction procedure and not to a given confidence interval itself, what insight can be derived from a given confidence interval? For example, going back to the pennies example, we found that the percentile method 95% confidence interval for \\(\\mu\\) was (1991.24, 1999.42), whereas the standard error method 95% confidence interval was (1991.35, 1999.53). What can be said about these two intervals? Loosely speaking, we can think of these intervals as our “best guess” of a plausible range of values for the mean year \\(\\mu\\) of all US pennies. For the rest of this book, we’ll use the following shorthand summary of the precise interpretation. Short-hand interpretation: We are 95% “confident” that a 95% confidence interval captures the value of the population parameter. We use quotation marks around “confident” to emphasize that while 95% relates to the reliability of our confidence interval construction procedure, ultimately a constructed confidence interval is our best guess of an interval that contains the population parameter. In other words, it’s our best net. So returning to our pennies example and focusing on the percentile method, we are 95% “confident” that the true mean year of pennies in circulation in 2019 is somewhere between 1991.24 and 1999.42. 15.5.3 Width of confidence intervals Now that we know how to interpret confidence intervals, let’s go over some factors that determine their width. Impact of confidence level One factor that determines confidence interval widths is the pre-specified confidence level. For example, in Figures 15.27 and 15.28, we compared the widths of 95% and 80% confidence intervals and observed that the 95% confidence intervals were wider. The quantification of the confidence level should match what many expect of the word “confident.” In order to be more confident in our best guess of a range of values, we need to widen the range of values. To elaborate on this, imagine we want to guess the forecasted high temperature in Seoul, South Korea on August 15th. Given Seoul’s temperate climate with four distinct seasons, we could say somewhat confidently that the high temperature would be between 50°F - 95°F (10°C - 35°C). However, if we wanted a temperature range we were absolutely confident about, we would need to widen it. We need this wider range to allow for the possibility of anomalous weather, like a freak cold spell or an extreme heat wave. So a range of temperatures we could be near certain about would be between 32°F - 110°F (0°C - 43°C). On the other hand, if we could tolerate being a little less confident, we could narrow this range to between 70°F - 85°F (21°C - 30°C). Let’s revisit our sampling bowl from Chapter 14. Let’s compare \\(10 \\cdot 3 = 30\\) confidence intervals for \\(p\\) based on three different confidence levels: 80%, 95%, and 99%. Specifically, we’ll first take 30 different random samples of size \\(n\\) = 50 balls from the bowl. Then we’ll construct 10 percentile-based confidence intervals using each of the three different confidence levels. Finally, we’ll compare the widths of these intervals. We visualize the resulting confidence intervals in Figure 15.29 along with a vertical line marking the true value of \\(p\\) = 0.375. FIGURE 15.29: Ten 80, 95, and 99% confidence intervals for \\(p\\) based on \\(n = 50\\). Observe that as the confidence level increases from 80% to 95% to 99%, the confidence intervals tend to get wider as seen in Table 15.2 where we compare their average widths. TABLE 15.2: Average width of 80, 95, and 99% confidence intervals Confidence level Mean width 80% 0.162 95% 0.262 99% 0.338 So in order to have a higher confidence level, our confidence intervals must be wider. Ideally, we would have both a high confidence level and narrow confidence intervals. However, we cannot have it both ways. If we want to be more confident, we need to allow for wider intervals. Conversely, if we would like a narrow interval, we must tolerate a lower confidence level. The moral of the story is: Higher confidence levels tend to produce wider confidence intervals. When looking at Figure 15.29 it is important to keep in mind that we kept the sample size fixed at \\(n\\) = 50. Thus, all \\(10 \\cdot 3 = 30\\) random samples from the bowl had the same sample size. What happens if instead we took samples of different sizes? Recall that we did this in Subsection 14.2 using virtual shovels with 25, 50, and 100 slots. Impact of sample size This time, let’s fix the confidence level at 95%, but consider three different sample sizes for \\(n\\): 25, 50, and 100. Specifically, we’ll first take 10 different random samples of size 25, 10 different random samples of size 50, and 10 different random samples of size 100. We’ll then construct 95% percentile-based confidence intervals for each sample. Finally, we’ll compare the widths of these intervals. We visualize the resulting 30 confidence intervals in Figure 15.30. Note also the vertical line marking the true value of \\(p\\) = 0.375. FIGURE 15.30: Ten 95% confidence intervals for \\(p\\) with \\(n = 25, 50,\\) and \\(100\\). Observe that as the confidence intervals are constructed from larger and larger sample sizes, they tend to get narrower. Let’s compare the average widths in Table 15.3. TABLE 15.3: Average width of 95% confidence intervals based on \\(n = 25\\), \\(50\\), and \\(100\\) Sample size Mean width n = 25 0.380 n = 50 0.268 n = 100 0.189 The moral of the story is: Larger sample sizes tend to produce narrower confidence intervals. Recall that this was a key message in Subsection 14.3.3. As we used larger and larger shovels for our samples, the sample proportions red \\(\\widehat{p}\\) tended to vary less. In other words, our estimates got more and more precise. Recall that we visualized these results in Figure 14.15, where we compared the sampling distributions for \\(\\widehat{p}\\) based on samples of size \\(n\\) equal 25, 50, and 100. We also quantified the sampling variation of these sampling distributions using their standard deviation, which has that special name: the standard error. So as the sample size increases, the standard error decreases. In fact, the standard error is another related factor in determining confidence interval width. We’ll explore this fact in Subsection 15.7.2 when we discuss theory-based methods for constructing confidence intervals using mathematical formulas. Such methods are an alternative to the computer-based methods we’ve been using so far. 15.6 Case study: Is yawning contagious? Let’s apply our knowledge of confidence intervals to answer the question: “Is yawning contagious?”. If you see someone else yawn, are you more likely to yawn? In an episode of the US show Mythbusters, the hosts conducted an experiment to answer this question. The episode is available to view in the United States on the Discovery Network website here and more information about the episode is also available on IMDb. 15.6.1 Mythbusters study data Fifty adult participants who thought they were being considered for an appearance on the show were interviewed by a show recruiter. In the interview, the recruiter either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the Mythbusters team watched the participants using a hidden camera to see if they yawned. The data frame containing the results of their experiment is available in the mythbusters_yawn data frame included in the moderndive package: mythbusters_yawn # A tibble: 50 × 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 2 control yes 3 3 seed no 4 4 seed yes 5 5 seed no 6 6 control no 7 7 seed yes 8 8 control no 9 9 control no 10 10 seed no # ℹ 40 more rows The variables are: subj: The participant ID with values 1 through 50. group: A binary treatment variable indicating whether the participant was exposed to yawning. \"seed\" indicates the participant was exposed to yawning while \"control\" indicates the participant was not. yawn: A binary response variable indicating whether the participant ultimately yawned. Recall that you learned about treatment and response variables in Subsection 17.3.1 in our discussion on confounding variables. Let’s use some data wrangling to obtain counts of the four possible outcomes: mythbusters_yawn %&gt;% group_by(group, yawn) %&gt;% summarize(count = n()) # A tibble: 4 × 3 # Groups: group [2] group yawn count &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 control no 12 2 control yes 4 3 seed no 24 4 seed yes 10 Let’s first focus on the \"control\" group participants who were not exposed to yawning. 12 such participants did not yawn, while 4 such participants did. So out of the 16 people who were not exposed to yawning, 4/16 = 0.25 = 25% did yawn. Let’s now focus on the \"seed\" group participants who were exposed to yawning where 24 such participants did not yawn, while 10 such participants did yawn. So out of the 34 people who were exposed to yawning, 10/34 = 0.294 = 29.4% did yawn. Comparing these two percentages, the participants who were exposed to yawning yawned 29.4% - 25% = 4.4% more often than those who were not. 15.6.2 Sampling scenario Let’s review the terminology and notation related to sampling we studied in Subsection 14.3.1. In Chapter 14 our study population was the bowl of \\(N\\) = 2400 balls. Our population parameter of interest was the population proportion of these balls that were red, denoted mathematically by \\(p\\). In order to estimate \\(p\\), we extracted a sample of 50 balls using the shovel and computed the relevant point estimate: the sample proportion that were red, denoted mathematically by \\(\\widehat{p}\\). Who is the study population here? All humans? All the people who watch the show Mythbusters? It’s hard to say! This question can only be answered if we know how the show’s hosts recruited participants! In other words, what was the sampling methodology used by the Mythbusters to recruit participants? We alas are not provided with this information. Only for the purposes of this case study, however, we’ll assume that the 50 participants are a representative sample of all Americans given the popularity of this show. Thus, we’ll be assuming that any results of this experiment will generalize to all \\(N\\) = 327 million Americans (2018 population). Just like with our sampling bowl, the population parameter here will involve proportions. However, in this case it will be the difference in population proportions \\(p_{seed} - p_{control}\\), where \\(p_{seed}\\) is the proportion of all Americans who if exposed to yawning will yawn themselves, and \\(p_{control}\\) is the proportion of all Americans who if not exposed to yawning still yawn themselves. Correspondingly, the point estimate/sample statistic based the Mythbusters’ sample of participants will be the difference in sample proportions \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\). Let’s extend Table 14.5 of scenarios of sampling for inference to include our latest scenario. TABLE 15.4: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) This is known as a two-sample inference situation since we have two separate samples. Based on their two-samples of size \\(n_{seed}\\) = 34 and \\(n_{control}\\) = 16, the point estimate is \\[ \\widehat{p}_{seed} - \\widehat{p}_{control} = \\frac{24}{34} - \\frac{12}{16} = 0.04411765 \\approx 4.4\\% \\] However, say the Mythbusters repeated this experiment. In other words, say they recruited 50 new participants and exposed 34 of them to yawning and 16 not. Would they obtain the exact same estimated difference of 4.4%? Probably not, again, because of sampling variation. How does this sampling variation affect their estimate of 4.4%? In other words, what would be a plausible range of values for this difference that accounts for this sampling variation? We can answer this question with confidence intervals! Furthermore, since the Mythbusters only have a single two-sample of 50 participants, they would have to construct a 95% confidence interval for \\(p_{seed} - p_{control}\\) using bootstrap resampling with replacement. We make a couple of important notes. First, for the comparison between the \"seed\" and \"control\" groups to make sense, however, both groups need to be independent from each other. Otherwise, they could influence each other’s results. This means that a participant being selected for the \"seed\" or \"control\" group has no influence on another participant being assigned to one of the two groups. As an example, if there were a mother and her child as participants in the study, they wouldn’t necessarily be in the same group. They would each be assigned randomly to one of the two groups of the explanatory variable. Second, the order of the subtraction in the difference doesn’t matter so long as you are consistent and tailor your interpretations accordingly. In other words, using a point estimate of \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) or \\(\\widehat{p}_{control} - \\widehat{p}_{seed}\\) does not make a material difference, you just need to stay consistent and interpret your results accordingly. 15.6.3 Constructing the confidence interval As we did in Subsection 15.4.2, let’s first construct the bootstrap distribution for \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) and then use this to construct 95% confidence intervals for \\(p_{seed} - p_{control}\\). We’ll do this using the infer workflow again. However, since the difference in proportions is a new scenario for inference, we’ll need to use some new arguments in the infer functions along the way. 1. specify variables Let’s take our mythbusters_yawn data frame and specify() which variables are of interest using the y ~ x formula interface where: Our response variable is yawn: whether or not a participant yawned. It has levels \"yes\" and \"no\". The explanatory variable is group: whether or not a participant was exposed to yawning. It has levels \"seed\" (exposed to yawning) and \"control\" (not exposed to yawning). mythbusters_yawn %&gt;% specify(formula = yawn ~ group) Error: A level of the response variable `yawn` needs to be specified for the `success` argument in `specify()`. Alas, we got an error message similar to the one from Subsection 15.5.1: infer is telling us that one of the levels of the categorical variable yawn needs to be defined as the success. Recall that we define success to be the event of interest we are trying to count and compute proportions of. Are we interested in those participants who \"yes\" yawned or those who \"no\" didn’t yawn? This isn’t clear to R or someone just picking up the code and results for the first time, so we need to set the success argument to \"yes\" as follows to improve the transparency of the code: mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) Response: yawn (factor) Explanatory: group (factor) # A tibble: 50 × 2 yawn group &lt;fct&gt; &lt;fct&gt; 1 yes seed 2 yes control 3 no seed 4 yes seed 5 no seed 6 no control 7 yes seed 8 no control 9 no control 10 no seed # ℹ 40 more rows 2. generate replicates Our next step is to perform bootstrap resampling with replacement like we did with the slips of paper in our pennies activity in Section 15.1. We saw how it works with both a single variable in computing bootstrap means in Section 15.4 and in computing bootstrap proportions in Section 15.5, but we haven’t yet worked with bootstrapping involving multiple variables. In the infer package, bootstrapping with multiple variables means that each row is potentially resampled. Let’s investigate this by focusing only on the first six rows of mythbusters_yawn: first_six_rows &lt;- head(mythbusters_yawn) first_six_rows # A tibble: 6 × 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 2 control yes 3 3 seed no 4 4 seed yes 5 5 seed no 6 6 control no When we bootstrap this data, we are potentially pulling the subject’s readings multiple times. Thus, we could see the entries of \"seed\" for group and \"no\" for yawn together in a new row in a bootstrap sample. This is further seen by exploring the sample_n() function in dplyr on this smaller 6-row data frame comprised of head(mythbusters_yawn). The sample_n() function can perform this bootstrapping procedure and is similar to the rep_sample_n() function in infer, except that it is not repeated, but rather only performs one sample with or without replacement. first_six_rows %&gt;% sample_n(size = 6, replace = TRUE) # A tibble: 6 × 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 6 control no 3 1 seed yes 4 5 seed no 5 4 seed yes 6 4 seed yes We can see that in this bootstrap sample generated from the first six rows of mythbusters_yawn, we have some rows repeated. The same is true when we perform the generate() step in infer as done in what follows. Using this fact, we generate 1000 replicates, or, in other words, we bootstrap resample the 50 participants with replacement 1000 times. mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: yawn (factor) Explanatory: group (factor) # A tibble: 50,000 × 3 # Groups: replicate [1,000] replicate yawn group &lt;int&gt; &lt;fct&gt; &lt;fct&gt; 1 1 yes seed 2 1 yes control 3 1 no control 4 1 no control 5 1 yes seed 6 1 yes seed 7 1 yes seed 8 1 yes seed 9 1 no seed 10 1 yes seed # ℹ 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 participants with replacement 1000 times and 50,000 = 1000 \\(\\cdot\\) 50. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. 3. calculate summary statistics After we generate() many replicates of bootstrap resampling with replacement, we next want to summarize the bootstrap resamples of size 50 with a single summary statistic, the difference in proportions. We do this by setting the stat argument to \"diff in props\": mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;) Error: Statistic is based on a difference; specify the `order` in which to subtract the levels of the explanatory variable. We see another error here. We need to specify the order of the subtraction. Is it \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) or \\(\\widehat{p}_{control} - \\widehat{p}_{seed}\\). We specify it to be \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) by setting order = c(\"seed\", \"control\"). Note that you could’ve also set order = c(\"control\", \"seed\"). As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly. Let’s save the output in a data frame bootstrap_distribution_yawning: bootstrap_distribution_yawning &lt;- mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;seed&quot;, &quot;control&quot;)) bootstrap_distribution_yawning # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.0357143 2 2 0.229167 3 3 0.00952381 4 4 0.0106952 5 5 0.00483092 6 6 0.00793651 7 7 -0.0845588 8 8 -0.00466200 9 9 0.164686 10 10 0.124777 # ℹ 990 more rows Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 replicate ID’s and the 1000 differences in proportions for each bootstrap resample in stat. 4. visualize the results In Figure 15.31 we visualize() the resulting bootstrap resampling distribution. Let’s also add a vertical line at 0 by adding a geom_vline() layer. visualize(bootstrap_distribution_yawning) + geom_vline(xintercept = 0) FIGURE 15.31: Bootstrap distribution. First, let’s compute the 95% confidence interval for \\(p_{seed} - p_{control}\\) using the percentile method, in other words, by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped. bootstrap_distribution_yawning %&gt;% get_confidence_interval(type = &quot;percentile&quot;, level = 0.95) # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 -0.238276 0.302464 Second, since the bootstrap distribution is roughly bell-shaped, we can construct a confidence interval using the standard error method as well. Recall that to construct a confidence interval using the standard error method, we need to specify the center of the interval using the point_estimate argument. In our case, we need to set it to be the difference in sample proportions of 4.4% that the Mythbusters observed. We can also use the infer workflow to compute this value by excluding the generate() 1000 bootstrap replicates step. In other words, do not generate replicates, but rather use only the original sample data. We can achieve this by commenting out the generate() line, telling R to ignore it: obs_diff_in_props &lt;- mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% # generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;seed&quot;, &quot;control&quot;)) obs_diff_in_props Response: yawn (factor) Explanatory: group (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 0.0441176 We thus plug this value in as the point_estimate argument. myth_ci_se &lt;- bootstrap_distribution_yawning %&gt;% get_confidence_interval(type = &quot;se&quot;, point_estimate = obs_diff_in_props) Using `level = 0.95` to compute confidence interval. myth_ci_se # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 -0.227291 0.315526 Let’s visualize both confidence intervals in Figure 15.32, with the percentile-method interval marked with black lines and the standard-error-method marked with grey lines. Observe that they are both similar to each other. FIGURE 15.32: Two 95% confidence intervals: percentile method (black) and standard error method (grey). 15.6.4 Interpreting the confidence interval Given that both confidence intervals are quite similar, let’s focus our interpretation to only the percentile-method confidence interval of (-0.238, 0.302). Recall from Subsection 15.5.2 that the precise statistical interpretation of a 95% confidence interval is: if this construction procedure is repeated 100 times, then we expect about 95 of the confidence intervals to capture the true value of \\(p_{seed} - p_{control}\\). In other words, if we gathered 100 samples of \\(n\\) = 50 participants from a similar pool of people and constructed 100 confidence intervals each based on each of the 100 samples, about 95 of them will contain the true value of \\(p_{seed} - p_{control}\\) while about five won’t. Given that this is a little long winded, we use the shorthand interpretation: we’re 95% “confident” that the true difference in proportions \\(p_{seed} - p_{control}\\) is between (-0.238, 0.302). There is one value of particular interest that this 95% confidence interval contains: zero. If \\(p_{seed} - p_{control}\\) were equal to 0, then there would be no difference in proportion yawning between the two groups. This would suggest that there is no associated effect of being exposed to a yawning recruiter on whether you yawn yourself. In our case, since the 95% confidence interval includes 0, we cannot conclusively say if either proportion is larger. Of our 1000 bootstrap resamples with replacement, sometimes \\(\\widehat{p}_{seed}\\) was higher and thus those exposed to yawning yawned themselves more often. At other times, the reverse happened. Say, on the other hand, the 95% confidence interval was entirely above zero. This would suggest that \\(p_{seed} - p_{control} &gt; 0\\), or, in other words \\(p_{seed} &gt; p_{control}\\), and thus we’d have evidence suggesting those exposed to yawning do yawn more often. 15.7 Conclusion 15.7.1 Comparing bootstrap and sampling distributions Let’s talk more about the relationship between sampling distributions and bootstrap distributions. Recall back in Subsection 14.2, we took 1000 virtual samples from the bowl using a virtual shovel, computed 1000 values of the sample proportion red \\(\\widehat{p}\\), then visualized their distribution in a histogram. Recall that this distribution is called the sampling distribution of \\(\\widehat{p}\\). Furthermore, the standard deviation of the sampling distribution has a special name: the standard error. We also mentioned that this sampling activity does not reflect how sampling is done in real life. Rather, it was an idealized version of sampling so that we could study the effects of sampling variation on estimates, like the proportion of the shovel’s balls that are red. In real life, however, one would take a single sample that’s as large as possible, much like in the Obama poll we saw in Section 14.4. But how can we get a sense of the effect of sampling variation on estimates if we only have one sample and thus only one estimate? Don’t we need many samples and hence many estimates? The workaround to having a single sample was to perform bootstrap resampling with replacement from the single sample. We did this in the resampling activity in Section 15.1 where we focused on the mean year of minting of pennies. We used pieces of paper representing the original sample of 50 pennies from the bank and resampled them with replacement from a hat. We had 35 of our friends perform this activity and visualized the resulting 35 sample means \\(\\overline{x}\\) in a histogram in Figure 15.11. This distribution was called the bootstrap distribution of \\(\\overline{x}\\). We stated at the time that the bootstrap distribution is an approximation to the sampling distribution of \\(\\overline{x}\\) in the sense that both distributions will have a similar shape and similar spread. Thus the standard error of the bootstrap distribution can be used as an approximation to the standard error of the sampling distribution. Let’s show you that this is the case by now comparing these two types of distributions. Specifically, we’ll compare the sampling distribution of \\(\\widehat{p}\\) based on 1000 virtual samples from the bowl from Subsection 14.2 to the bootstrap distribution of \\(\\widehat{p}\\) based on 1000 virtual resamples with replacement from Ilyas and Yohan’s single sample bowl_sample_1 from Subsection 15.5.1. Sampling distribution Here is the code you saw in Subsection 14.2 to construct the sampling distribution of \\(\\widehat{p}\\) shown again in Figure 15.33, with some changes to incorporate the statistical terminology relating to sampling from Subsection 14.3.1. # Take 1000 virtual samples of size 50 from the bowl: virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) # Compute the sampling distribution of 1000 values of p-hat sampling_distribution &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) # Visualize sampling distribution of p-hat ggplot(sampling_distribution, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Sampling distribution&quot;) FIGURE 15.33: Previously seen sampling distribution of sample proportion red for \\(n = 1000\\). An important thing to keep in mind is the default value for replace is FALSE when using rep_sample_n(). This is because when sampling 50 balls with a shovel, we are extracting 50 balls one-by-one without replacing them. This is in contrast to bootstrap resampling with replacement, where we resample a ball and put it back, and repeat this process 50 times. Let’s quantify the variability in this sampling distribution by calculating the standard deviation of the prop_red variable representing 1000 values of the sample proportion \\(\\widehat{p}\\). Remember that the standard deviation of the sampling distribution is the standard error, frequently denoted as se. sampling_distribution %&gt;% summarize(se = sd(prop_red)) # A tibble: 1 × 1 se &lt;dbl&gt; 1 0.0673987 Bootstrap distribution Here is the code you previously saw in Subsection 15.5.1 to construct the bootstrap distribution of \\(\\widehat{p}\\) based on Ilyas and Yohan’s original sample of 50 balls saved in bowl_sample_1. bootstrap_distribution &lt;- bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) FIGURE 15.34: Bootstrap distribution of proportion red for \\(n = 1000\\). bootstrap_distribution %&gt;% summarize(se = sd(stat)) # A tibble: 1 × 1 se &lt;dbl&gt; 1 0.0712212 Comparison Now that we have computed both the sampling distribution and the bootstrap distributions, let’s compare them side-by-side in Figure 15.35. We’ll make both histograms have matching scales on the x- and y-axes to make them more comparable. Furthermore, we’ll add: To the sampling distribution on the top: a solid line denoting the proportion of the bowl’s balls that are red \\(p\\) = 0.375. To the bootstrap distribution on the bottom: a dashed line at the sample proportion \\(\\widehat{p}\\) = 21/50 = 0.42 = 42% that Ilyas and Yohan observed. FIGURE 15.35: Comparing the sampling and bootstrap distributions of \\(\\widehat{p}\\). There is a lot going on in Figure 15.35, so let’s break down all the comparisons slowly. First, observe how the sampling distribution on top is centered at \\(p\\) = 0.375. This is because the sampling is done at random and in an unbiased fashion. So the estimates \\(\\widehat{p}\\) are centered at the true value of \\(p\\). However, this is not the case with the following bootstrap distribution. The bootstrap distribution is centered at 0.42, which is the proportion red of Ilyas and Yohan’s 50 sampled balls. This is because we are resampling from the same sample over and over again. Since the bootstrap distribution is centered at the original sample’s proportion, it doesn’t necessarily provide a better estimate of \\(p\\) = 0.375. This leads us to our first lesson about bootstrapping: The bootstrap distribution will likely not have the same center as the sampling distribution. In other words, bootstrapping cannot improve the quality of an estimate. Second, let’s now compare the spread of the two distributions: they are somewhat similar. In the previous code, we computed the standard deviations of both distributions as well. Recall that such standard deviations have a special name: standard errors. Let’s compare them in Table 15.5. TABLE 15.5: Comparing standard errors Distribution type Standard error Sampling distribution 0.067 Bootstrap distribution 0.071 Notice that the bootstrap distribution’s standard error is a rather good approximation to the sampling distribution’s standard error. This leads us to our second lesson about bootstrapping: Even if the bootstrap distribution might not have the same center as the sampling distribution, it will likely have very similar shape and spread. In other words, bootstrapping will give you a good estimate of the standard error. Thus, using the fact that the bootstrap distribution and sampling distributions have similar spreads, we can build confidence intervals using bootstrapping as we’ve done all throughout this chapter! 15.7.2 Theory-based confidence intervals So far in this chapter, we’ve constructed confidence intervals using two methods: the percentile method and the standard error method. Recall also from Subsection 15.3.2 that we can only use the standard-error method if the bootstrap distribution is bell-shaped (i.e., normally distributed). In a similar vein, if the sampling distribution is normally shaped, there is another method for constructing confidence intervals that does not involve using your computer. You can use a theory-based method involving mathematical formulas! The formula uses the rule of thumb we saw in Appendix ?? that 95% of values in a normal distribution are within \\(\\pm 1.96\\) standard deviations of the mean. In the case of sampling and bootstrap distributions, remember that the standard deviation has a special name: the standard error. Recall further in Subsection 14.6.2 you saw that there is a theory-based formula to approximate the standard error for sample proportions \\(\\widehat{p}\\): \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] If you’ve forgotten this fact and what it says about the relationship between “precision” of your estimates and your sample size \\(n\\), we highly recommend you re-read Subsection 14.6.2. Recall from bowl_sample_1 that Yohan and Ilyas sampled \\(n = 50\\) balls and observed a sample proportion \\(\\widehat{p}\\) of 21/50 = 0.42. An approximation of the standard error of \\(\\widehat{p}\\) based on Yohan and Ilyas’ sample is thus: \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{50}} = \\sqrt{0.004872} = 0.0698 \\approx 0.070\\] Let’s compare this theory-based standard error to the standard error of the sampling and bootstrap distributions you computed previously in Subsection 15.7.1 in Table 15.6. Notice how they are all similar! TABLE 15.6: Comparing standard errors Distribution type Standard error Sampling distribution 0.067 Bootstrap distribution 0.071 Formula approximation 0.070 Using the theory-based standard error, let’s present a theory-based method for constructing 95% confidence intervals that does not involve using a computer, but rather mathematical formulas. Note that this theory-based method only holds if the sampling distribution is normally shaped, so that we can use the 95% rule of thumb about normal distributions discussed in Appendix ??. Collect a single representative sample of size \\(n\\) that’s as large as possible. Compute the point estimate: the sample proportion \\(\\widehat{p}\\). Think of this as the center of your “net.” Compute the approximation to the standard error \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Compute a quantity known as the margin of error (more on this later after we list the five steps): \\[\\text{MoE}_{\\widehat{p}} = 1.96 \\cdot \\text{SE}_{\\widehat{p}} = 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Compute both endpoints of the confidence interval. The lower end-point. Think of this as the left end-point of the net: \\[\\widehat{p} - \\text{MoE}_{\\widehat{p}} = \\widehat{p} - 1.96 \\cdot \\text{SE}_{\\widehat{p}} = \\widehat{p} - 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] The upper endpoint. Think of this as the right end-point of the net: \\[\\widehat{p} + \\text{MoE}_{\\widehat{p}} = \\widehat{p} + 1.96 \\cdot \\text{SE}_{\\widehat{p}} = \\widehat{p} + 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Alternatively, you can succinctly summarize a 95% confidence interval for \\(p\\) using the \\(\\pm\\) symbol: \\[\\widehat{p} \\pm \\text{MoE}_{\\widehat{p}} = \\widehat{p} \\pm (1.96 \\cdot \\text{SE}_{\\widehat{p}}) = \\widehat{p} \\pm \\left( 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}} \\right)\\] So going back to Yohan and Ilyas’ sample of \\(n = 50\\) balls that had 21 red balls, the 95% confidence interval for \\(p\\) is \\[ \\begin{aligned} 0.41 \\pm 1.96 \\cdot 0.0698 &amp;= 0.41 \\, \\pm \\, 0.137 \\\\ &amp;= (0.41 - 0.137, \\, 0.41 + 0.137) \\\\ &amp;= (0.273, \\, 0.547). \\end{aligned} \\] Yohan and Ilyas are 95% “confident” that the true proportion red of the bowl’s balls is between 28.3% and 55.7%. Given that the true population proportion \\(p\\) was 0.375, in this case they successfully captured the fish. In Step 4, we defined a statistical quantity known as the margin of error. You can think of this quantity as how much the net extends to the left and to the right of the center of our net. The 1.96 multiplier is rooted in the 95% rule of thumb we introduced earlier and the fact that we want the confidence level to be 95%. The value of the margin of error entirely determines the width of the confidence interval. Recall from Subsection 15.5.3 that confidence interval widths are determined by an interplay of the confidence level, the sample size \\(n\\), and the standard error. Let’s revisit the poll of President Obama’s approval rating among young Americans aged 18-29 which we introduced in Section 14.4. Pollsters found that based on a representative sample of \\(n\\) = 2089 young Americans, \\(\\widehat{p}\\) = 0.41 = 41% supported President Obama. If you look towards the end of the article, it also states: “The poll’s margin of error was plus or minus 2.1 percentage points.” This is precisely the \\(\\text{MoE}\\): \\[ \\begin{aligned} \\text{MoE} &amp;= 1.96 \\cdot \\text{SE} = 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}} = 1.96 \\cdot \\sqrt{\\frac{0.41(1-0.41)}{2089}} \\\\ &amp;= 1.96 \\cdot 0.0108 = 0.021 = 2.1\\% \\end{aligned} \\] Their poll results are based on a confidence level of 95% and the resulting 95% confidence interval for the proportion of all young Americans who support Obama is: \\[\\widehat{p} \\pm \\text{MoE} = 0.41 \\pm 0.021 = (0.389, \\, 0.431) = (38.9\\%, \\, 43.1\\%).\\] Confidence intervals based on 33 tactile samples Let’s revisit our 33 friends’ samples from the bowl from Subsection 14.1.3. We’ll use their 33 samples to construct 33 theory-based 95% confidence intervals for \\(p\\). Recall this data was saved in the tactile_prop_red data frame included in the moderndive package: rename() the variable prop_red to p_hat, the statistical name of the sample proportion \\(\\widehat{p}\\). mutate() a new variable n making explicit the sample size of 50. mutate() other new variables computing: The standard error SE for \\(\\widehat{p}\\) using the previous formula. The margin of error MoE by multiplying the SE by 1.96 The left endpoint of the confidence interval lower_ci The right endpoint of the confidence interval upper_ci conf_ints &lt;- tactile_prop_red %&gt;% rename(p_hat = prop_red) %&gt;% mutate( n = 50, SE = sqrt(p_hat * (1 - p_hat) / n), MoE = 1.96 * SE, lower_ci = p_hat - MoE, upper_ci = p_hat + MoE ) # A tibble: 33 × 9 group replicate red_balls p_hat n SE MoE lower_ci upper_ci &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Ilyas, … 1 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 2 Morgan,… 2 17 0.34 50 0.0669925 0.131305 0.208695 0.471305 3 Martin,… 3 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 4 Clark, … 4 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 5 Riddhi,… 5 18 0.36 50 0.0678823 0.133049 0.226951 0.493049 6 Andrew,… 6 19 0.38 50 0.0686440 0.134542 0.245458 0.514542 7 Julia 7 19 0.38 50 0.0686440 0.134542 0.245458 0.514542 8 Rachel,… 8 11 0.22 50 0.0585833 0.114823 0.105177 0.334823 9 Daniel,… 9 15 0.3 50 0.0648074 0.127023 0.172977 0.427023 10 Josh, M… 10 17 0.34 50 0.0669925 0.131305 0.208695 0.471305 # ℹ 23 more rows In Figure 15.36, let’s plot the 33 confidence intervals for \\(p\\) saved in conf_ints along with a vertical line at \\(p\\) = 0.375 indicating the true proportion of the bowl’s balls that are red. Furthermore, let’s mark the sample proportions \\(\\widehat{p}\\) with dots since they represent the centers of these confidence intervals. FIGURE 15.36: 33 confidence intervals at the 95% level based on 33 tactile samples of size \\(n = 50\\). Observe that 31 of the 33 confidence intervals “captured” the true value of \\(p\\), for a success rate of 31 / 33 = 93.94%. While this is not quite 95%, recall that we expect about 95% of such confidence intervals to capture \\(p\\). The actual observed success rate will vary slightly. Theory-based methods like this have largely been used in the past because we didn’t have the computing power to perform simulation-based methods such as bootstrapping. They are still commonly used, however, and if the sampling distribution is normally distributed, we have access to an alternative method for constructing confidence intervals as well as performing hypothesis tests as we will see in Chapter 16. The kind of computer-based statistical inference we’ve seen so far has a particular name in the field of statistics: simulation-based inference. This is because we are performing statistical inference using computer simulations. In our opinion, two large benefits of simulation-based methods over theory-based methods are that (1) they are easier for people new to statistical inference to understand and (2) they also work in situations where theory-based methods and mathematical formulas don’t exist. 15.7.3 Additional resources An R script file of all R code used in this chapter is available here. If you want more examples of the infer workflow to construct confidence intervals, we suggest you check out the infer package homepage, in particular, a series of example analyses available at https://infer.netlify.app/articles/. 15.7.4 What’s to come? Now that we’ve equipped ourselves with confidence intervals, in Chapter 16 we’ll cover the other common tool for statistical inference: hypothesis testing. Just like confidence intervals, hypothesis tests are used to infer about a population using a sample. However, we’ll see that the framework for making such inferences is slightly different. "],["16-hypothesis-testing.html", "Chapter 16 Hypothesis Testing 16.1 Promotions activity 16.2 Understanding hypothesis tests 16.3 Conducting hypothesis tests 16.4 Interpreting hypothesis tests 16.5 Case study: Are action or romance movies rated higher? 16.6 Conclusion", " Chapter 16 Hypothesis Testing Now that we’ve studied confidence intervals in Chapter 15, let’s study another commonly used method for statistical inference: hypothesis testing. Hypothesis tests allow us to take a sample of data from a population and infer about the plausibility of competing hypotheses. For example, in the upcoming “promotions” activity in Section 16.1, you’ll study the data collected from a psychology study in the 1970s to investigate whether gender-based discrimination in promotion rates existed in the banking industry at the time of the study. The good news is we’ve already covered many of the necessary concepts to understand hypothesis testing in Chapters 14 and 15. We will expand further on these ideas here and also provide a general framework for understanding hypothesis tests. By understanding this general framework, you’ll be able to adapt it to many different scenarios. The same can be said for confidence intervals. There was one general framework that applies to all confidence intervals and the infer package was designed around this framework. While the specifics may change slightly for different types of confidence intervals, the general framework stays the same. We believe that this approach is much better for long-term learning than focusing on specific details for specific confidence intervals using theory-based approaches. As you’ll now see, we prefer this general framework for hypothesis tests as well. If you’d like more practice or you’re curious to see how this framework applies to different scenarios, you can find fully-worked out examples for many common hypothesis tests and their corresponding confidence intervals in Appendix B. We recommend that you carefully review these examples as they also cover how the general frameworks apply to traditional theory-based methods like the \\(t\\)-test and normal-theory confidence intervals. You’ll see there that these traditional methods are just approximations for the computer-based methods we’ve been focusing on. However, they also require conditions to be met for their results to be valid. Computer-based methods using randomization, simulation, and bootstrapping have much fewer restrictions. Furthermore, they help develop your computational thinking, which is one big reason they are emphasized throughout this book. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section ?? that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section ?? for information on how to install and load R packages. library(tidyverse) library(infer) library(moderndive) library(nycflights13) library(ggplot2movies) 16.1 Promotions activity Let’s start with an activity studying the effect of gender on promotions at a bank. 16.1.1 Does gender affect promotions at a bank? Say you are working at a bank in the 1970s and you are submitting your résumé to apply for a promotion. Will your gender affect your chances of getting promoted? To answer this question, we’ll focus on data from a study published in the Journal of Applied Psychology in 1974. This data is also used in the OpenIntro series of statistics textbooks. To begin the study, 48 bank supervisors were asked to assume the role of a hypothetical director of a bank with multiple branches. Every one of the bank supervisors was given a résumé and asked whether or not the candidate on the résumé was fit to be promoted to a new position in one of their branches. However, each of these 48 résumés were identical in all respects except one: the name of the applicant at the top of the résumé. Of the supervisors, 24 were randomly given résumés with stereotypically “male” names, while 24 of the supervisors were randomly given résumés with stereotypically “female” names. Since only (binary) gender varied from résumé to résumé, researchers could isolate the effect of this variable in promotion rates. While many people today (including us, the authors) disagree with such binary views of gender, it is important to remember that this study was conducted at a time where more nuanced views of gender were not as prevalent. Despite this imperfection, we decided to still use this example as we feel it presents ideas still relevant today about how we could study discrimination in the workplace. The moderndive package contains the data on the 48 applicants in the promotions data frame. Let’s explore this data by looking at six randomly selected rows: promotions %&gt;% sample_n(size = 6) %&gt;% arrange(id) # A tibble: 6 × 3 id decision gender &lt;int&gt; &lt;fct&gt; &lt;fct&gt; 1 11 promoted male 2 26 promoted female 3 28 promoted female 4 36 not male 5 37 not male 6 46 not female The variable id acts as an identification variable for all 48 rows, the decision variable indicates whether the applicant was selected for promotion or not, while the gender variable indicates the gender of the name used on the résumé. Recall that this data does not pertain to 24 actual men and 24 actual women, but rather 48 identical résumés of which 24 were assigned stereotypically “male” names and 24 were assigned stereotypically “female” names. Let’s perform an exploratory data analysis of the relationship between the two categorical variables decision and gender. Recall that we saw in Subsection 12.8.3 that one way we can visualize such a relationship is by using a stacked barplot. ggplot(promotions, aes(x = gender, fill = decision)) + geom_bar() + labs(x = &quot;Gender of name on résumé&quot;) FIGURE 16.1: Barplot relating gender to promotion decision. Observe in Figure 16.1 that it appears that résumés with female names were much less likely to be accepted for promotion. Let’s quantify these promotion rates by computing the proportion of résumés accepted for promotion for each group using the dplyr package for data wrangling. Note the use of the tally() function here which is a shortcut for summarize(n = n()) to get counts. promotions %&gt;% group_by(gender, decision) %&gt;% tally() # A tibble: 4 × 3 # Groups: gender [2] gender decision n &lt;fct&gt; &lt;fct&gt; &lt;int&gt; 1 male not 3 2 male promoted 21 3 female not 10 4 female promoted 14 So of the 24 résumés with male names, 21 were selected for promotion, for a proportion of 21/24 = 0.875 = 87.5%. On the other hand, of the 24 résumés with female names, 14 were selected for promotion, for a proportion of 14/24 = 0.583 = 58.3%. Comparing these two rates of promotion, it appears that résumés with male names were selected for promotion at a rate 0.875 - 0.583 = 0.292 = 29.2% higher than résumés with female names. This is suggestive of an advantage for résumés with a male name on it. The question is, however, does this provide conclusive evidence that there is gender discrimination in promotions at banks? Could a difference in promotion rates of 29.2% still occur by chance, even in a hypothetical world where no gender-based discrimination existed? In other words, what is the role of sampling variation in this hypothesized world? To answer this question, we’ll again rely on a computer to run simulations. 16.1.2 Shuffling once First, try to imagine a hypothetical universe where no gender discrimination in promotions existed. In such a hypothetical universe, the gender of an applicant would have no bearing on their chances of promotion. Bringing things back to our promotions data frame, the gender variable would thus be an irrelevant label. If these gender labels were irrelevant, then we could randomly reassign them by “shuffling” them to no consequence! To illustrate this idea, let’s narrow our focus to 6 arbitrarily chosen résumés of the 48 in Table 16.1. The decision column shows that 3 résumés resulted in promotion while 3 didn’t. The gender column shows what the original gender of the résumé name was. However, in our hypothesized universe of no gender discrimination, gender is irrelevant and thus it is of no consequence to randomly “shuffle” the values of gender. The shuffled_gender column shows one such possible random shuffling. Observe in the fourth column how the number of male and female names remains the same at 3 each, but they are now listed in a different order. TABLE 16.1: One example of shuffling gender variable résumé number decision gender shuffled gender 1 not male male 2 not female male 3 not female female 4 promoted male female 5 promoted male female 6 promoted female male Again, such random shuffling of the gender label only makes sense in our hypothesized universe of no gender discrimination. How could we extend this shuffling of the gender variable to all 48 résumés by hand? One way would be by using standard deck of 52 playing cards, which we display in Figure 16.2. FIGURE 16.2: Standard deck of 52 playing cards. Since half the cards are red (diamonds and hearts) and the other half are black (spades and clubs), by removing two red cards and two black cards, we would end up with 24 red cards and 24 black cards. After shuffling these 48 cards as seen in Figure 16.3, we can flip the cards over one-by-one, assigning “male” for each red card and “female” for each black card. FIGURE 16.3: Shuffling a deck of cards. We’ve saved one such shuffling in the promotions_shuffled data frame of the moderndive package. If you compare the original promotions and the shuffled promotions_shuffled data frames, you’ll see that while the decision variable is identical, the gender variable has changed. Let’s repeat the same exploratory data analysis we did for the original promotions data on our promotions_shuffled data frame. Let’s create a barplot visualizing the relationship between decision and the new shuffled gender variable and compare this to the original unshuffled version in Figure 16.4. ggplot(promotions_shuffled, aes(x = gender, fill = decision)) + geom_bar() + labs(x = &quot;Gender of résumé name&quot;) FIGURE 16.4: Barplots of relationship of promotion with gender (left) and shuffled gender (right). It appears the difference in “male names” versus “female names” promotion rates is now different. Compared to the original data in the left barplot, the new “shuffled” data in the right barplot has promotion rates that are much more similar. Let’s also compute the proportion of résumés accepted for promotion for each group: promotions_shuffled %&gt;% group_by(gender, decision) %&gt;% tally() # Same as summarize(n = n()) # A tibble: 4 × 3 # Groups: gender [2] gender decision n &lt;fct&gt; &lt;fct&gt; &lt;int&gt; 1 male not 6 2 male promoted 18 3 female not 7 4 female promoted 17 So in this hypothetical universe of no discrimination, \\(18/24 = 0.75 = 75\\%\\) of “male” résumés were selected for promotion. On the other hand, \\(17/24 = 0.708 = 70.8\\%\\) of “female” résumés were selected for promotion. Let’s next compare these two values. It appears that résumés with stereotypically male names were selected for promotion at a rate that was \\(0.75 - 0.708 = 0.042 = 4.2\\%\\) different than résumés with stereotypically female names. Observe how this difference in rates is not the same as the difference in rates of 0.292 = 29.2% we originally observed. This is once again due to sampling variation. How can we better understand the effect of this sampling variation? By repeating this shuffling several times! 16.1.3 Shuffling 16 times We recruited 16 groups of our friends to repeat this shuffling exercise. They recorded these values in a shared spreadsheet; we display a snapshot of the first 10 rows and 5 columns in Figure 16.5. FIGURE 16.5: Snapshot of shared spreadsheet of shuffling results (m for male, f for female). For each of these 16 columns of shuffles, we computed the difference in promotion rates, and in Figure 16.6 we display their distribution in a histogram. We also mark the observed difference in promotion rate that occurred in real life of 0.292 = 29.2% with a dark line. FIGURE 16.6: Distribution of shuffled differences in promotions. Before we discuss the distribution of the histogram, we emphasize the key thing to remember: this histogram represents differences in promotion rates that one would observe in our hypothesized universe of no gender discrimination. Observe first that the histogram is roughly centered at 0. Saying that the difference in promotion rates is 0 is equivalent to saying that both genders had the same promotion rate. In other words, the center of these 16 values is consistent with what we would expect in our hypothesized universe of no gender discrimination. However, while the values are centered at 0, there is variation about 0. This is because even in a hypothesized universe of no gender discrimination, you will still likely observe small differences in promotion rates because of chance sampling variation. Looking at the histogram in Figure 16.6, such differences could even be as extreme as -0.292 or 0.208. Turning our attention to what we observed in real life: the difference of 0.292 = 29.2% is marked with a vertical dark line. Ask yourself: in a hypothesized world of no gender discrimination, how likely would it be that we observe this difference? While opinions here may differ, in our opinion not often! Now ask yourself: what do these results say about our hypothesized universe of no gender discrimination? 16.1.4 What did we just do? What we just demonstrated in this activity is the statistical procedure known as hypothesis testing using a permutation test. The term “permutation” is the mathematical term for “shuffling”: taking a series of values and reordering them randomly, as you did with the playing cards. In fact, permutations are another form of resampling, like the bootstrap method you performed in Chapter 15. While the bootstrap method involves resampling with replacement, permutation methods involve resampling without replacement. Think of our exercise involving the slips of paper representing pennies and the hat in Section 15.1: after sampling a penny, you put it back in the hat. Now think of our deck of cards. After drawing a card, you laid it out in front of you, recorded the color, and then you did not put it back in the deck. In our previous example, we tested the validity of the hypothesized universe of no gender discrimination. The evidence contained in our observed sample of 48 résumés was somewhat inconsistent with our hypothesized universe. Thus, we would be inclined to reject this hypothesized universe and declare that the evidence suggests there is gender discrimination. Recall our case study on whether yawning is contagious from Section 15.6. The previous example involves inference about an unknown difference of population proportions as well. This time, it will be \\(p_{m} - p_{f}\\), where \\(p_{m}\\) is the population proportion of résumés with male names being recommended for promotion and \\(p_{f}\\) is the equivalent for résumés with female names. Recall that this is one of the scenarios for inference we’ve seen so far in Table 16.2. TABLE 16.2: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) So, based on our sample of \\(n_m\\) = 24 “male” applicants and \\(n_w\\) = 24 “female” applicants, the point estimate for \\(p_{m} - p_{f}\\) is the difference in sample proportions \\(\\widehat{p}_{m} -\\widehat{p}_{f}\\) = 0.875 - 0.583 = 0.292 = 29.2%. This difference in favor of “male” résumés of 0.292 is greater than 0, suggesting discrimination in favor of men. However, the question we asked ourselves was “is this difference meaningfully greater than 0?”. In other words, is that difference indicative of true discrimination, or can we just attribute it to sampling variation? Hypothesis testing allows us to make such distinctions. 16.2 Understanding hypothesis tests Much like the terminology, notation, and definitions relating to sampling you saw in Section 14.3, there are a lot of terminology, notation, and definitions related to hypothesis testing as well. Learning these may seem like a very daunting task at first. However, with practice, practice, and more practice, anyone can master them. First, a hypothesis is a statement about the value of an unknown population parameter. In our résumé activity, our population parameter of interest is the difference in population proportions \\(p_{m} - p_{f}\\). Hypothesis tests can involve any of the population parameters in Table 14.5 of the five inference scenarios we’ll cover in this book and also more advanced types we won’t cover here. Second, a hypothesis test consists of a test between two competing hypotheses: (1) a null hypothesis \\(H_0\\) (pronounced “H-naught”) versus (2) an alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\)). Generally the null hypothesis is a claim that there is “no effect” or “no difference of interest.” In many cases, the null hypothesis represents the status quo or a situation that nothing interesting is happening. Furthermore, generally the alternative hypothesis is the claim the experimenter or researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis \\(H_0\\). In our résumé activity, an appropriate hypothesis test would be: \\[ \\begin{aligned} H_0 &amp;: \\text{men and women are promoted at the same rate}\\\\ \\text{vs } H_A &amp;: \\text{men are promoted at a higher rate than women} \\end{aligned} \\] Note some of the choices we have made. First, we set the null hypothesis \\(H_0\\) to be that there is no difference in promotion rate and the “challenger” alternative hypothesis \\(H_A\\) to be that there is a difference. While it would not be wrong in principle to reverse the two, it is a convention in statistical inference that the null hypothesis is set to reflect a “null” situation where “nothing is going on.” As we discussed earlier, in this case, \\(H_0\\) corresponds to there being no difference in promotion rates. Furthermore, we set \\(H_A\\) to be that men are promoted at a higher rate, a subjective choice reflecting a prior suspicion we have that this is the case. We call such alternative hypotheses one-sided alternatives. If someone else however does not share such suspicions and only wants to investigate that there is a difference, whether higher or lower, they would set what is known as a two-sided alternative. We can re-express the formulation of our hypothesis test using the mathematical notation for our population parameter of interest, the difference in population proportions \\(p_{m} - p_{f}\\): \\[ \\begin{aligned} H_0 &amp;: p_{m} - p_{f} = 0\\\\ \\text{vs } H_A&amp;: p_{m} - p_{f} &gt; 0 \\end{aligned} \\] Observe how the alternative hypothesis \\(H_A\\) is one-sided with \\(p_{m} - p_{f} &gt; 0\\). Had we opted for a two-sided alternative, we would have set \\(p_{m} - p_{f} \\neq 0\\). To keep things simple for now, we’ll stick with the simpler one-sided alternative. We’ll present an example of a two-sided alternative in Section 16.5. Third, a test statistic is a point estimate/sample statistic formula used for hypothesis testing. Note that a sample statistic is merely a summary statistic based on a sample of observations. Recall we saw in Section 13.3 that a summary statistic takes in many values and returns only one. Here, the samples would be the \\(n_m\\) = 24 résumés with male names and the \\(n_f\\) = 24 résumés with female names. Hence, the point estimate of interest is the difference in sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\). Fourth, the observed test statistic is the value of the test statistic that we observed in real life. In our case, we computed this value using the data saved in the promotions data frame. It was the observed difference of \\(\\widehat{p}_{m} -\\widehat{p}_{f} = 0.875 - 0.583 = 0.292 = 29.2\\%\\) in favor of résumés with male names. Fifth, the null distribution is the sampling distribution of the test statistic assuming the null hypothesis \\(H_0\\) is true. Ooof! That’s a long one! Let’s unpack it slowly. The key to understanding the null distribution is that the null hypothesis \\(H_0\\) is assumed to be true. We’re not saying that \\(H_0\\) is true at this point, we’re only assuming it to be true for hypothesis testing purposes. In our case, this corresponds to our hypothesized universe of no gender discrimination in promotion rates. Assuming the null hypothesis \\(H_0\\), also stated as “Under \\(H_0\\),” how does the test statistic vary due to sampling variation? In our case, how will the difference in sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) vary due to sampling under \\(H_0\\)? Recall from Subsection 14.3.2 that distributions displaying how point estimates vary due to sampling variation are called sampling distributions. The only additional thing to keep in mind about null distributions is that they are sampling distributions assuming the null hypothesis \\(H_0\\) is true. In our case, we previously visualized a null distribution in Figure 16.6, which we re-display in Figure 16.7 using our new notation and terminology. It is the distribution of the 16 differences in sample proportions our friends computed assuming a hypothetical universe of no gender discrimination. We also mark the value of the observed test statistic of 0.292 with a vertical line. FIGURE 16.7: Null distribution and observed test statistic. Sixth, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. Double ooof! Let’s unpack this slowly as well. You can think of the \\(p\\)-value as a quantification of “surprise”: assuming \\(H_0\\) is true, how surprised are we with what we observed? Or in our case, in our hypothesized universe of no gender discrimination, how surprised are we that we observed a difference in promotion rates of 0.292 from our collected samples assuming \\(H_0\\) is true? Very surprised? Somewhat surprised? The \\(p\\)-value quantifies this probability, or in the case of our 16 differences in sample proportions in Figure 16.7, what proportion had a more “extreme” result? Here, extreme is defined in terms of the alternative hypothesis \\(H_A\\) that “male” applicants are promoted at a higher rate than “female” applicants. In other words, how often was the discrimination in favor of men even more pronounced than \\(0.875 - 0.583 = 0.292 = 29.2\\%\\)? In this case, 0 times out of 16, we obtained a difference in proportion greater than or equal to the observed difference of 0.292 = 29.2%. A very rare (in fact, not occurring) outcome! Given the rarity of such a pronounced difference in promotion rates in our hypothesized universe of no gender discrimination, we’re inclined to reject our hypothesized universe. Instead, we favor the hypothesis stating there is discrimination in favor of the “male” applicants. In other words, we reject \\(H_0\\) in favor of \\(H_A\\). Seventh and lastly, in many hypothesis testing procedures, it is commonly recommended to set the significance level of the test beforehand. It is denoted by the Greek letter \\(\\alpha\\) (pronounced “alpha”). This value acts as a cutoff on the \\(p\\)-value, where if the \\(p\\)-value falls below \\(\\alpha\\), we would “reject the null hypothesis \\(H_0\\).” Alternatively, if the \\(p\\)-value does not fall below \\(\\alpha\\), we would “fail to reject \\(H_0\\).” Note the latter statement is not quite the same as saying we “accept \\(H_0\\).” This distinction is rather subtle and not immediately obvious. So we’ll revisit it later in Section 16.4. While different fields tend to use different values of \\(\\alpha\\), some commonly used values for \\(\\alpha\\) are 0.1, 0.01, and 0.05; with 0.05 being the choice people often make without putting much thought into it. We’ll talk more about \\(\\alpha\\) significance levels in Section 16.4, but first let’s fully conduct the hypothesis test corresponding to our promotions activity using the infer package. 16.3 Conducting hypothesis tests In Section 15.4, we showed you how to construct confidence intervals. We first illustrated how to do this using dplyr data wrangling verbs and the rep_sample_n() function from Subsection 14.2.3 which we used as a virtual shovel. In particular, we constructed confidence intervals by resampling with replacement by setting the replace = TRUE argument to the rep_sample_n() function. We then showed you how to perform the same task using the infer package workflow. While both workflows resulted in the same bootstrap distribution from which we can construct confidence intervals, the infer package workflow emphasizes each of the steps in the overall process in Figure 16.8. It does so using function names that are intuitively named with verbs: specify() the variables of interest in your data frame. generate() replicates of bootstrap resamples with replacement. calculate() the summary statistic of interest. visualize() the resulting bootstrap distribution and confidence interval. FIGURE 16.8: Confidence intervals with the infer package. In this section, we’ll now show you how to seamlessly modify the previously seen infer code for constructing confidence intervals to conduct hypothesis tests. You’ll notice that the basic outline of the workflow is almost identical, except for an additional hypothesize() step between the specify() and generate() steps, as can be seen in Figure 16.9. FIGURE 16.9: Hypothesis testing with the infer package. Furthermore, we’ll use a pre-specified significance level \\(\\alpha\\) = 0.05 for this hypothesis test. Let’s leave discussion on the choice of this \\(\\alpha\\) value until later on in Section 16.4. 16.3.1 infer package workflow 1. specify variables Recall that we use the specify() verb to specify the response variable and, if needed, any explanatory variables for our study. In this case, since we are interested in any potential effects of gender on promotion decisions, we set decision as the response variable and gender as the explanatory variable. We do so using formula = response ~ explanatory where response is the name of the response variable in the data frame and explanatory is the name of the explanatory variable. So in our case it is decision ~ gender. Furthermore, since we are interested in the proportion of résumés \"promoted\", and not the proportion of résumés not promoted, we set the argument success to \"promoted\". promotions %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) Response: decision (factor) Explanatory: gender (factor) # A tibble: 48 × 2 decision gender &lt;fct&gt; &lt;fct&gt; 1 promoted male 2 promoted male 3 promoted male 4 promoted male 5 promoted male 6 promoted male 7 promoted male 8 promoted male 9 promoted male 10 promoted male # ℹ 38 more rows Again, notice how the promotions data itself doesn’t change, but the Response: decision (factor) and Explanatory: gender (factor) meta-data do. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Section 13.4. 2. hypothesize the null In order to conduct hypothesis tests using the infer workflow, we need a new step not present for confidence intervals: hypothesize(). Recall from Section 16.2 that our hypothesis test was \\[ \\begin{aligned} H_0 &amp;: p_{m} - p_{f} = 0\\\\ \\text{vs. } H_A&amp;: p_{m} - p_{f} &gt; 0 \\end{aligned} \\] In other words, the null hypothesis \\(H_0\\) corresponding to our “hypothesized universe” stated that there was no difference in gender-based discrimination rates. We set this null hypothesis \\(H_0\\) in our infer workflow using the null argument of the hypothesize() function to either: \"point\" for hypotheses involving a single sample or \"independence\" for hypotheses involving two samples. In our case, since we have two samples (the résumés with “male” and “female” names), we set null = \"independence\". promotions %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) Response: decision (factor) Explanatory: gender (factor) Null Hypothesis: independence # A tibble: 48 × 2 decision gender &lt;fct&gt; &lt;fct&gt; 1 promoted male 2 promoted male 3 promoted male 4 promoted male 5 promoted male 6 promoted male 7 promoted male 8 promoted male 9 promoted male 10 promoted male # ℹ 38 more rows Again, the data has not changed yet. This will occur at the upcoming generate() step; we’re merely setting meta-data for now. Where do the terms \"point\" and \"independence\" come from? These are two technical statistical terms. The term “point” relates from the fact that for a single group of observations, you will test the value of a single point. Going back to the pennies example from Chapter 15, say we wanted to test if the mean year of all US pennies was equal to 1993 or not. We would be testing the value of a “point” \\(\\mu\\), the mean year of all US pennies, as follows \\[ \\begin{aligned} H_0 &amp;: \\mu = 1993\\\\ \\text{vs } H_A&amp;: \\mu \\neq 1993 \\end{aligned} \\] The term “independence” relates to the fact that for two groups of observations, you are testing whether or not the response variable is independent of the explanatory variable that assigns the groups. In our case, we are testing whether the decision response variable is “independent” of the explanatory variable gender that assigns each résumé to either of the two groups. 3. generate replicates After we hypothesize() the null hypothesis, we generate() replicates of “shuffled” datasets assuming the null hypothesis is true. We do this by repeating the shuffling exercise you performed in Section 16.1 several times. Instead of merely doing it 16 times as our groups of friends did, let’s use the computer to repeat this 1000 times by setting reps = 1000 in the generate() function. However, unlike for confidence intervals where we generated replicates using type = \"bootstrap\" resampling with replacement, we’ll now perform shuffles/permutations by setting type = \"permute\". Recall that shuffles/permutations are a kind of resampling, but unlike the bootstrap method, they involve resampling without replacement. promotions_generate &lt;- promotions %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) nrow(promotions_generate) [1] 48000 Observe that the resulting data frame has 48,000 rows. This is because we performed shuffles/permutations for each of the 48 rows 1000 times and \\(48,000 = 1000 \\cdot 48\\). If you explore the promotions_generate data frame with View(), you’ll notice that the variable replicate indicates which resample each row belongs to. So it has the value 1 48 times, the value 2 48 times, all the way through to the value 1000 48 times. 4. calculate summary statistics Now that we have generated 1000 replicates of “shuffles” assuming the null hypothesis is true, let’s calculate() the appropriate summary statistic for each of our 1000 shuffles. From Section 16.2, point estimates related to hypothesis testing have a specific name: test statistics. Since the unknown population parameter of interest is the difference in population proportions \\(p_{m} - p_{f}\\), the test statistic here is the difference in sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\). For each of our 1000 shuffles, we can calculate this test statistic by setting stat = \"diff in props\". Furthermore, since we are interested in \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) we set order = c(\"male\", \"female\"). As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly. Let’s save the result in a data frame called null_distribution: null_distribution &lt;- promotions %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) null_distribution # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 -0.0416667 2 2 -0.125 3 3 -0.125 4 4 -0.0416667 5 5 -0.0416667 6 6 -0.125 7 7 -0.125 8 8 -0.125 9 9 -0.0416667 10 10 -0.0416667 # ℹ 990 more rows Observe that we have 1000 values of stat, each representing one instance of \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) in a hypothesized world of no gender discrimination. Observe as well that we chose the name of this data frame carefully: null_distribution. Recall once again from Section 16.2 that sampling distributions when the null hypothesis \\(H_0\\) is assumed to be true have a special name: the null distribution. What was the observed difference in promotion rates? In other words, what was the observed test statistic \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\)? Recall from Section 16.1 that we computed this observed difference by hand to be 0.875 - 0.583 = 0.292 = 29.2%. We can also compute this value using the previous infer code but with the hypothesize() and generate() steps removed. Let’s save this in obs_diff_prop: obs_diff_prop &lt;- promotions %&gt;% specify(decision ~ gender, success = &quot;promoted&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) obs_diff_prop Response: decision (factor) Explanatory: gender (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 0.291667 5. visualize the p-value The final step is to measure how surprised we are by a promotion difference of 29.2% in a hypothesized universe of no gender discrimination. If the observed difference of 0.292 is highly unlikely, then we would be inclined to reject the validity of our hypothesized universe. We start by visualizing the null distribution of our 1000 values of \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) using visualize() in Figure 16.10. Recall that these are values of the difference in promotion rates assuming \\(H_0\\) is true. This corresponds to being in our hypothesized universe of no gender discrimination. visualize(null_distribution, bins = 10) FIGURE 16.10: Null distribution. Let’s now add what happened in real life to Figure 16.10, the observed difference in promotion rates of 0.875 - 0.583 = 0.292 = 29.2%. However, instead of merely adding a vertical line using geom_vline(), let’s use the shade_p_value() function with obs_stat set to the observed test statistic value we saved in obs_diff_prop. Furthermore, we’ll set the direction = \"right\" reflecting our alternative hypothesis \\(H_A: p_{m} - p_{f} &gt; 0\\). Recall our alternative hypothesis \\(H_A\\) is that \\(p_{m} - p_{f} &gt; 0\\), stating that there is a difference in promotion rates in favor of résumés with male names. “More extreme” here corresponds to differences that are “bigger” or “more positive” or “more to the right.” Hence we set the direction argument of shade_p_value() to be \"right\". On the other hand, had our alternative hypothesis \\(H_A\\) been the other possible one-sided alternative \\(p_{m} - p_{f} &lt; 0\\), suggesting discrimination in favor of résumés with female names, we would’ve set direction = \"left\". Had our alternative hypothesis \\(H_A\\) been two-sided \\(p_{m} - p_{f} \\neq 0\\), suggesting discrimination in either direction, we would’ve set direction = \"both\". visualize(null_distribution, bins = 10) + shade_p_value(obs_stat = obs_diff_prop, direction = &quot;right&quot;) FIGURE 16.11: Shaded histogram to show \\(p\\)-value. In the resulting Figure 16.11, the solid dark line marks 0.292 = 29.2%. However, what does the shaded-region correspond to? This is the \\(p\\)-value. Recall the definition of the \\(p\\)-value from Section 16.2: A \\(p\\)-value is the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. So judging by the shaded region in Figure 16.11, it seems we would somewhat rarely observe differences in promotion rates of 0.292 = 29.2% or more in a hypothesized universe of no gender discrimination. In other words, the \\(p\\)-value is somewhat small. Hence, we would be inclined to reject this hypothesized universe, or using statistical language we would “reject \\(H_0\\).” What fraction of the null distribution is shaded? In other words, what is the exact value of the \\(p\\)-value? We can compute it using the get_p_value() function with the same arguments as the previous shade_p_value() code: null_distribution %&gt;% get_p_value(obs_stat = obs_diff_prop, direction = &quot;right&quot;) # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0.027 Keeping the definition of a \\(p\\)-value in mind, the probability of observing a difference in promotion rates as large as 0.292 = 29.2% due to sampling variation alone in the null distribution is 0.027 = 2.7%. Since this \\(p\\)-value is smaller than our pre-specified significance level \\(\\alpha\\) = 0.05, we reject the null hypothesis \\(H_0: p_{m} - p_{f} = 0\\). In other words, this \\(p\\)-value is sufficiently small to reject our hypothesized universe of no gender discrimination. We instead have enough evidence to change our mind in favor of gender discrimination being a likely culprit here. Observe that whether we reject the null hypothesis \\(H_0\\) or not depends in large part on our choice of significance level \\(\\alpha\\). We’ll discuss this more in Subsection 16.4.3. 16.3.2 Comparison with confidence intervals One of the great things about the infer package is that we can jump seamlessly between conducting hypothesis tests and constructing confidence intervals with minimal changes! Recall the code from the previous section that creates the null distribution, which in turn is needed to compute the \\(p\\)-value: null_distribution &lt;- promotions %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) To create the corresponding bootstrap distribution needed to construct a 95% confidence interval for \\(p_{m} - p_{f}\\), we only need to make two changes. First, we remove the hypothesize() step since we are no longer assuming a null hypothesis \\(H_0\\) is true. We can do this by deleting or commenting out the hypothesize() line of code. Second, we switch the type of resampling in the generate() step to be \"bootstrap\" instead of \"permute\". bootstrap_distribution &lt;- promotions %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) %&gt;% # Change 1 - Remove hypothesize(): # hypothesize(null = &quot;independence&quot;) %&gt;% # Change 2 - Switch type from &quot;permute&quot; to &quot;bootstrap&quot;: generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) Using this bootstrap_distribution, let’s first compute the percentile-based confidence intervals, as we did in Section 15.4: percentile_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.0444444 0.538542 Using our shorthand interpretation for 95% confidence intervals from Subsection 15.5.2, we are 95% “confident” that the true difference in population proportions \\(p_{m} - p_{f}\\) is between (0.044, 0.539). Let’s visualize bootstrap_distribution and this percentile-based 95% confidence interval for \\(p_{m} - p_{f}\\) in Figure 16.12. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = percentile_ci) FIGURE 16.12: Percentile-based 95% confidence interval. Notice a key value that is not included in the 95% confidence interval for \\(p_{m} - p_{f}\\): the value 0. In other words, a difference of 0 is not included in our net, suggesting that \\(p_{m}\\) and \\(p_{f}\\) are truly different! Furthermore, observe how the entirety of the 95% confidence interval for \\(p_{m} - p_{f}\\) lies above 0, suggesting that this difference is in favor of men. Since the bootstrap distribution appears to be roughly normally shaped, we can also use the standard error method as we did in Section 15.4. In this case, we must specify the point_estimate argument as the observed difference in promotion rates 0.292 = 29.2% saved in obs_diff_prop. This value acts as the center of the confidence interval. se_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(level = 0.95, type = &quot;se&quot;, point_estimate = obs_diff_prop) se_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.0514129 0.531920 Let’s visualize bootstrap_distribution again, but now the standard error based 95% confidence interval for \\(p_{m} - p_{f}\\) in Figure 16.13. Again, notice how the value 0 is not included in our confidence interval, again suggesting that \\(p_{m}\\) and \\(p_{f}\\) are truly different! visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = se_ci) FIGURE 16.13: Standard error-based 95% confidence interval. Learning check (LC9.1) Why does the following code produce an error? In other words, what about the response and predictor variables make this not a possible computation with the infer package? library(moderndive) library(infer) null_distribution_mean &lt;- promotions %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) (LC9.2) Why are we relatively confident that the distributions of the sample proportions will be good approximations of the population distributions of promotion proportions for the two genders? (LC9.3) Using the definition of p-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the promotion rates for males and females. 16.3.3 “There is only one test” Let’s recap the steps necessary to conduct a hypothesis test using the terminology, notation, and definitions related to sampling you saw in Section 16.2 and the infer workflow from Subsection 16.3.1: specify() the variables of interest in your data frame. hypothesize() the null hypothesis \\(H_0\\). In other words, set a “model for the universe” assuming \\(H_0\\) is true. generate() shuffles assuming \\(H_0\\) is true. In other words, simulate data assuming \\(H_0\\) is true. calculate() the test statistic of interest, both for the observed data and your simulated data. visualize() the resulting null distribution and compute the \\(p\\)-value by comparing the null distribution to the observed test statistic. While this is a lot to digest, especially the first time you encounter hypothesis testing, the nice thing is that once you understand this general framework, then you can understand any hypothesis test. In a famous blog post, computer scientist Allen Downey called this the “There is only one test” framework, for which he created the flowchart displayed in Figure 16.14. FIGURE 16.14: Allen Downey’s hypothesis testing framework. Notice its similarity with the “hypothesis testing with infer” diagram you saw in Figure 16.9. That’s because the infer package was explicitly designed to match the “There is only one test” framework. So if you can understand the framework, you can easily generalize these ideas for all hypothesis testing scenarios. Whether for population proportions \\(p\\), population means \\(\\mu\\), differences in population proportions \\(p_1 - p_2\\), differences in population means \\(\\mu_1 - \\mu_2\\), and as you’ll see in Chapter 18 on inference for regression, population regression slopes \\(\\beta_1\\) as well. In fact, it applies more generally even than just these examples to more complicated hypothesis tests and test statistics as well. Learning check (LC9.4) Describe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between the promotion rate of males and females using this study. 16.4 Interpreting hypothesis tests Interpreting the results of hypothesis tests is one of the more challenging aspects of this method for statistical inference. In this section, we’ll focus on ways to help with deciphering the process and address some common misconceptions. 16.4.1 Two possible outcomes In Section 16.2, we mentioned that given a pre-specified significance level \\(\\alpha\\) there are two possible outcomes of a hypothesis test: If the \\(p\\)-value is less than \\(\\alpha\\), then we reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\). If the \\(p\\)-value is greater than or equal to \\(\\alpha\\), we fail to reject the null hypothesis \\(H_0\\). Unfortunately, the latter result is often misinterpreted as “accepting the null hypothesis \\(H_0\\).” While at first glance it may seem that the statements “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent, there actually is a subtle difference. Saying that we “accept the null hypothesis \\(H_0\\)” is equivalent to stating that “we think the null hypothesis \\(H_0\\) is true.” However, saying that we “fail to reject the null hypothesis \\(H_0\\)” is saying something else: “While \\(H_0\\) might still be false, we don’t have enough evidence to say so.” In other words, there is an absence of enough proof. However, the absence of proof is not proof of absence. To further shed light on this distinction, let’s use the United States criminal justice system as an analogy. A criminal trial in the United States is a similar situation to hypothesis tests whereby a choice between two contradictory claims must be made about a defendant who is on trial: The defendant is truly either “innocent” or “guilty.” The defendant is presumed “innocent until proven guilty.” The defendant is found guilty only if there is strong evidence that the defendant is guilty. The phrase “beyond a reasonable doubt” is often used as a guideline for determining a cutoff for when enough evidence exists to find the defendant guilty. The defendant is found to be either “not guilty” or “guilty” in the ultimate verdict. In other words, not guilty verdicts are not suggesting the defendant is innocent, but instead that “while the defendant may still actually be guilty, there wasn’t enough evidence to prove this fact.” Now let’s make the connection with hypothesis tests: Either the null hypothesis \\(H_0\\) or the alternative hypothesis \\(H_A\\) is true. Hypothesis tests are conducted assuming the null hypothesis \\(H_0\\) is true. We reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\) only if the evidence found in the sample suggests that \\(H_A\\) is true. The significance level \\(\\alpha\\) is used as a guideline to set the threshold on just how strong of evidence we require. We ultimately decide to either “fail to reject \\(H_0\\)” or “reject \\(H_0\\).” So while gut instinct may suggest “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent statements, they are not. “Accepting \\(H_0\\)” is equivalent to finding a defendant innocent. However, courts do not find defendants “innocent,” but rather they find them “not guilty.” Putting things differently, defense attorneys do not need to prove that their clients are innocent, rather they only need to prove that clients are not “guilty beyond a reasonable doubt”. So going back to our résumés activity in Section 16.3, recall that our hypothesis test was \\(H_0: p_{m} - p_{f} = 0\\) versus \\(H_A: p_{m} - p_{f} &gt; 0\\) and that we used a pre-specified significance level of \\(\\alpha\\) = 0.05. We found a \\(p\\)-value of 0.027. Since the \\(p\\)-value was smaller than \\(\\alpha\\) = 0.05, we rejected \\(H_0\\). In other words, we found needed levels of evidence in this particular sample to say that \\(H_0\\) is false at the \\(\\alpha\\) = 0.05 significance level. We also state this conclusion using non-statistical language: we found enough evidence in this data to suggest that there was gender discrimination at play. 16.4.2 Types of errors Unfortunately, there is some chance a jury or a judge can make an incorrect decision in a criminal trial by reaching the wrong verdict. For example, finding a truly innocent defendant “guilty”. Or on the other hand, finding a truly guilty defendant “not guilty.” This can often stem from the fact that prosecutors don’t have access to all the relevant evidence, but instead are limited to whatever evidence the police can find. The same holds for hypothesis tests. We can make incorrect decisions about a population parameter because we only have a sample of data from the population and thus sampling variation can lead us to incorrect conclusions. There are two possible erroneous conclusions in a criminal trial: either (1) a truly innocent person is found guilty or (2) a truly guilty person is found not guilty. Similarly, there are two possible errors in a hypothesis test: either (1) rejecting \\(H_0\\) when in fact \\(H_0\\) is true, called a Type I error or (2) failing to reject \\(H_0\\) when in fact \\(H_0\\) is false, called a Type II error. Another term used for “Type I error” is “false positive,” while another term for “Type II error” is “false negative.” This risk of error is the price researchers pay for basing inference on a sample instead of performing a census on the entire population. But as we’ve seen in our numerous examples and activities so far, censuses are often very expensive and other times impossible, and thus researchers have no choice but to use a sample. Thus in any hypothesis test based on a sample, we have no choice but to tolerate some chance that a Type I error will be made and some chance that a Type II error will occur. To help understand the concepts of Type I error and Type II errors, we apply these terms to our criminal justice analogy in Figure 16.15. FIGURE 16.15: Type I and Type II errors in criminal trials. Thus a Type I error corresponds to incorrectly putting a truly innocent person in jail, whereas a Type II error corresponds to letting a truly guilty person go free. Let’s show the corresponding table in Figure 16.16 for hypothesis tests. FIGURE 16.16: Type I and Type II errors in hypothesis tests. 16.4.3 How do we choose alpha? If we are using a sample to make inferences about a population, we run the risk of making errors. For confidence intervals, a corresponding “error” would be constructing a confidence interval that does not contain the true value of the population parameter. For hypothesis tests, this would be making either a Type I or Type II error. Obviously, we want to minimize the probability of either error; we want a small probability of making an incorrect conclusion: The probability of a Type I Error occurring is denoted by \\(\\alpha\\). The value of \\(\\alpha\\) is called the significance level of the hypothesis test, which we defined in Section 16.2. The probability of a Type II Error is denoted by \\(\\beta\\). The value of \\(1-\\beta\\) is known as the power of the hypothesis test. In other words, \\(\\alpha\\) corresponds to the probability of incorrectly rejecting \\(H_0\\) when in fact \\(H_0\\) is true. On the other hand, \\(\\beta\\) corresponds to the probability of incorrectly failing to reject \\(H_0\\) when in fact \\(H_0\\) is false. Ideally, we want \\(\\alpha = 0\\) and \\(\\beta = 0\\), meaning that the chance of making either error is 0. However, this can never be the case in any situation where we are sampling for inference. There will always be the possibility of making either error when we use sample data. Furthermore, these two error probabilities are inversely related. As the probability of a Type I error goes down, the probability of a Type II error goes up. What is typically done in practice is to fix the probability of a Type I error by pre-specifying a significance level \\(\\alpha\\) and then try to minimize \\(\\beta\\). In other words, we will tolerate a certain fraction of incorrect rejections of the null hypothesis \\(H_0\\), and then try to minimize the fraction of incorrect non-rejections of \\(H_0\\). So for example if we used \\(\\alpha\\) = 0.01, we would be using a hypothesis testing procedure that in the long run would incorrectly reject the null hypothesis \\(H_0\\) one percent of the time. This is analogous to setting the confidence level of a confidence interval. So what value should you use for \\(\\alpha\\)? Different fields have different conventions, but some commonly used values include 0.10, 0.05, 0.01, and 0.001. However, it is important to keep in mind that if you use a relatively small value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have a harder time being less than \\(\\alpha\\). Thus we would reject the null hypothesis less often. In other words, we would reject the null hypothesis \\(H_0\\) only if we have very strong evidence to do so. This is known as a “conservative” test. On the other hand, if we used a relatively large value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have an easier time being less than \\(\\alpha\\). Thus we would reject the null hypothesis more often. In other words, we would reject the null hypothesis \\(H_0\\) even if we only have mild evidence to do so. This is known as a “liberal” test. Learning check (LC9.5) What is wrong about saying, “The defendant is innocent.” based on the US system of criminal trials? (LC9.6) What is the purpose of hypothesis testing? (LC9.7) What are some flaws with hypothesis testing? How could we alleviate them? (LC9.8) Consider two \\(\\alpha\\) significance levels of 0.1 and 0.01. Of the two, which would lead to a more liberal hypothesis testing procedure? In other words, one that will, all things being equal, lead to more rejections of the null hypothesis \\(H_0\\). 16.5 Case study: Are action or romance movies rated higher? Let’s apply our knowledge of hypothesis testing to answer the question: “Are action or romance movies rated higher on IMDb?”. IMDb is a database on the internet providing information on movie and television show casts, plot summaries, trivia, and ratings. We’ll investigate if, on average, action or romance movies get higher ratings on IMDb. 16.5.1 IMDb ratings data The movies dataset in the ggplot2movies package contains information on 58,788 movies that have been rated by users of IMDb.com. movies # A tibble: 58,788 × 24 title year length budget rating votes r1 r2 r3 r4 r5 r6 &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 $ 1971 121 NA 6.4 348 4.5 4.5 4.5 4.5 14.5 24.5 2 $1000 a… 1939 71 NA 6 20 0 14.5 4.5 24.5 14.5 14.5 3 $21 a D… 1941 7 NA 8.2 5 0 0 0 0 0 24.5 4 $40,000 1996 70 NA 8.2 6 14.5 0 0 0 0 0 5 $50,000… 1975 71 NA 3.4 17 24.5 4.5 0 14.5 14.5 4.5 6 $pent 2000 91 NA 4.3 45 4.5 4.5 4.5 14.5 14.5 14.5 7 $windle 2002 93 NA 5.3 200 4.5 0 4.5 4.5 24.5 24.5 8 &#39;15&#39; 2002 25 NA 6.7 24 4.5 4.5 4.5 4.5 4.5 14.5 9 &#39;38 1987 97 NA 6.6 18 4.5 4.5 4.5 0 0 0 10 &#39;49-&#39;17 1917 61 NA 6 51 4.5 0 4.5 4.5 4.5 44.5 # ℹ 58,778 more rows # ℹ 12 more variables: r7 &lt;dbl&gt;, r8 &lt;dbl&gt;, r9 &lt;dbl&gt;, r10 &lt;dbl&gt;, mpaa &lt;chr&gt;, # Action &lt;int&gt;, Animation &lt;int&gt;, Comedy &lt;int&gt;, Drama &lt;int&gt;, # Documentary &lt;int&gt;, Romance &lt;int&gt;, Short &lt;int&gt; We’ll focus on a random sample of 68 movies that are classified as either “action” or “romance” movies but not both. We disregard movies that are classified as both so that we can assign all 68 movies into either category. Furthermore, since the original movies dataset was a little messy, we provide a pre-wrangled version of our data in the movies_sample data frame included in the moderndive package. If you’re curious, you can look at the necessary data wrangling code to do this on GitHub. movies_sample # A tibble: 68 × 4 title year rating genre &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; 1 Underworld 1985 3.1 Action 2 Love Affair 1932 6.3 Romance 3 Junglee 1961 6.8 Romance 4 Eversmile, New Jersey 1989 5 Romance 5 Search and Destroy 1979 4 Action 6 Secreto de Romelia, El 1988 4.9 Romance 7 Amants du Pont-Neuf, Les 1991 7.4 Romance 8 Illicit Dreams 1995 3.5 Action 9 Kabhi Kabhie 1976 7.7 Romance 10 Electric Horseman, The 1979 5.8 Romance # ℹ 58 more rows The variables include the title and year the movie was filmed. Furthermore, we have a numerical variable rating, which is the IMDb rating out of 10 stars, and a binary categorical variable genre indicating if the movie was an Action or Romance movie. We are interested in whether Action or Romance movies got a higher rating on average. Let’s perform an exploratory data analysis of this data. Recall from Subsection 12.7.1 that a boxplot is a visualization we can use to show the relationship between a numerical and a categorical variable. Another option you saw in Section 12.6 would be to use a faceted histogram. However, in the interest of brevity, let’s only present the boxplot in Figure 16.17. ggplot(data = movies_sample, aes(x = genre, y = rating)) + geom_boxplot() + labs(y = &quot;IMDb rating&quot;) FIGURE 16.17: Boxplot of IMDb rating vs. genre. Eyeballing Figure 16.17, romance movies have a higher median rating. Do we have reason to believe, however, that there is a significant difference between the mean rating for action movies compared to romance movies? It’s hard to say just based on this plot. The boxplot does show that the median sample rating is higher for romance movies. However, there is a large amount of overlap between the boxes. Recall that the median isn’t necessarily the same as the mean either, depending on whether the distribution is skewed. Let’s calculate some summary statistics split by the binary categorical variable genre: the number of movies, the mean rating, and the standard deviation split by genre. We’ll do this using dplyr data wrangling verbs. Notice in particular how we count the number of each type of movie using the n() summary function. movies_sample %&gt;% group_by(genre) %&gt;% summarize(n = n(), mean_rating = mean(rating), std_dev = sd(rating)) # A tibble: 2 × 4 genre n mean_rating std_dev &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Action 32 5.275 1.36121 2 Romance 36 6.32222 1.60963 Observe that we have 36 movies with an average rating of 6.322 stars and 32 movies with an average rating of 5.275 stars. The difference in these average ratings is thus 6.322 - 5.275 = 1.047. So there appears to be an edge of 1.047 stars in favor of romance movies. The question is, however, are these results indicative of a true difference for all romance and action movies? Or could we attribute this difference to chance sampling variation? 16.5.2 Sampling scenario Let’s now revisit this study in terms of terminology and notation related to sampling we studied in Subsection 14.3.1. The study population is all movies in the IMDb database that are either action or romance (but not both). The sample from this population is the 68 movies included in the movies_sample dataset. Since this sample was randomly taken from the population movies, it is representative of all romance and action movies on IMDb. Thus, any analysis and results based on movies_sample can generalize to the entire population. What are the relevant population parameter and point estimates? We introduce the fourth sampling scenario in Table 16.3. TABLE 16.3: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) or \\(\\widehat{\\mu}_1 - \\widehat{\\mu}_2\\) So, whereas the sampling bowl exercise in Section 14.1 concerned proportions, the pennies exercise in Section 15.1 concerned means, the case study on whether yawning is contagious in Section 15.6 and the promotions activity in Section 16.1 concerned differences in proportions, we are now concerned with differences in means. In other words, the population parameter of interest is the difference in population mean ratings \\(\\mu_a - \\mu_r\\), where \\(\\mu_a\\) is the mean rating of all action movies on IMDb and similarly \\(\\mu_r\\) is the mean rating of all romance movies. Additionally the point estimate/sample statistic of interest is the difference in sample means \\(\\overline{x}_a - \\overline{x}_r\\), where \\(\\overline{x}_a\\) is the mean rating of the \\(n_a\\) = 32 movies in our sample and \\(\\overline{x}_r\\) is the mean rating of the \\(n_r\\) = 36 in our sample. Based on our earlier exploratory data analysis, our estimate \\(\\overline{x}_a - \\overline{x}_r\\) is \\(5.275 - 6.322 = -1.047\\). So there appears to be a slight difference of -1.047 in favor of romance movies. The question is, however, could this difference of -1.047 be merely due to chance and sampling variation? Or are these results indicative of a true difference in mean ratings for all romance and action movies on IMDb? To answer this question, we’ll use hypothesis testing. 16.5.3 Conducting the hypothesis test We’ll be testing: \\[ \\begin{aligned} H_0 &amp;: \\mu_a - \\mu_r = 0\\\\ \\text{vs } H_A&amp;: \\mu_a - \\mu_r \\neq 0 \\end{aligned} \\] In other words, the null hypothesis \\(H_0\\) suggests that both romance and action movies have the same mean rating. This is the “hypothesized universe” we’ll assume is true. On the other hand, the alternative hypothesis \\(H_A\\) suggests that there is a difference. Unlike the one-sided alternative we used in the promotions exercise \\(H_A: p_m - p_f &gt; 0\\), we are now considering a two-sided alternative of \\(H_A: \\mu_a - \\mu_r \\neq 0\\). Furthermore, we’ll pre-specify a low significance level of \\(\\alpha\\) = 0.001. By setting this value low, all things being equal, there is a lower chance that the \\(p\\)-value will be less than \\(\\alpha\\). Thus, there is a lower chance that we’ll reject the null hypothesis \\(H_0\\) in favor of the alternative hypothesis \\(H_A\\). In other words, we’ll reject the hypothesis that there is no difference in mean ratings for all action and romance movies, only if we have quite strong evidence. This is known as a “conservative” hypothesis testing procedure. 1. specify variables Let’s now perform all the steps of the infer workflow. We first specify() the variables of interest in the movies_sample data frame using the formula rating ~ genre. This tells infer that the numerical variable rating is the outcome variable, while the binary variable genre is the explanatory variable. Note that unlike previously when we were interested in proportions, since we are now interested in the mean of a numerical variable, we do not need to set the success argument. movies_sample %&gt;% specify(formula = rating ~ genre) Response: rating (numeric) Explanatory: genre (factor) # A tibble: 68 × 2 rating genre &lt;dbl&gt; &lt;fct&gt; 1 3.1 Action 2 6.3 Romance 3 6.8 Romance 4 5 Romance 5 4 Action 6 4.9 Romance 7 7.4 Romance 8 3.5 Action 9 7.7 Romance 10 5.8 Romance # ℹ 58 more rows Observe at this point that the data in movies_sample has not changed. The only change so far is the newly defined Response: rating (numeric) and Explanatory: genre (factor) meta-data. 2. hypothesize the null We set the null hypothesis \\(H_0: \\mu_a - \\mu_r = 0\\) by using the hypothesize() function. Since we have two samples, action and romance movies, we set null to be \"independence\" as we described in Section 16.3. movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% hypothesize(null = &quot;independence&quot;) Response: rating (numeric) Explanatory: genre (factor) Null Hypothesis: independence # A tibble: 68 × 2 rating genre &lt;dbl&gt; &lt;fct&gt; 1 3.1 Action 2 6.3 Romance 3 6.8 Romance 4 5 Romance 5 4 Action 6 4.9 Romance 7 7.4 Romance 8 3.5 Action 9 7.7 Romance 10 5.8 Romance # ℹ 58 more rows 3. generate replicates After we have set the null hypothesis, we generate “shuffled” replicates assuming the null hypothesis is true by repeating the shuffling/permutation exercise you performed in Section 16.1. We’ll repeat this resampling without replacement of type = \"permute\" a total of reps = 1000 times. Feel free to run the code below to check out what the generate() step produces. movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% View() 4. calculate summary statistics Now that we have 1000 replicated “shuffles” assuming the null hypothesis \\(H_0\\) that both Action and Romance movies on average have the same ratings on IMDb, let’s calculate() the appropriate summary statistic for these 1000 replicated shuffles. From Section 16.2, summary statistics relating to hypothesis testing have a specific name: test statistics. Since the unknown population parameter of interest is the difference in population means \\(\\mu_{a} - \\mu_{r}\\), the test statistic of interest here is the difference in sample means \\(\\overline{x}_{a} - \\overline{x}_{r}\\). For each of our 1000 shuffles, we can calculate this test statistic by setting stat = \"diff in means\". Furthermore, since we are interested in \\(\\overline{x}_{a} - \\overline{x}_{r}\\), we set order = c(\"Action\", \"Romance\"). Let’s save the results in a data frame called null_distribution_movies: null_distribution_movies &lt;- movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Action&quot;, &quot;Romance&quot;)) null_distribution_movies # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.511111 2 2 0.345833 3 3 -0.327083 4 4 -0.209028 5 5 -0.433333 6 6 -0.102778 7 7 0.387153 8 8 0.168750 9 9 0.257292 10 10 0.334028 # ℹ 990 more rows Observe that we have 1000 values of stat, each representing one instance of \\(\\overline{x}_{a} - \\overline{x}_{r}\\). The 1000 values form the null distribution, which is the technical term for the sampling distribution of the difference in sample means \\(\\overline{x}_{a} - \\overline{x}_{r}\\) assuming \\(H_0\\) is true. What happened in real life? What was the observed difference in promotion rates? What was the observed test statistic \\(\\overline{x}_{a} - \\overline{x}_{r}\\)? Recall from our earlier data wrangling, this observed difference in means was \\(5.275 - 6.322 = -1.047\\). We can also achieve this using the code that constructed the null distribution null_distribution_movies but with the hypothesize() and generate() steps removed. Let’s save this in obs_diff_means: obs_diff_means &lt;- movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Action&quot;, &quot;Romance&quot;)) obs_diff_means Response: rating (numeric) Explanatory: genre (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 -1.04722 5. visualize the p-value Lastly, in order to compute the \\(p\\)-value, we have to assess how “extreme” the observed difference in means of -1.047 is. We do this by comparing -1.047 to our null distribution, which was constructed in a hypothesized universe of no true difference in movie ratings. Let’s visualize both the null distribution and the \\(p\\)-value in Figure 16.18. Unlike our example in Subsection 16.3.1 involving promotions, since we have a two-sided \\(H_A: \\mu_a - \\mu_r \\neq 0\\), we have to allow for both possibilities for more extreme, so we set direction = \"both\". visualize(null_distribution_movies, bins = 10) + shade_p_value(obs_stat = obs_diff_means, direction = &quot;both&quot;) FIGURE 16.18: Null distribution, observed test statistic, and \\(p\\)-value. Let’s go over the elements of this plot. First, the histogram is the null distribution. Second, the solid line is the observed test statistic, or the difference in sample means we observed in real life of \\(5.275 - 6.322 = -1.047\\). Third, the two shaded areas of the histogram form the \\(p\\)-value, or the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. What proportion of the null distribution is shaded? In other words, what is the numerical value of the \\(p\\)-value? We use the get_p_value() function to compute this value: null_distribution_movies %&gt;% get_p_value(obs_stat = obs_diff_means, direction = &quot;both&quot;) # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0.004 This \\(p\\)-value of 0.004 is very small. In other words, there is a very small chance that we’d observe a difference of 5.275 - 6.322 = -1.047 in a hypothesized universe where there was truly no difference in ratings. But this \\(p\\)-value is larger than our (even smaller) pre-specified \\(\\alpha\\) significance level of 0.001. Thus, we are inclined to fail to reject the null hypothesis \\(H_0: \\mu_a - \\mu_r = 0\\). In non-statistical language, the conclusion is: we do not have the evidence needed in this sample of data to suggest that we should reject the hypothesis that there is no difference in mean IMDb ratings between romance and action movies. We, thus, cannot say that a difference exists in romance and action movie ratings, on average, for all IMDb movies. Learning check (LC9.9) Conduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating. What was different and what was the same? (LC9.10) What conclusions can you make from viewing the faceted histogram looking at rating versus genre that you couldn’t see when looking at the boxplot? (LC9.11) Describe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies. (LC9.12) Why are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres? (LC9.13) Using the definition of \\(p\\)-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the mean rating of romance to action movies. (LC9.14) What is the value of the \\(p\\)-value for the hypothesis test comparing the mean rating of romance to action movies? (LC9.15) Test your data wrangling knowledge and EDA skills: Use dplyr and tidyr to create the necessary data frame focused on only action and romance movies (but not both) from the movies data frame in the ggplot2movies package. Make a boxplot and a faceted histogram of this population data comparing ratings of action and romance movies from IMDb. Discuss how these plots compare to the similar plots produced for the movies_sample data. 16.6 Conclusion 16.6.1 Theory-based hypothesis tests Much as we did in Subsections 14.6.2 and 15.7.2 when we showed you theory-based methods for compututing standard errors and constructing confidence intervals that involved mathematical formulas, we now present an example of a traditional theory-based method to conduct hypothesis tests. This method relies on probability models, probability distributions, and a few assumptions to construct the null distribution. This is in contrast to the approach we’ve been using throughout this book where we relied on computer simulations to construct the null distribution. These traditional theory-based methods have been used for decades mostly because researchers didn’t have access to computers that could run thousands of calculations quickly and efficiently. Now that computing power is much cheaper and more accessible, simulation-based methods are much more feasible. However, researchers in many fields continue to use theory-based methods. Hence, we make it a point to include an example here. As we’ll show in this section, any theory-based method is ultimately an approximation to the simulation-based method. The theory-based method we’ll focus on is known as the two-sample \\(t\\)-test for testing differences in sample means. However, the test statistic we’ll use won’t be the difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\), but rather the related two-sample \\(t\\)-statistic. The data we’ll use will once again be the movies_sample data of action and romance movies from Section 16.5. Two-sample t-statistic A common task in statistics is the process of “standardizing a variable.” By standardizing different variables, we make them more comparable. For example, say you are interested in studying the distribution of temperature recordings from Portland, Oregon, USA and comparing it to that of the temperature recordings in Montreal, Quebec, Canada. Given that US temperatures are generally recorded in degrees Fahrenheit and Canadian temperatures are generally recorded in degrees Celsius, how can we make them comparable? One approach would be to convert degrees Fahrenheit into Celsius, or vice versa. Another approach would be to convert them both to a common “standardized” scale, like Kelvin units of temperature. One common method for standardizing a variable from probability and statistics theory is to compute the \\(z\\)-score: \\[z = \\frac{x - \\mu}{\\sigma}\\] where \\(x\\) represents one value of a variable, \\(\\mu\\) represents the mean of that variable, and \\(\\sigma\\) represents the standard deviation of that variable. You first subtract the mean \\(\\mu\\) from each value of \\(x\\) and then divide \\(x - \\mu\\) by the standard deviation \\(\\sigma\\). These operations will have the effect of re-centering your variable around 0 and re-scaling your variable \\(x\\) so that they have what are known as “standard units.” Thus for every value that your variable can take, it has a corresponding \\(z\\)-score that gives how many standard units away that value is from the mean \\(\\mu\\). \\(z\\)-scores are normally distributed with mean 0 and standard deviation 1. This curve is called a “\\(z\\)-distribution” or “standard normal” curve and has the common, bell-shaped pattern from Figure 16.19 discussed in Appendix ??. FIGURE 16.19: Standard normal z curve. Bringing these back to the difference of sample mean ratings \\(\\overline{x}_a - \\overline{x}_r\\) of action versus romance movies, how would we standardize this variable? By once again subtracting its mean and dividing by its standard deviation. Recall two facts from Subsection 14.3.3. First, if the sampling was done in a representative fashion, then the sampling distribution of \\(\\overline{x}_a - \\overline{x}_r\\) will be centered at the true population parameter \\(\\mu_a - \\mu_r\\). Second, the standard deviation of point estimates like \\(\\overline{x}_a - \\overline{x}_r\\) has a special name: the standard error. Applying these ideas, we present the two-sample \\(t\\)-statistic: \\[t = \\dfrac{ (\\bar{x}_a - \\bar{x}_r) - (\\mu_a - \\mu_r)}{ \\text{SE}_{\\bar{x}_a - \\bar{x}_r} } = \\dfrac{ (\\bar{x}_a - \\bar{x}_r) - (\\mu_a - \\mu_r)}{ \\sqrt{\\dfrac{{s_a}^2}{n_a} + \\dfrac{{s_r}^2}{n_r}} }\\] Oofda! There is a lot to try to unpack here! Let’s go slowly. In the numerator, \\(\\bar{x}_a-\\bar{x}_r\\) is the difference in sample means, while \\(\\mu_a - \\mu_r\\) is the difference in population means. In the denominator, \\(s_a\\) and \\(s_r\\) are the sample standard deviations of the action and romance movies in our sample movies_sample. Lastly, \\(n_a\\) and \\(n_r\\) are the sample sizes of the action and romance movies. Putting this together under the square root gives us the standard error \\(\\text{SE}_{\\bar{x}_a - \\bar{x}_r}\\). Observe that the formula for \\(\\text{SE}_{\\bar{x}_a - \\bar{x}_r}\\) has the sample sizes \\(n_a\\) and \\(n_r\\) in them. So as the sample sizes increase, the standard error goes down. We’ve seen this concept numerous times now, in particular (1) in our simulations using the three virtual shovels with \\(n\\) = 25, 50, and 100 slots in Figure 14.15, (2) in Subsection 15.5.3 where we studied the effect of using larger sample sizes on the widths of confidence intervals, and (3) in Subsection 14.6.2 where we studied the formula-based approximation to the standard error of the sample proportion \\(\\widehat{p}\\). So how can we use the two-sample \\(t\\)-statistic as a test statistic in our hypothesis test? First, assuming the null hypothesis \\(H_0: \\mu_a - \\mu_r = 0\\) is true, the right-hand side of the numerator (to the right of the \\(-\\) sign), \\(\\mu_a - \\mu_r\\), becomes 0. Second, similarly to how the Central Limit Theorem from Subsection 14.5 states that sample means follow a normal distribution, it can be mathematically proven that the two-sample \\(t\\)-statistic follows a \\(t\\) distribution with degrees of freedom “roughly equal” to \\(df = n_a + n_r - 2\\). To better understand this concept of degrees of freedom, we next display three examples of \\(t\\)-distributions in Figure 16.20 along with the standard normal \\(z\\) curve. FIGURE 16.20: Examples of t-distributions and the z curve. Begin by looking at the center of the plot at 0 on the horizontal axis. As you move up from the value of 0, follow along with the labels and note that the bottom curve corresponds to 1 degree of freedom, the curve above it is for 3 degrees of freedom, the curve above that is for 10 degrees of freedom, and lastly the dotted curve is the standard normal \\(z\\) curve. Observe that all four curves have a bell shape, are centered at 0, and that as the degrees of freedom increase, the \\(t\\)-distribution more and more resembles the standard normal \\(z\\) curve. The “degrees of freedom” measures how different the \\(t\\) distribution will be from a normal distribution. \\(t\\)-distributions tend to have more values in the tails of their distributions than the standard normal \\(z\\) curve. This “roughly equal” statement indicates that the equation \\(df = n_a + n_r - 2\\) is a “good enough” approximation to the true degrees of freedom. The true formula is a bit more complicated than this simple expression, but we’ve found the formula to be beyond the reach of those new to statistical inference and it does little to build the intuition of the \\(t\\)-test. The message to retain, however, is that small sample sizes lead to small degrees of freedom and thus small sample sizes lead to \\(t\\)-distributions that are different than the \\(z\\) curve. On the other hand, large sample sizes correspond to large degrees of freedom and thus produce \\(t\\) distributions that closely align with the standard normal \\(z\\)-curve. So, assuming the null hypothesis \\(H_0\\) is true, our formula for the test statistic simplifies a bit: \\[t = \\dfrac{ (\\bar{x}_a - \\bar{x}_r) - 0}{ \\sqrt{\\dfrac{{s_a}^2}{n_a} + \\dfrac{{s_r}^2}{n_r}} } = \\dfrac{ \\bar{x}_a - \\bar{x}_r}{ \\sqrt{\\dfrac{{s_a}^2}{n_a} + \\dfrac{{s_r}^2}{n_r}} }\\] Let’s compute the values necessary for this two-sample \\(t\\)-statistic. Recall the summary statistics we computed during our exploratory data analysis in Section 16.5.1. movies_sample %&gt;% group_by(genre) %&gt;% summarize(n = n(), mean_rating = mean(rating), std_dev = sd(rating)) # A tibble: 2 × 4 genre n mean_rating std_dev &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Action 32 5.275 1.36121 2 Romance 36 6.32222 1.60963 Using these values, the observed two-sample \\(t\\)-test statistic is \\[ \\dfrac{ \\bar{x}_a - \\bar{x}_r}{ \\sqrt{\\dfrac{{s_a}^2}{n_a} + \\dfrac{{s_r}^2}{n_r}} } = \\dfrac{5.28 - 6.32}{ \\sqrt{\\dfrac{{1.36}^2}{32} + \\dfrac{{1.61}^2}{36}} } = -2.906 \\] Great! How can we compute the \\(p\\)-value using this theory-based test statistic? We need to compare it to a null distribution, which we construct next. Null distribution Let’s revisit the null distribution for the test statistic \\(\\bar{x}_a - \\bar{x}_r\\) we constructed in Section 16.5. Let’s visualize this in the left-hand plot of Figure 16.21. # Construct null distribution of xbar_a - xbar_r: null_distribution_movies &lt;- movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Action&quot;, &quot;Romance&quot;)) visualize(null_distribution_movies, bins = 10) The infer package also includes some built-in theory-based test statistics as well. So instead of calculating the test statistic of interest as the \"diff in means\" \\(\\bar{x}_a - \\bar{x}_r\\), we can calculate this defined two-sample \\(t\\)-statistic by setting stat = \"t\". Let’s visualize this in the right-hand plot of Figure 16.21. # Construct null distribution of t: null_distribution_movies_t &lt;- movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% # Notice we switched stat from &quot;diff in means&quot; to &quot;t&quot; calculate(stat = &quot;t&quot;, order = c(&quot;Action&quot;, &quot;Romance&quot;)) visualize(null_distribution_movies_t, bins = 10) FIGURE 16.21: Comparing the null distributions of two test statistics. Observe that while the shape of the null distributions of both the difference in means \\(\\bar{x}_a - \\bar{x}_r\\) and the two-sample \\(t\\)-statistics are similar, the scales on the x-axis are different. The two-sample \\(t\\)-statistic values are spread out over a larger range. However, a traditional theory-based \\(t\\)-test doesn’t look at the simulated histogram in null_distribution_movies_t, but instead it looks at the \\(t\\)-distribution curve with degrees of freedom equal to roughly 65.85. This calculation is based on the complicated formula referenced previously, which we approximated with \\(df = n_a + n_r - 2 = 32 + 36 - 2 = 66\\). Let’s overlay this \\(t\\)-distribution curve over the top of our simulated two-sample \\(t\\)-statistics using the method = \"both\" argument in visualize(). visualize(null_distribution_movies_t, bins = 10, method = &quot;both&quot;) FIGURE 16.22: Null distribution using t-statistic and t-distribution. Observe that the curve does a good job of approximating the histogram here. To calculate the \\(p\\)-value in this case, we need to figure out how much of the total area under the \\(t\\)-distribution curve is at or “more extreme” than our observed two-sample \\(t\\)-statistic. Since \\(H_A: \\mu_a - \\mu_r \\neq 0\\) is a two-sided alternative, we need to add up the areas in both tails. We first compute the observed two-sample \\(t\\)-statistic using infer verbs. This shortcut calculation further assumes that the null hypothesis is true: that the population of action and romance movies have an equal average rating. obs_two_sample_t &lt;- movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% calculate(stat = &quot;t&quot;, order = c(&quot;Action&quot;, &quot;Romance&quot;)) obs_two_sample_t Response: rating (numeric) Explanatory: genre (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 -2.90589 We want to find the percentage of values that are at or below obs_two_sample_t \\(= -2.906\\) or at or above -obs_two_sample_t \\(= 2.906\\). We use the shade_p_value() function with the direction argument set to \"both\" to do this: visualize(null_distribution_movies_t, method = &quot;both&quot;) + shade_p_value(obs_stat = obs_two_sample_t, direction = &quot;both&quot;) Warning: Check to make sure the conditions have been met for the theoretical method. {infer} currently does not check these for you. FIGURE 16.23: Null distribution using t-statistic and t-distribution with \\(p\\)-value shaded. (We’ll discuss this warning message shortly.) What is the \\(p\\)-value? We apply get_p_value() to our null distribution saved in null_distribution_movies_t: null_distribution_movies_t %&gt;% get_p_value(obs_stat = obs_two_sample_t, direction = &quot;both&quot;) # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0.002 We have a very small \\(p\\)-value, and thus it is very unlikely that these results are due to sampling variation. Thus, we are inclined to reject \\(H_0\\). Let’s come back to that earlier warning message: Check to make sure the conditions have been met for the theoretical method. {infer} currently does not check these for you. To be able to use the \\(t\\)-test and other such theoretical methods, there are always a few conditions to check. The infer package does not automatically check these conditions, hence the warning message we received. These conditions are necessary so that the underlying mathematical theory holds. In order for the results of our two-sample \\(t\\)-test to be valid, three conditions must be met: Nearly normal populations or large sample sizes. A general rule of thumb that works in many (but not all) situations is that the sample size \\(n\\) should be greater than 30. Both samples are selected independently of each other. All observations are independent from each other. Let’s see if these conditions hold for our movies_sample data: This is met since \\(n_a\\) = 32 and \\(n_r\\) = 36 are both larger than 30, satisfying our rule of thumb. This is met since we sampled the action and romance movies at random and in an unbiased fashion from the database of all IMDb movies. Unfortunately, we don’t know how IMDb computes the ratings. For example, if the same person rated multiple movies, then those observations would be related and hence not independent. Assuming all three conditions are roughly met, we can be reasonably certain that the theory-based \\(t\\)-test results are valid. If any of the conditions were clearly not met, we couldn’t put as much trust into any conclusions reached. On the other hand, in most scenarios, the only assumption that needs to be met in the simulation-based method is that the sample is selected at random. Thus, in our experience, we prefer simulation-based methods as they have fewer assumptions, are conceptually easier to understand, and since computing power has recently become easily accessible, they can be run quickly. That being said since much of the world’s research still relies on traditional theory-based methods, we also believe it is important to understand them. You may be wondering why we chose reps = 1000 for these simulation-based methods. We’ve noticed that after around 1000 replicates for the null distribution and the bootstrap distribution for most problems you can start to get a general sense for how the statistic behaves. You can change this value to something like 10,000 though for reps if you would like even finer detail but this will take more time to compute. Feel free to iterate on this as you like to get an even better idea about the shape of the null and bootstrap distributions as you wish. 16.6.2 When inference is not needed We’ve now walked through several different examples of how to use the infer package to perform statistical inference: constructing confidence intervals and conducting hypothesis tests. For each of these examples, we made it a point to always perform an exploratory data analysis (EDA) first; specifically, by looking at the raw data values, by using data visualization with ggplot2, and by data wrangling with dplyr beforehand. We highly encourage you to always do the same. As a beginner to statistics, EDA helps you develop intuition as to what statistical methods like confidence intervals and hypothesis tests can tell us. Even as a seasoned practitioner of statistics, EDA helps guide your statistical investigations. In particular, is statistical inference even needed? Let’s consider an example. Say we’re interested in the following question: Of all flights leaving a New York City airport, are Hawaiian Airlines flights in the air for longer than Alaska Airlines flights? Furthermore, let’s assume that 2013 flights are a representative sample of all such flights. Then we can use the flights data frame in the nycflights13 package we introduced in Section ?? to answer our question. Let’s filter this data frame to only include Hawaiian and Alaska Airlines using their carrier codes HA and AS: flights_sample &lt;- flights %&gt;% filter(carrier %in% c(&quot;HA&quot;, &quot;AS&quot;)) There are two possible statistical inference methods we could use to answer such questions. First, we could construct a 95% confidence interval for the difference in population means \\(\\mu_{HA} - \\mu_{AS}\\), where \\(\\mu_{HA}\\) is the mean air time of all Hawaiian Airlines flights and \\(\\mu_{AS}\\) is the mean air time of all Alaska Airlines flights. We could then check if the entirety of the interval is greater than 0, suggesting that \\(\\mu_{HA} - \\mu_{AS} &gt; 0\\), or, in other words suggesting that \\(\\mu_{HA} &gt; \\mu_{AS}\\). Second, we could perform a hypothesis test of the null hypothesis \\(H_0: \\mu_{HA} - \\mu_{AS} = 0\\) versus the alternative hypothesis \\(H_A: \\mu_{HA} - \\mu_{AS} &gt; 0\\). However, let’s first construct an exploratory visualization as we suggested earlier. Since air_time is numerical and carrier is categorical, a boxplot can display the relationship between these two variables, which we display in Figure 16.24. ggplot(data = flights_sample, mapping = aes(x = carrier, y = air_time)) + geom_boxplot() + labs(x = &quot;Carrier&quot;, y = &quot;Air Time&quot;) FIGURE 16.24: Air time for Hawaiian and Alaska Airlines flights departing NYC in 2013. This is what we like to call “no PhD in Statistics needed” moments. You don’t have to be an expert in statistics to know that Alaska Airlines and Hawaiian Airlines have significantly different air times. The two boxplots don’t even overlap! Constructing a confidence interval or conducting a hypothesis test would frankly not provide much more insight than Figure 16.24. Let’s investigate why we observe such a clear cut difference between these two airlines using data wrangling. Let’s first group by the rows of flights_sample not only by carrier but also by destination dest. Subsequently, we’ll compute two summary statistics: the number of observations using n() and the mean airtime: flights_sample %&gt;% group_by(carrier, dest) %&gt;% summarize(n = n(), mean_time = mean(air_time, na.rm = TRUE)) # A tibble: 2 × 4 # Groups: carrier [2] carrier dest n mean_time &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 AS SEA 714 325.618 2 HA HNL 342 623.088 It turns out that from New York City in 2013, Alaska only flew to SEA (Seattle) from New York City (NYC) while Hawaiian only flew to HNL (Honolulu) from NYC. Given the clear difference in distance from New York City to Seattle versus New York City to Honolulu, it is not surprising that we observe such different (statistically significantly different, in fact) air times in flights. This is a clear example of not needing to do anything more than a simple exploratory data analysis using data visualization and descriptive statistics to get an appropriate conclusion. This is why we highly recommend you perform an EDA of any sample data before running statistical inference methods like confidence intervals and hypothesis tests. 16.6.3 Problems with p-values On top of the many common misunderstandings about hypothesis testing and \\(p\\)-values we listed in Section 16.4, another unfortunate consequence of the expanded use of \\(p\\)-values and hypothesis testing is a phenomenon known as “p-hacking.” p-hacking is the act of “cherry-picking” only results that are “statistically significant” while dismissing those that aren’t, even if at the expense of the scientific ideas. There are lots of articles written recently about misunderstandings and the problems with \\(p\\)-values. We encourage you to check some of them out: Misunderstandings of \\(p\\)-values What a nerdy debate about \\(p\\)-values shows about science - and how to fix it Statisticians issue warning over misuse of \\(P\\) values You Can’t Trust What You Read About Nutrition A Litany of Problems with p-values Such issues were getting so problematic that the American Statistical Association (ASA) put out a statement in 2016 titled, “The ASA Statement on Statistical Significance and \\(P\\)-Values,” with six principles underlying the proper use and interpretation of \\(p\\)-values. The ASA released this guidance on \\(p\\)-values to improve the conduct and interpretation of quantitative science and to inform the growing emphasis on reproducibility of science research. We as authors much prefer the use of confidence intervals for statistical inference, since in our opinion they are much less prone to large misinterpretation. However, many fields still exclusively use \\(p\\)-values for statistical inference and this is one reason for including them in this text. We encourage you to learn more about “p-hacking” as well and its implication for science. 16.6.4 Additional resources An R script file of all R code used in this chapter is available here. If you want more examples of the infer workflow for conducting hypothesis tests, we suggest you check out the infer package homepage, in particular, a series of example analyses available at https://infer.netlify.app/articles/. 16.6.5 What’s to come We conclude with the infer pipeline for hypothesis testing in Figure 16.25. FIGURE 16.25: infer package workflow for hypothesis testing. Now that we’ve armed ourselves with an understanding of confidence intervals from Chapter 15 and hypothesis tests from this chapter, we’ll now study inference for regression in the upcoming Chapter 18. We’ll revisit the regression models we studied in Chapter 17 on basic regression and Chapter 19 on multiple regression. For example, recall Table 17.2 (shown again here in Table 16.4), corresponding to our regression model for an instructor’s teaching score as a function of their “beauty” score. # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals) # Get regression table: get_regression_table(score_model) TABLE 16.4: Linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 We previously saw in Subsection 17.1.2 that the values in the estimate column are the fitted intercept \\(b_0\\) and fitted slope for “beauty” score \\(b_1\\). In Chapter 18, we’ll unpack the remaining columns: std_error which is the standard error, statistic which is the observed standardized test statistic to compute the p_value, and the 95% confidence intervals as given by lower_ci and upper_ci. "],["17-regression.html", "Chapter 17 Basic Regression 17.1 One numerical explanatory variable 17.2 One categorical explanatory variable 17.3 Related topics 17.4 Conclusion", " Chapter 17 Basic Regression Now that we are equipped with data visualization skills from Chapter 12, data wrangling skills from Chapter 13, and an understanding of how to import data and the concept of a “tidy” data format from Chapter ??, let’s now proceed with data modeling. The fundamental premise of data modeling is to make explicit the relationship between: an outcome variable \\(y\\), also called a dependent variable or response variable, and an explanatory/predictor variable \\(x\\), also called an independent variable or covariate. Another way to state this is using mathematical terminology: we will model the outcome variable \\(y\\) “as a function” of the explanatory/predictor variable \\(x\\). When we say “function” here, we aren’t referring to functions in R like the ggplot() function, but rather as a mathematical function. But, why do we have two different labels, explanatory and predictor, for the variable \\(x\\)? That’s because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes: Modeling for explanation: When you want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(x\\), determine the significance of any relationships, have measures summarizing these relationships, and possibly identify any causal relationships between the variables. Modeling for prediction: When you want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(x\\). Unlike modeling for explanation, however, you don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about \\(y\\) using the information in \\(x\\). For example, say you are interested in an outcome variable \\(y\\) of whether patients develop lung cancer and information \\(x\\) on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as targeting smokers of a specific age group with advertising for smoking cessation programs. If we are modeling for prediction, however, we wouldn’t care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer. In this book, we’ll focus on modeling for explanation and hence refer to \\(x\\) as explanatory variables. If you are interested in learning about modeling for prediction, we suggest you check out books and courses on the field of machine learning such as An Introduction to Statistical Learning with Applications in R (ISLR) (James et al. 2017). Furthermore, while there exist many techniques for modeling, such as tree-based models and neural networks, in this book we’ll focus on one particular technique: linear regression. Linear regression is one of the most commonly-used and easy-to-understand approaches to modeling. Linear regression involves a numerical outcome variable \\(y\\) and explanatory variables \\(x\\) that are either numerical or categorical. Furthermore, the relationship between \\(y\\) and \\(x\\) is assumed to be linear, or in other words, a line. However, we’ll see that what constitutes a “line” will vary depending on the nature of your explanatory variables \\(x\\). In Chapter 17 on basic regression, we’ll only consider models with a single explanatory variable \\(x\\). In Section 17.1, the explanatory variable will be numerical. This scenario is known as simple linear regression. In Section 17.2, the explanatory variable will be categorical. In Chapter 19 on multiple regression, we’ll extend the ideas behind basic regression and consider models with two explanatory variables \\(x_1\\) and \\(x_2\\). In Section 19.1, we’ll have two numerical explanatory variables. In Section 19.2, we’ll have one numerical and one categorical explanatory variable. In particular, we’ll consider two such models: interaction and parallel slopes models. In Chapter 18 on inference for regression, we’ll revisit our regression models and analyze the results using the tools for statistical inference you’ll develop in Chapters 14, 15, and 16 on sampling, bootstrapping and confidence intervals, and hypothesis testing and \\(p\\)-values, respectively. Let’s now begin with basic regression, which refers to linear regression models with a single explanatory variable \\(x\\). We’ll also discuss important statistical concepts like the correlation coefficient, that “correlation isn’t necessarily causation,” and what it means for a line to be “best-fitting.” Needed packages Let’s now load all the packages needed for this chapter (this assumes you’ve already installed them). In this chapter, we introduce some new packages: The tidyverse “umbrella” (Wickham 2023) package. Recall from our discussion in Section ?? that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages The moderndive package of datasets and functions for tidyverse-friendly introductory linear regression. The skimr (Quinn et al. 2019) package, which provides a simple-to-use function to quickly compute a wide array of commonly used summary statistics. If needed, read Section ?? for information on how to install and load R packages. library(tidyverse) library(moderndive) library(skimr) library(gapminder) 17.1 One numerical explanatory variable Why do some professors and instructors at universities and colleges receive high teaching evaluations scores from students while others receive lower ones? Are there differences in teaching evaluations between instructors of different demographic groups? Could there be an impact due to student biases? These are all questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which instructors and professors get promoted. Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer the following research question: what factors explain differences in instructor teaching evaluation scores? To this end, they collected instructor and course information on 463 courses. A full description of the study can be found at openintro.org. In this section, we’ll keep things simple for now and try to explain differences in instructor teaching scores as a function of one numerical variable: the instructor’s “beauty” score (we’ll describe how this score was determined shortly). Could it be that instructors with higher “beauty” scores also have higher teaching evaluations? Could it be instead that instructors with higher “beauty” scores tend to have lower teaching evaluations? Or could it be that there is no relationship between “beauty” score and teaching evaluations? We’ll answer these questions by modeling the relationship between teaching scores and “beauty” scores using simple linear regression where we have: A numerical outcome variable \\(y\\) (the instructor’s teaching score) and A single numerical explanatory variable \\(x\\) (the instructor’s “beauty” score). 17.1.1 Exploratory data analysis The data on the 463 courses at UT Austin can be found in the evals data frame included in the moderndive package. However, to keep things simple, let’s select() only the subset of the variables we’ll consider in this chapter, and save this data in a new data frame called evals_ch5: evals_ch5 &lt;- evals %&gt;% select(ID, score, bty_avg, age) A crucial step before doing any kind of analysis or modeling is performing an exploratory data analysis, or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA: Most crucially, looking at the raw data values. Computing summary statistics, such as means, medians, and interquartile ranges. Creating data visualizations. Let’s perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road. You can do this by using RStudio’s spreadsheet viewer or by using the glimpse() function as introduced in Subsection ?? on exploring data frames: glimpse(evals_ch5) Rows: 463 Columns: 4 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… $ score &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4.… $ bty_avg &lt;dbl&gt; 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, 3.17, 3.… $ age &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 40… Observe that Rows: 463 indicates that there are 463 rows/observations in evals_ch5, where each row corresponds to one observed course at UT Austin. It is important to note that the observational unit is an individual course and not an individual instructor. Recall from Subsection ?? that the observational unit is the “type of thing” that is being measured by our variables. Since instructors teach more than one course in an academic year, the same instructor will appear more than once in the data. Hence there are fewer than 463 unique instructors being represented in evals_ch5. We’ll revisit this idea in Section 18.3, when we talk about the “independence assumption” for inference for regression. A full description of all the variables included in evals can be found at openintro.org or by reading the associated help file (run ?evals in the console). However, let’s fully describe only the 4 variables we selected in evals_ch5: ID: An identification variable used to distinguish between the 1 through 463 courses in the dataset. score: A numerical variable of the course instructor’s average teaching score, where the average is computed from the evaluation scores from all students in that course. Teaching scores of 1 are lowest and 5 are highest. This is the outcome variable \\(y\\) of interest. bty_avg: A numerical variable of the course instructor’s average “beauty” score, where the average is computed from a separate panel of six students. “Beauty” scores of 1 are lowest and 10 are highest. This is the explanatory variable \\(x\\) of interest. age: A numerical variable of the course instructor’s age. This will be another explanatory variable \\(x\\) that we’ll use in the Learning check at the end of this subsection. An alternative way to look at the raw data values is by choosing a random sample of the rows in evals_ch5 by piping it into the sample_n() function from the dplyr package. Here we set the size argument to be 5, indicating that we want a random sample of 5 rows. We display the results in Table 17.1. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows. evals_ch5 %&gt;% sample_n(size = 5) TABLE 17.1: A random sample of 5 out of the 463 courses at UT Austin ID score bty_avg age 129 3.7 3.00 62 109 4.7 4.33 46 28 4.8 5.50 62 434 2.8 2.00 62 330 4.0 2.33 64 Now that we’ve looked at the raw values in our evals_ch5 data frame and got a preliminary sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s start by computing the mean and median of our numerical outcome variable score and our numerical explanatory variable “beauty” score denoted as bty_avg. We’ll do this by using the summarize() function from dplyr along with the mean() and median() summary functions we saw in Section 13.3. evals_ch5 %&gt;% summarize(mean_bty_avg = mean(bty_avg), mean_score = mean(score), median_bty_avg = median(bty_avg), median_score = median(score)) # A tibble: 1 × 4 mean_bty_avg mean_score median_bty_avg median_score &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4.41784 4.17473 4.333 4.3 However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? Typing out all these summary statistic functions in summarize() would be long and tedious. Instead, let’s use the convenient skim() function from the skimr package. This function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our evals_ch5 data frame, select() only the outcome and explanatory variables teaching score and bty_avg, and pipe them into the skim() function: evals_ch5 %&gt;% select(score, bty_avg) %&gt;% skim() Skim summary statistics n obs: 463 n variables: 2 ── Variable type:numeric variable missing complete n mean sd p0 p25 p50 p75 p100 bty_avg 0 463 463 4.42 1.53 1.67 3.17 4.33 5.5 8.17 score 0 463 463 4.17 0.54 2.3 3.8 4.3 4.6 5 (For formatting purposes in this book, the inline histogram that is usually printed with skim() has been removed. This can be done by using skim_with(numeric = list(hist = NULL)) prior to using the skim() function for version 1.0.6 of skimr.) For the numerical variables teaching score and bty_avg it returns: missing: the number of missing values complete: the number of non-missing or complete values n: the total number of values mean: the average sd: the standard deviation p0: the 0th percentile: the value at which 0% of observations are smaller than it (the minimum value) p25: the 25th percentile: the value at which 25% of observations are smaller than it (the 1st quartile) p50: the 50th percentile: the value at which 50% of observations are smaller than it (the 2nd quartile and more commonly called the median) p75: the 75th percentile: the value at which 75% of observations are smaller than it (the 3rd quartile) p100: the 100th percentile: the value at which 100% of observations are smaller than it (the maximum value) Looking at this output, we can see how the values of both variables distribute. For example, the mean teaching score was 4.17 out of 5, whereas the mean “beauty” score was 4.42 out of 10. Furthermore, the middle 50% of teaching scores was between 3.80 and 4.6 (the first and third quartiles), whereas the middle 50% of “beauty” scores falls within 3.17 to 5.5 out of 10. The skim() function only returns what are known as univariate summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist bivariate summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the correlation coefficient. Generally speaking, coefficients are quantitative expressions of a specific phenomenon. A correlation coefficient is a quantitative expression of the strength of the linear relationship between two numerical variables. Its value ranges between -1 and 1 where: -1 indicates a perfect negative relationship: As one variable increases, the value of the other variable tends to go down, following a straight line. 0 indicates no relationship: The values of both variables go up/down independently of each other. +1 indicates a perfect positive relationship: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion. Figure 17.1 gives examples of 9 different correlation coefficient values for hypothetical numerical variables \\(x\\) and \\(y\\). For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between \\(x\\) and \\(y\\), but it is not as strong as the negative linear relationship between \\(x\\) and \\(y\\) when the correlation coefficient is -0.9 or -1. FIGURE 17.1: Nine different correlation coefficients. The correlation coefficient can be computed using the get_correlation() function in the moderndive package. In this case, the inputs to the function are the two numerical variables for which we want to calculate the correlation coefficient. We put the name of the outcome variable on the left-hand side of the ~ “tilde” sign, while putting the name of the explanatory variable on the right-hand side. This is known as R’s formula notation. We will use this same “formula” syntax with regression later in this chapter. evals_ch5 %&gt;% get_correlation(formula = score ~ bty_avg) # A tibble: 1 × 1 cor &lt;dbl&gt; 1 0.187142 An alternative way to compute correlation is to use the cor() summary function within a summarize(): evals_ch5 %&gt;% summarize(correlation = cor(score, bty_avg)) In our case, the correlation coefficient of 0.187 indicates that the relationship between teaching evaluation score and “beauty” average is “weakly positive.” There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren’t close to the extreme values of -1, 0, and 1. To develop your intuition about correlation coefficients, play the “Guess the Correlation” 1980’s style video game mentioned in Subsection 17.4.1. Let’s now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the score and bty_avg variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let’s do this using geom_point() and display the result in Figure 17.2. Furthermore, let’s highlight the six points in the top right of the visualization in a box. ggplot(evals_ch5, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Scatterplot of relationship of teaching and beauty scores&quot;) FIGURE 17.2: Instructor evaluation scores at UT Austin. Observe that most “beauty” scores lie between 2 and 8, while most teaching scores lie between 3 and 5. Furthermore, while opinions may vary, it is our opinion that the relationship between teaching score and “beauty” score is “weakly positive.” This is consistent with our earlier computed correlation coefficient of 0.187. Furthermore, there appear to be six points in the top-right of this plot highlighted in the box. However, this is not actually the case, as this plot suffers from overplotting. Recall from Subsection 12.3.2 that overplotting occurs when several points are stacked directly on top of each other, making it difficult to distinguish them. So while it may appear that there are only six points in the box, there are actually more. This fact is only apparent when using geom_jitter() in place of geom_point(). We display the resulting plot in Figure 17.3 along with the same small box as in Figure 17.2. ggplot(evals_ch5, aes(x = bty_avg, y = score)) + geom_jitter() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Scatterplot of relationship of teaching and beauty scores&quot;) FIGURE 17.3: Instructor evaluation scores at UT Austin. It is now apparent that there are 12 points in the area highlighted in the box and not six as originally suggested in Figure 17.2. Recall from Subsection 12.3.2 on overplotting that jittering adds a little random “nudge” to each of the points to break up these ties. Furthermore, recall that jittering is strictly a visualization tool; it does not alter the original values in the data frame evals_ch5. To keep things simple going forward, however, we’ll only present regular scatterplots rather than their jittered counterparts. Let’s build on the unjittered scatterplot in Figure 17.2 by adding a “best-fitting” line: of all possible lines we can draw on this scatterplot, it is the line that “best” fits through the cloud of points. We do this by adding a new geom_smooth(method = \"lm\", se = FALSE) layer to the ggplot() code that created the scatterplot in Figure 17.2. The method = \"lm\" argument sets the line to be a “linear model.” The se = FALSE argument suppresses standard error uncertainty bars. (We’ll define the concept of standard error later in Subsection 14.3.2.) ggplot(evals_ch5, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship between teaching and beauty scores&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 17.4: Regression line. The line in the resulting Figure 17.4 is called a “regression line.” The regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable score and the explanatory variable bty_avg. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of 0.187 suggesting that there is a positive relationship between these two variables: as instructors have higher “beauty” scores, so also do they receive higher teaching evaluations. We’ll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value. Furthermore, a regression line is “best-fitting” in that it minimizes some mathematical criteria. We present these mathematical criteria in Subsection 17.3.2, but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable. Learning check (LC5.1) Conduct a new exploratory data analysis with the same outcome variable \\(y\\) being score but with age as the new explanatory variable \\(x\\). Remember, this involves three things: Looking at the raw data values. Computing summary statistics. Creating data visualizations. What can you say about the relationship between age and teaching scores based on this exploration? 17.1.2 Simple linear regression You may recall from secondary/high school algebra that the equation of a line is \\(y = a + b\\cdot x\\). (Note that the \\(\\cdot\\) symbol is equivalent to the \\(\\times\\) “multiply by” mathematical symbol. We’ll use the \\(\\cdot\\) symbol in the rest of this book as it is more succinct.) It is defined by two coefficients \\(a\\) and \\(b\\). The intercept coefficient \\(a\\) is the value of \\(y\\) when \\(x = 0\\). The slope coefficient \\(b\\) for \\(x\\) is the increase in \\(y\\) for every increase of one in \\(x\\). This is also called the “rise over run.” However, when defining a regression line like the regression line in Figure 17.4, we use slightly different notation: the equation of the regression line is \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) . The intercept coefficient is \\(b_0\\), so \\(b_0\\) is the value of \\(\\widehat{y}\\) when \\(x = 0\\). The slope coefficient for \\(x\\) is \\(b_1\\), i.e., the increase in \\(\\widehat{y}\\) for every increase of one in \\(x\\). Why do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression to indicate that we have a “fitted value,” or the value of \\(y\\) on the regression line for a given \\(x\\) value. We’ll discuss this more in the upcoming Subsection 17.1.3. We know that the regression line in Figure 17.4 has a positive slope \\(b_1\\) corresponding to our explanatory \\(x\\) variable bty_avg. Why? Because as instructors tend to have higher bty_avg scores, so also do they tend to have higher teaching evaluation scores. However, what is the numerical value of the slope \\(b_1\\)? What about the intercept \\(b_0\\)? Let’s not compute these two values by hand, but rather let’s use a computer! We can obtain the values of the intercept \\(b_0\\) and the slope for bty_avg \\(b_1\\) by outputting a linear regression table. This is done in two steps: We first “fit” the linear regression model using the lm() function and save it in score_model. We get the regression table by applying the get_regression_table() function from the moderndive package to score_model. # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch5) # Get regression table: get_regression_table(score_model) TABLE 17.2: Linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Let’s first focus on interpreting the regression table output in Table 17.2, and then we’ll later revisit the code that produced it. In the estimate column of Table 17.2 are the intercept \\(b_0\\) = 3.88 and the slope \\(b_1\\) = 0.067 for bty_avg. Thus the equation of the regression line in Figure 17.4 follows: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x\\\\ \\widehat{\\text{score}} &amp;= b_0 + b_{\\text{bty}\\_\\text{avg}} \\cdot\\text{bty}\\_\\text{avg}\\\\ &amp;= 3.88 + 0.067\\cdot\\text{bty}\\_\\text{avg} \\end{aligned} \\] The intercept \\(b_0\\) = 3.88 is the average teaching score \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) for those courses where the instructor had a “beauty” score bty_avg of 0. Or in graphical terms, it’s where the line intersects the \\(y\\) axis when \\(x\\) = 0. Note, however, that while the intercept of the regression line has a mathematical interpretation, it has no practical interpretation here, since observing a bty_avg of 0 is impossible; it is the average of six panelists’ “beauty” scores ranging from 1 to 10. Furthermore, looking at the scatterplot with the regression line in Figure 17.4, no instructors had a “beauty” score anywhere near 0. Of greater interest is the slope \\(b_1\\) = \\(b_{\\text{bty}\\_\\text{avg}}\\) for bty_avg of 0.067, as this summarizes the relationship between the teaching and “beauty” score variables. Note that the sign is positive, suggesting a positive relationship between these two variables, meaning teachers with higher “beauty” scores also tend to have higher teaching scores. Recall from earlier that the correlation coefficient is 0.187. They both have the same positive sign, but have a different value. Recall further that the correlation’s interpretation is the “strength of linear association”. The slope’s interpretation is a little different: For every increase of 1 unit in bty_avg, there is an associated increase of, on average, 0.067 units of score. We only state that there is an associated increase and not necessarily a causal increase. For example, perhaps it’s not that higher “beauty” scores directly cause higher teaching scores per se. Instead, the following could hold true: individuals from wealthier backgrounds tend to have stronger educational backgrounds and hence have higher teaching scores, while at the same time these wealthy individuals also tend to have higher “beauty” scores. In other words, just because two variables are strongly associated, it doesn’t necessarily mean that one causes the other. This is summed up in the often quoted phrase, “correlation is not necessarily causation.” We discuss this idea further in Subsection 17.3.1. Furthermore, we say that this associated increase is on average 0.067 units of teaching score, because you might have two instructors whose bty_avg scores differ by 1 unit, but their difference in teaching scores won’t necessarily be exactly 0.067. What the slope of 0.067 is saying is that across all possible courses, the average difference in teaching score between two instructors whose “beauty” scores differ by one is 0.067. Now that we’ve learned how to compute the equation for the regression line in Figure 17.4 using the values in the estimate column of Table 17.2, and how to interpret the resulting intercept and slope, let’s revisit the code that generated this table: # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch5) # Get regression table: get_regression_table(score_model) First, we “fit” the linear regression model to the data using the lm() function and save this as score_model. When we say “fit”, we mean “find the best fitting line to this data.” lm() stands for “linear model” and is used as follows: lm(y ~ x, data = data_frame_name) where: y is the outcome variable, followed by a tilde ~. In our case, y is set to score. x is the explanatory variable. In our case, x is set to bty_avg. The combination of y ~ x is called a model formula. (Note the order of y and x.) In our case, the model formula is score ~ bty_avg. We saw such model formulas earlier when we computed the correlation coefficient using the get_correlation() function in Subsection 17.1.1. data_frame_name is the name of the data frame that contains the variables y and x. In our case, data_frame_name is the evals_ch5 data frame. Second, we take the saved model in score_model and apply the get_regression_table() function from the moderndive package to it to obtain the regression table in Table 17.2. This function is an example of what’s known in computer programming as a wrapper function. They take other pre-existing functions and “wrap” them into a single function that hides its inner workings. This concept is illustrated in Figure 17.5. FIGURE 17.5: The concept of a wrapper function. So all you need to worry about is what the inputs look like and what the outputs look like; you leave all the other details “under the hood of the car.” In our regression modeling example, the get_regression_table() function takes a saved lm() linear regression model as input and returns a data frame of the regression table as output. If you’re interested in learning more about the get_regression_table() function’s inner workings, check out Subsection 17.3.3. Lastly, you might be wondering what the remaining five columns in Table 17.2 are: std_error, statistic, p_value, lower_ci and upper_ci. They are the standard error, test statistic, p-value, lower 95% confidence interval bound, and upper 95% confidence interval bound. They tell us about both the statistical significance and practical significance of our results. This is loosely the “meaningfulness” of our results from a statistical perspective. Let’s put aside these ideas for now and revisit them in Chapter 18 on (statistical) inference for regression. We’ll do this after we’ve had a chance to cover standard errors in Chapter 14, confidence intervals in Chapter 15, and hypothesis testing and \\(p\\)-values in Chapter 16. Learning check (LC5.2) Fit a new simple linear regression using lm(score ~ age, data = evals_ch5) where age is the new explanatory variable \\(x\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your earlier exploratory data analysis? 17.1.3 Observed/fitted values and residuals We just saw how to get the value of the intercept and the slope of a regression line from the estimate column of a regression table generated by the get_regression_table() function. Now instead say we want information on individual observations. For example, let’s focus on the 21st of the 463 courses in the evals_ch5 data frame in Table 17.3: TABLE 17.3: Data for the 21st course out of 463 ID score bty_avg age 21 4.9 7.33 31 What is the value \\(\\widehat{y}\\) on the regression line corresponding to this instructor’s bty_avg “beauty” score of 7.333? In Figure 17.6 we mark three values corresponding to the instructor for this 21st course and give their statistical names: Circle: The observed value \\(y\\) = 4.9 is this course’s instructor’s actual teaching score. Square: The fitted value \\(\\widehat{y}\\) is the value on the regression line for \\(x\\) = bty_avg = 7.333. This value is computed using the intercept and slope in the previous regression table: \\[\\widehat{y} = b_0 + b_1 \\cdot x = 3.88 + 0.067 \\cdot 7.333 = 4.369\\] Arrow: The length of this arrow is the residual and is computed by subtracting the fitted value \\(\\widehat{y}\\) from the observed value \\(y\\). The residual can be thought of as a model’s error or “lack of fit” for a particular observation. In the case of this course’s instructor, it is \\(y - \\widehat{y}\\) = 4.9 - 4.369 = 0.531. FIGURE 17.6: Example of observed value, fitted value, and residual. Now say we want to compute both the fitted value \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) and the residual \\(y - \\widehat{y}\\) for all 463 courses in the study. Recall that each course corresponds to one of the 463 rows in the evals_ch5 data frame and also one of the 463 points in the regression plot in Figure 17.6. We could repeat the previous calculations we performed by hand 463 times, but that would be tedious and time consuming. Instead, let’s do this using a computer with the get_regression_points() function. Just like the get_regression_table() function, the get_regression_points() function is a “wrapper” function. However, this function returns a different output. Let’s apply the get_regression_points() function to score_model, which is where we saved our lm() model in the previous section. In Table 17.4 we present the results of only the 21st through 24th courses for brevity’s sake. regression_points &lt;- get_regression_points(score_model) regression_points TABLE 17.4: Regression points (for only the 21st through 24th courses) ID score bty_avg score_hat residual 21 4.9 7.33 4.37 0.531 22 4.6 7.33 4.37 0.231 23 4.5 7.33 4.37 0.131 24 4.4 5.50 4.25 0.153 Let’s inspect the individual columns and match them with the elements of Figure 17.6: The score column represents the observed outcome variable \\(y\\). This is the y-position of the 463 black points. The bty_avg column represents the values of the explanatory variable \\(x\\). This is the x-position of the 463 black points. The score_hat column represents the fitted values \\(\\widehat{y}\\). This is the corresponding value on the regression line for the 463 \\(x\\) values. The residual column represents the residuals \\(y - \\widehat{y}\\). This is the 463 vertical distances between the 463 black points and the regression line. Just as we did for the instructor of the 21st course in the evals_ch5 dataset (in the first row of the table), let’s repeat the calculations for the instructor of the 24th course (in the fourth row of Table 17.4): score = 4.4 is the observed teaching score \\(y\\) for this course’s instructor. bty_avg = 5.50 is the value of the explanatory variable bty_avg \\(x\\) for this course’s instructor. score_hat = 4.25 = 3.88 + 0.067 \\(\\cdot\\) 5.50 is the fitted value \\(\\widehat{y}\\) on the regression line for this course’s instructor. residual = 0.153 = 4.4 - 4.25 is the value of the residual for this instructor. In other words, the model’s fitted value was off by 0.153 teaching score units for this course’s instructor. At this point, you can skip ahead if you like to Subsection 17.3.2 to learn about the processes behind what makes “best-fitting” regression lines. As a primer, a “best-fitting” line refers to the line that minimizes the sum of squared residuals out of all possible lines we can draw through the points. In Section 17.2, we’ll discuss another common scenario of having a categorical explanatory variable and a numerical outcome variable. Learning check (LC5.3) Generate a data frame of the residuals of the model where you used age as the explanatory \\(x\\) variable. 17.2 One categorical explanatory variable It’s an unfortunate truth that life expectancy is not the same across all countries in the world. International development agencies are interested in studying these differences in life expectancy in the hopes of identifying where governments should allocate resources to address this problem. In this section, we’ll explore differences in life expectancy in two ways: Differences between continents: Are there significant differences in average life expectancy between the five populated continents of the world: Africa, the Americas, Asia, Europe, and Oceania? Differences within continents: How does life expectancy vary within the world’s five continents? For example, is the spread of life expectancy among the countries of Africa larger than the spread of life expectancy among the countries of Asia? To answer such questions, we’ll use the gapminder data frame included in the gapminder package. This dataset has international development statistics such as life expectancy, GDP per capita, and population for 142 countries for 5-year intervals between 1952 and 2007. Recall we visualized some of this data in Figure 12.1 in Subsection 12.1.2 on the grammar of graphics. We’ll use this data for basic regression again, but now using an explanatory variable \\(x\\) that is categorical, as opposed to the numerical explanatory variable model we used in the previous Section 17.1. A numerical outcome variable \\(y\\) (a country’s life expectancy) and A single categorical explanatory variable \\(x\\) (the continent that the country is a part of). When the explanatory variable \\(x\\) is categorical, the concept of a “best-fitting” regression line is a little different than the one we saw previously in Section 17.1 where the explanatory variable \\(x\\) was numerical. We’ll study these differences shortly in Subsection 17.2.2, but first we conduct an exploratory data analysis. 17.2.1 Exploratory data analysis The data on the 142 countries can be found in the gapminder data frame included in the gapminder package. However, to keep things simple, let’s filter() for only those observations/rows corresponding to the year 2007. Additionally, let’s select() only the subset of the variables we’ll consider in this chapter. We’ll save this data in a new data frame called gapminder2007: library(gapminder) gapminder2007 &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% select(country, lifeExp, continent, gdpPercap) Let’s perform the first common step in an exploratory data analysis: looking at the raw data values. You can do this by using RStudio’s spreadsheet viewer or by using the glimpse() command as introduced in Subsection ?? on exploring data frames: glimpse(gapminder2007) Rows: 142 Columns: 4 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, … $ lifeExp &lt;dbl&gt; 43.8, 76.4, 72.3, 42.7, 75.3, 81.2, 79.8, 75.6, 64.1, 79.4, … $ continent &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, Asi… $ gdpPercap &lt;dbl&gt; 975, 5937, 6223, 4797, 12779, 34435, 36126, 29796, 1391, 336… Observe that Rows: 142 indicates that there are 142 rows/observations in gapminder2007, where each row corresponds to one country. In other words, the observational unit is an individual country. Furthermore, observe that the variable continent is of type &lt;fct&gt;, which stands for factor, which is R’s way of encoding categorical variables. A full description of all the variables included in gapminder can be found by reading the associated help file (run ?gapminder in the console). However, let’s fully describe only the 4 variables we selected in gapminder2007: country: An identification variable of type character/text used to distinguish the 142 countries in the dataset. lifeExp: A numerical variable of that country’s life expectancy at birth. This is the outcome variable \\(y\\) of interest. continent: A categorical variable with five levels. Here “levels” correspond to the possible categories: Africa, Asia, Americas, Europe, and Oceania. This is the explanatory variable \\(x\\) of interest. gdpPercap: A numerical variable of that country’s GDP per capita in US inflation-adjusted dollars that we’ll use as another outcome variable \\(y\\) in the Learning check at the end of this subsection. Let’s look at a random sample of five out of the 142 countries in Table 17.5. gapminder2007 %&gt;% sample_n(size = 5) TABLE 17.5: Random sample of 5 out of 142 countries country lifeExp continent gdpPercap Togo 58.4 Africa 883 Sao Tome and Principe 65.5 Africa 1598 Congo, Dem. Rep.  46.5 Africa 278 Lesotho 42.6 Africa 1569 Bulgaria 73.0 Europe 10681 Note that random sampling will likely produce a different subset of 5 rows for you than what’s shown. Now that we’ve looked at the raw values in our gapminder2007 data frame and got a sense of the data, let’s move on to computing summary statistics. Let’s once again apply the skim() function from the skimr package. Recall from our previous EDA that this function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our gapminder2007 data frame, select() only the outcome and explanatory variables lifeExp and continent, and pipe them into the skim() function: gapminder2007 %&gt;% select(lifeExp, continent) %&gt;% skim() Skim summary statistics n obs: 142 n variables: 2 ── Variable type:factor variable missing complete n n_unique top_counts ordered continent 0 142 142 5 Afr: 52, Asi: 33, Eur: 30, Ame: 25 FALSE ── Variable type:numeric variable missing complete n mean sd p0 p25 p50 p75 p100 lifeExp 0 142 142 67.01 12.07 39.61 57.16 71.94 76.41 82.6 The skim() output now reports summaries for categorical variables (Variable type:factor) separately from the numerical variables (Variable type:numeric). For the categorical variable continent, it reports: missing, complete, and n, which are the number of missing, complete, and total number of values as before, respectively. n_unique: The number of unique levels to this variable, corresponding to Africa, Asia, Americas, Europe, and Oceania. This refers to how many countries are in the data for each continent. top_counts: In this case, the top four counts: Africa has 52 countries, Asia has 33, Europe has 30, and Americas has 25. Not displayed is Oceania with 2 countries. ordered: This tells us whether the categorical variable is “ordinal”: whether there is an encoded hierarchy (like low, medium, high). In this case, continent is not ordered. Turning our attention to the summary statistics of the numerical variable lifeExp, we observe that the global median life expectancy in 2007 was 71.94. Thus, half of the world’s countries (71 countries) had a life expectancy less than 71.94. The mean life expectancy of 67.01 is lower, however. Why is the mean life expectancy lower than the median? We can answer this question by performing the last of the three common steps in an exploratory data analysis: creating data visualizations. Let’s visualize the distribution of our outcome variable \\(y\\) = lifeExp in Figure 17.7. ggplot(gapminder2007, aes(x = lifeExp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + labs(x = &quot;Life expectancy&quot;, y = &quot;Number of countries&quot;, title = &quot;Histogram of distribution of worldwide life expectancies&quot;) FIGURE 17.7: Histogram of life expectancy in 2007. We see that this data is left-skewed, also known as negatively skewed: there are a few countries with low life expectancy that are bringing down the mean life expectancy. However, the median is less sensitive to the effects of such outliers; hence, the median is greater than the mean in this case. Remember, however, that we want to compare life expectancies both between continents and within continents. In other words, our visualizations need to incorporate some notion of the variable continent. We can do this easily with a faceted histogram. Recall from Section 12.6 that facets allow us to split a visualization by the different values of another variable. We display the resulting visualization in Figure 17.8 by adding a facet_wrap(~ continent, nrow = 2) layer. ggplot(gapminder2007, aes(x = lifeExp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + labs(x = &quot;Life expectancy&quot;, y = &quot;Number of countries&quot;, title = &quot;Histogram of distribution of worldwide life expectancies&quot;) + facet_wrap(~ continent, nrow = 2) FIGURE 17.8: Life expectancy in 2007. Observe that unfortunately the distribution of African life expectancies is much lower than the other continents, while in Europe life expectancies tend to be higher and furthermore do not vary as much. On the other hand, both Asia and Africa have the most variation in life expectancies. There is the least variation in Oceania, but keep in mind that there are only two countries in Oceania: Australia and New Zealand. Recall that an alternative method to visualize the distribution of a numerical variable split by a categorical variable is by using a side-by-side boxplot. We map the categorical variable continent to the \\(x\\)-axis and the different life expectancies within each continent on the \\(y\\)-axis in Figure 17.9. ggplot(gapminder2007, aes(x = continent, y = lifeExp)) + geom_boxplot() + labs(x = &quot;Continent&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy by continent&quot;) FIGURE 17.9: Life expectancy in 2007. Some people prefer comparing the distributions of a numerical variable between different levels of a categorical variable using a boxplot instead of a faceted histogram. This is because we can make quick comparisons between the categorical variable’s levels with imaginary horizontal lines. For example, observe in Figure 17.9 that we can quickly convince ourselves that Oceania has the highest median life expectancies by drawing an imaginary horizontal line at \\(y\\) = 80. Furthermore, as we observed in the faceted histogram in Figure 17.8, Africa and Asia have the largest variation in life expectancy as evidenced by their large interquartile ranges (the heights of the boxes). It’s important to remember, however, that the solid lines in the middle of the boxes correspond to the medians (the middle value) rather than the mean (the average). So, for example, if you look at Asia, the solid line denotes the median life expectancy of around 72 years. This tells us that half of all countries in Asia have a life expectancy below 72 years, whereas half have a life expectancy above 72 years. Let’s compute the median and mean life expectancy for each continent with a little more data wrangling and display the results in Table 17.6. lifeExp_by_continent &lt;- gapminder2007 %&gt;% group_by(continent) %&gt;% summarize(median = median(lifeExp), mean = mean(lifeExp)) TABLE 17.6: Life expectancy by continent continent median mean Africa 52.9 54.8 Americas 72.9 73.6 Asia 72.4 70.7 Europe 78.6 77.6 Oceania 80.7 80.7 Observe the order of the second column median life expectancy: Africa is lowest, the Americas and Asia are next with similar medians, then Europe, then Oceania. This ordering corresponds to the ordering of the solid black lines inside the boxes in our side-by-side boxplot in Figure 17.9. Let’s now turn our attention to the values in the third column mean. Using Africa’s mean life expectancy of 54.8 as a baseline for comparison, let’s start making comparisons to the mean life expectancies of the other four continents and put these values in Table 17.7, which we’ll revisit later on in this section. For the Americas, it is 73.6 - 54.8 = 18.8 years higher. For Asia, it is 70.7 - 54.8 = 15.9 years higher. For Europe, it is 77.6 - 54.8 = 22.8 years higher. For Oceania, it is 80.7 - 54.8 = 25.9 years higher. TABLE 17.7: Mean life expectancy by continent and relative differences from mean for Africa continent mean Difference versus Africa Africa 54.8 0.0 Americas 73.6 18.8 Asia 70.7 15.9 Europe 77.6 22.8 Oceania 80.7 25.9 Learning check (LC5.4) Conduct a new exploratory data analysis with the same explanatory variable \\(x\\) being continent but with gdpPercap as the new outcome variable \\(y\\). What can you say about the differences in GDP per capita between continents based on this exploration? 17.2.2 Linear regression In Subsection 17.1.2 we introduced simple linear regression, which involves modeling the relationship between a numerical outcome variable \\(y\\) and a numerical explanatory variable \\(x\\). In our life expectancy example, we now instead have a categorical explanatory variable continent. Our model will not yield a “best-fitting” regression line like in Figure 17.4, but rather offsets relative to a baseline for comparison. As we did in Subsection 17.1.2 when studying the relationship between teaching scores and “beauty” scores, let’s output the regression table for this model. Recall that this is done in two steps: We first “fit” the linear regression model using the lm(y ~ x, data) function and save it in lifeExp_model. We get the regression table by applying the get_regression_table() function from the moderndive package to lifeExp_model. lifeExp_model &lt;- lm(lifeExp ~ continent, data = gapminder2007) get_regression_table(lifeExp_model) TABLE 17.8: Linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 54.8 1.02 53.45 0 52.8 56.8 continent: Americas 18.8 1.80 10.45 0 15.2 22.4 continent: Asia 15.9 1.65 9.68 0 12.7 19.2 continent: Europe 22.8 1.70 13.47 0 19.5 26.2 continent: Oceania 25.9 5.33 4.86 0 15.4 36.5 Let’s once again focus on the values in the term and estimate columns of Table 17.8. Why are there now 5 rows? Let’s break them down one-by-one: intercept corresponds to the mean life expectancy of countries in Africa of 54.8 years. continent: Americas corresponds to countries in the Americas and the value +18.8 is the same difference in mean life expectancy relative to Africa we displayed in Table 17.7. In other words, the mean life expectancy of countries in the Americas is \\(54.8 + 18.8 = 73.6\\). continent: Asia corresponds to countries in Asia and the value +15.9 is the same difference in mean life expectancy relative to Africa we displayed in Table 17.7. In other words, the mean life expectancy of countries in Asia is \\(54.8 + 15.9 = 70.7\\). continent: Europe corresponds to countries in Europe and the value +22.8 is the same difference in mean life expectancy relative to Africa we displayed in Table 17.7. In other words, the mean life expectancy of countries in Europe is \\(54.8 + 22.8 = 77.6\\). continent: Oceania corresponds to countries in Oceania and the value +25.9 is the same difference in mean life expectancy relative to Africa we displayed in Table 17.7. In other words, the mean life expectancy of countries in Oceania is \\(54.8 + 25.9 = 80.7\\). To summarize, the 5 values in the estimate column in Table 17.8 correspond to the “baseline for comparison” continent Africa (the intercept) as well as four “offsets” from this baseline for the remaining 4 continents: the Americas, Asia, Europe, and Oceania. You might be asking at this point why was Africa chosen as the “baseline for comparison” group. This is the case for no other reason than it comes first alphabetically of the five continents; by default R arranges factors/categorical variables in alphanumeric order. You can change this baseline group to be another continent if you manipulate the variable continent’s factor “levels” using the forcats package. See Chapter 15 of R for Data Science (Grolemund and Wickham 2017) for examples. Let’s now write the equation for our fitted values \\(\\widehat{y} = \\widehat{\\text{life exp}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\text{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\text{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x) \\end{aligned} \\] Whoa! That looks daunting! Don’t fret, however, as once you understand what all the elements mean, things simplify greatly. First, \\(\\mathbb{1}_{A}(x)\\) is what’s known in mathematics as an “indicator function.” It returns only one of two possible values, 0 and 1, where \\[ \\mathbb{1}_{A}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } x \\text{ is in } A \\\\ 0 &amp; \\text{if } \\text{otherwise} \\end{array} \\right. \\] In a statistical modeling context, this is also known as a dummy variable. In our case, let’s consider the first such indicator variable \\(\\mathbb{1}_{\\text{Amer}}(x)\\). This indicator function returns 1 if a country is in the Americas, 0 otherwise: \\[ \\mathbb{1}_{\\text{Amer}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{country } x \\text{ is in the Americas} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\] Second, \\(b_0\\) corresponds to the intercept as before; in this case, it’s the mean life expectancy of all countries in Africa. Third, the \\(b_{\\text{Amer}}\\), \\(b_{\\text{Asia}}\\), \\(b_{\\text{Euro}}\\), and \\(b_{\\text{Ocean}}\\) represent the 4 “offsets relative to the baseline for comparison” in the regression table output in Table 17.8: continent: Americas, continent: Asia, continent: Europe, and continent: Oceania. Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{life exp}}\\) for a country in Africa. Since the country is in Africa, all four indicator functions \\(\\mathbb{1}_{\\text{Amer}}(x)\\), \\(\\mathbb{1}_{\\text{Asia}}(x)\\), \\(\\mathbb{1}_{\\text{Euro}}(x)\\), and \\(\\mathbb{1}_{\\text{Ocean}}(x)\\) will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\text{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\text{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 0 + 15.9\\cdot 0 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 \\end{aligned} \\] In other words, all that’s left is the intercept \\(b_0\\), corresponding to the average life expectancy of African countries of 54.8 years. Next, say we are considering a country in the Americas. In this case, only the indicator function \\(\\mathbb{1}_{\\text{Amer}}(x)\\) for the Americas will equal 1, while all the others will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + \\\\ &amp; \\qquad 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 1 + 15.9\\cdot 0 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 + 18.8 \\\\ &amp; = 73.6 \\end{aligned} \\] which is the mean life expectancy for countries in the Americas of 73.6 years in Table 17.7. Note the “offset from the baseline for comparison” is +18.8 years. Let’s do one more. Say we are considering a country in Asia. In this case, only the indicator function \\(\\mathbb{1}_{\\text{Asia}}(x)\\) for Asia will equal 1, while all the others will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + \\\\ &amp; \\qquad 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 0 + 15.9\\cdot 1 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 + 15.9 \\\\ &amp; = 70.7 \\end{aligned} \\] which is the mean life expectancy for Asian countries of 70.7 years in Table 17.7. The “offset from the baseline for comparison” here is +15.9 years. Let’s generalize this idea a bit. If we fit a linear regression model using a categorical explanatory variable \\(x\\) that has \\(k\\) possible categories, the regression table will return an intercept and \\(k - 1\\) “offsets.” In our case, since there are \\(k = 5\\) continents, the regression model returns an intercept corresponding to the baseline for comparison group of Africa and \\(k - 1 = 4\\) offsets corresponding to the Americas, Asia, Europe, and Oceania. Understanding a regression table output when you’re using a categorical explanatory variable is a topic those new to regression often struggle with. The only real remedy for these struggles is practice, practice, practice. However, once you equip yourselves with an understanding of how to create regression models using categorical explanatory variables, you’ll be able to incorporate many new variables into your models, given the large amount of the world’s data that is categorical. If you feel like you’re still struggling at this point, however, we suggest you closely compare Tables 17.7 and 17.8 and note how you can compute all the values from one table using the values in the other. Learning check (LC5.5) Fit a new linear regression using lm(gdpPercap ~ continent, data = gapminder2007) where gdpPercap is the new outcome variable \\(y\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis? 17.2.3 Observed/fitted values and residuals Recall in Subsection 17.1.3, we defined the following three concepts: Observed values \\(y\\), or the observed value of the outcome variable Fitted values \\(\\widehat{y}\\), or the value on the regression line for a given \\(x\\) value Residuals \\(y - \\widehat{y}\\), or the error between the observed value and the fitted value We obtained these values and other values using the get_regression_points() function from the moderndive package. This time, however, let’s add an argument setting ID = \"country\": this is telling the function to use the variable country in gapminder2007 as an identification variable in the output. This will help contextualize our analysis by matching values to countries. regression_points &lt;- get_regression_points(lifeExp_model, ID = &quot;country&quot;) regression_points TABLE 17.9: Regression points (First 10 out of 142 countries) country lifeExp continent lifeExp_hat residual Afghanistan 43.8 Asia 70.7 -26.900 Albania 76.4 Europe 77.6 -1.226 Algeria 72.3 Africa 54.8 17.495 Angola 42.7 Africa 54.8 -12.075 Argentina 75.3 Americas 73.6 1.712 Australia 81.2 Oceania 80.7 0.516 Austria 79.8 Europe 77.6 2.180 Bahrain 75.6 Asia 70.7 4.907 Bangladesh 64.1 Asia 70.7 -6.666 Belgium 79.4 Europe 77.6 1.792 Observe in Table 17.9 that lifeExp_hat contains the fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{lifeExp}}\\). If you look closely, there are only 5 possible values for lifeExp_hat. These correspond to the five mean life expectancies for the 5 continents that we displayed in Table 17.7 and computed using the values in the estimate column of the regression table in Table 17.8. The residual column is simply \\(y - \\widehat{y}\\) = lifeExp - lifeExp_hat. These values can be interpreted as the deviation of a country’s life expectancy from its continent’s average life expectancy. For example, look at the first row of Table 17.9 corresponding to Afghanistan. The residual of \\(y - \\widehat{y} = 43.8 - 70.7 = -26.9\\) is telling us that Afghanistan’s life expectancy is a whopping 26.9 years lower than the mean life expectancy of all Asian countries. This can in part be explained by the many years of war that country has suffered. Learning check (LC5.6) Using either the sorting functionality of RStudio’s spreadsheet viewer or using the data wrangling tools you learned in Chapter 13, identify the five countries with the five smallest (most negative) residuals? What do these negative residuals say about their life expectancy relative to their continents’ life expectancy? (LC5.7) Repeat this process, but identify the five countries with the five largest (most positive) residuals. What do these positive residuals say about their life expectancy relative to their continents’ life expectancy? 17.3 Related topics 17.3.1 Correlation is not necessarily causation Throughout this chapter we’ve been cautious when interpreting regression slope coefficients. We always discussed the “associated” effect of an explanatory variable \\(x\\) on an outcome variable \\(y\\). For example, our statement from Subsection 17.1.2 that “for every increase of 1 unit in bty_avg, there is an associated increase of on average 0.067 units of score.” We include the term “associated” to be extra careful not to suggest we are making a causal statement. So while “beauty” score of bty_avg is positively correlated with teaching score, we can’t necessarily make any statements about “beauty” scores’ direct causal effect on teaching score without more information on how this study was conducted. Here is another example: a not-so-great medical doctor goes through medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares, “Sleeping with shoes on causes headaches!” FIGURE 17.10: Does sleeping with shoes on cause headaches? However, there is a good chance that if someone is sleeping with their shoes on, it’s potentially because they are intoxicated from alcohol. Furthermore, higher levels of drinking leads to more hangovers, and hence more headaches. The amount of alcohol consumption here is what’s known as a confounding/lurking variable. It “lurks” behind the scenes, confounding the causal relationship (if any) of “sleeping with shoes on” with “waking up with a headache.” We can summarize this in Figure 17.11 with a causal graph where: Y is a response variable; here it is “waking up with a headache.” X is a treatment variable whose causal effect we are interested in; here it is “sleeping with shoes on.” FIGURE 17.11: Causal graph. To study the relationship between Y and X, we could use a regression model where the outcome variable is set to Y and the explanatory variable is set to be X, as you’ve been doing throughout this chapter. However, Figure 17.11 also includes a third variable with arrows pointing at both X and Y: Z is a confounding variable that affects both X and Y, thereby “confounding” their relationship. Here the confounding variable is alcohol. Alcohol will cause people to be both more likely to sleep with their shoes on as well as be more likely to wake up with a headache. Thus any regression model of the relationship between X and Y should also use Z as an explanatory variable. In other words, our doctor needs to take into account who had been drinking the night before. In the next chapter, we’ll start covering multiple regression models that allow us to incorporate more than one variable in our regression models. Establishing causation is a tricky problem and frequently takes either carefully designed experiments or methods to control for the effects of confounding variables. Both these approaches attempt, as best they can, either to take all possible confounding variables into account or negate their impact. This allows researchers to focus only on the relationship of interest: the relationship between the outcome variable Y and the treatment variable X. As you read news stories, be careful not to fall into the trap of thinking that correlation necessarily implies causation. Check out the Spurious Correlations website for some rather comical examples of variables that are correlated, but are definitely not causally related. 17.3.2 Best-fitting line Regression lines are also known as “best-fitting” lines. But what do we mean by “best”? Let’s unpack the criteria that is used in regression to determine “best.” Recall Figure 17.6, where for an instructor with a “beauty” score of \\(x = 7.333\\) we mark the observed value \\(y\\) with a circle, the fitted value \\(\\widehat{y}\\) with a square, and the residual \\(y - \\widehat{y}\\) with an arrow. We re-display Figure 17.6 in the top-left plot of Figure 17.12 in addition to three more arbitrarily chosen course instructors: FIGURE 17.12: Example of observed value, fitted value, and residual. The three other plots refer to: A course whose instructor had a “beauty” score \\(x\\) = 2.333 and teaching score \\(y\\) = 2.7. The residual in this case is \\(2.7 - 4.036 = -1.336\\), which we mark with a new blue arrow in the top-right plot. A course whose instructor had a “beauty” score \\(x = 3.667\\) and teaching score \\(y = 4.4\\). The residual in this case is \\(4.4 - 4.125 = 0.2753\\), which we mark with a new blue arrow in the bottom-left plot. A course whose instructor had a “beauty” score \\(x = 6\\) and teaching score \\(y = 3.8\\). The residual in this case is \\(3.8 - 4.28 = -0.4802\\), which we mark with a new blue arrow in the bottom-right plot. Now say we repeated this process of computing residuals for all 463 courses’ instructors, then we squared all the residuals, and then we summed them. We call this quantity the sum of squared residuals; it is a measure of the lack of fit of a model. Larger values of the sum of squared residuals indicate a bigger lack of fit. This corresponds to a worse fitting model. If the regression line fits all the points perfectly, then the sum of squared residuals is 0. This is because if the regression line fits all the points perfectly, then the fitted value \\(\\widehat{y}\\) equals the observed value \\(y\\) in all cases, and hence the residual \\(y-\\widehat{y}\\) = 0 in all cases, and the sum of even a large number of 0’s is still 0. Furthermore, of all possible lines we can draw through the cloud of 463 points, the regression line minimizes this value. In other words, the regression and its corresponding fitted values \\(\\widehat{y}\\) minimizes the sum of the squared residuals: \\[ \\sum_{i=1}^{n}(y_i - \\widehat{y}_i)^2 \\] Let’s use our data wrangling tools from Chapter 13 to compute the sum of squared residuals exactly: # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch5) # Get regression points: regression_points &lt;- get_regression_points(score_model) regression_points # A tibble: 463 × 5 ID score bty_avg score_hat residual &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 4.7 5 4.214 0.486 2 2 4.1 5 4.214 -0.114 3 3 3.9 5 4.214 -0.314 4 4 4.8 5 4.214 0.586 5 5 4.6 3 4.08 0.52 6 6 4.3 3 4.08 0.22 7 7 2.8 3 4.08 -1.28 8 8 4.1 3.333 4.102 -0.002 9 9 3.4 3.333 4.102 -0.702 10 10 4.5 3.167 4.091 0.409 # ℹ 453 more rows # Compute sum of squared residuals regression_points %&gt;% mutate(squared_residuals = residual^2) %&gt;% summarize(sum_of_squared_residuals = sum(squared_residuals)) # A tibble: 1 × 1 sum_of_squared_residuals &lt;dbl&gt; 1 131.879 Any other straight line drawn in the figure would yield a sum of squared residuals greater than 132. This is a mathematically guaranteed fact that you can prove using calculus and linear algebra. That’s why alternative names for the linear regression line are the best-fitting line and the least-squares line. Why do we square the residuals (i.e., the arrow lengths)? So that both positive and negative deviations of the same amount are treated equally. (That being said, while taking the absolute value of the residuals would also treat both positive and negative deviations of the same amount equally, squaring the residuals is used for reasons related to calculus: taking derivatives and minimizing functions. To learn more we suggest you consult a textbook on mathematical statistics.) Learning check (LC5.8) Note in Figure 17.13 there are 3 points marked with dots and: The “best” fitting solid regression line in blue An arbitrarily chosen dotted red line Another arbitrarily chosen dashed green line FIGURE 17.13: Regression line and two others. Compute the sum of squared residuals by hand for each line and show that of these three lines, the regression line in blue has the smallest value. 17.3.3 get_regression_x() functions Recall in this chapter we introduced two functions from the moderndive package: get_regression_table() that returns a regression table in Subsection 17.1.2 and get_regression_points() that returns point-by-point information from a regression model in Subsection 17.1.3. What is going on behind the scenes with the get_regression_table() and get_regression_points() functions? We mentioned in Subsection 17.1.2 that these were examples of wrapper functions. Such functions take other pre-existing functions and “wrap” them into single functions that hide the user from their inner workings. This way all the user needs to worry about is what the inputs look like and what the outputs look like. In this subsection, we’ll “get under the hood” of these functions and see how the “engine” of these wrapper functions works. Recall our two-step process to generate a regression table from Subsection 17.1.2: # Fit regression model: score_model &lt;- lm(formula = score ~ bty_avg, data = evals_ch5) # Get regression table: get_regression_table(score_model) TABLE 17.10: Regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 The get_regression_table() wrapper function takes two pre-existing functions in other R packages: tidy() from the broom package (Robinson, Hayes, and Couch 2023) and clean_names() from the janitor package (Firke 2023) and “wraps” them into a single function that takes in a saved lm() linear model, here score_model, and returns a regression table saved as a “tidy” data frame. Here is how we used the tidy() and clean_names() functions to produce Table 17.11: library(broom) library(janitor) score_model %&gt;% tidy(conf.int = TRUE) %&gt;% mutate_if(is.numeric, round, digits = 3) %&gt;% clean_names() %&gt;% rename(lower_ci = conf_low, upper_ci = conf_high) TABLE 17.11: Regression table using tidy() from broom package term estimate std_error statistic p_value lower_ci upper_ci (Intercept) 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Yikes! That’s a lot of code! So, in order to simplify your lives, we made the editorial decision to “wrap” all the code into get_regression_table(), freeing you from the need to understand the inner workings of the function. Note that the mutate_if() function is from the dplyr package and applies the round() function to three significant digits precision only to those variables that are numerical. Similarly, the get_regression_points() function is another wrapper function, but this time returning information about the individual points involved in a regression model like the fitted values, observed values, and the residuals. get_regression_points() uses the augment() function in the broom package instead of the tidy() function as with get_regression_table() to produce the data shown in Table 17.12: library(broom) library(janitor) score_model %&gt;% augment() %&gt;% mutate_if(is.numeric, round, digits = 3) %&gt;% clean_names() %&gt;% select(-c(&quot;std_resid&quot;, &quot;hat&quot;, &quot;sigma&quot;, &quot;cooksd&quot;, &quot;std_resid&quot;)) TABLE 17.12: Regression points using augment() from broom package score bty_avg fitted resid 4.7 5.00 4.21 0.486 4.1 5.00 4.21 -0.114 3.9 5.00 4.21 -0.314 4.8 5.00 4.21 0.586 4.6 3.00 4.08 0.520 4.3 3.00 4.08 0.220 2.8 3.00 4.08 -1.280 4.1 3.33 4.10 -0.002 3.4 3.33 4.10 -0.702 4.5 3.17 4.09 0.409 In this case, it outputs only the variables of interest to students learning regression: the outcome variable \\(y\\) (score), all explanatory/predictor variables (bty_avg), all resulting fitted values \\(\\hat{y}\\) used by applying the equation of the regression line to bty_avg, and the residual \\(y - \\hat{y}\\). If you’re even more curious about how these and other wrapper functions work, take a look at the source code for these functions on GitHub. 17.4 Conclusion 17.4.1 Additional resources An R script file of all R code used in this chapter is available here. As we suggested in Subsection 17.1.1, interpreting coefficients that are not close to the extreme values of -1, 0, and 1 can be somewhat subjective. To help develop your sense of correlation coefficients, we suggest you play the 80s-style video game called, “Guess the Correlation”, at http://guessthecorrelation.com/. FIGURE 17.14: Preview of “Guess the Correlation” game. 17.4.2 What’s to come? In this chapter, you’ve studied the term basic regression, where you fit models that only have one explanatory variable. In Chapter 19, we’ll study multiple regression, where our regression models can now have more than one explanatory variable! In particular, we’ll consider two scenarios: regression models with one numerical and one categorical explanatory variable and regression models with two numerical explanatory variables. This will allow you to construct more sophisticated and more powerful models, all in the hopes of better explaining your outcome variable \\(y\\). References "],["18-inference-for-regression.html", "Chapter 18 Inference for Regression 18.1 Regression refresher 18.2 Interpreting regression tables 18.3 Conditions for inference for regression 18.4 Simulation-based inference for regression 18.5 Conclusion", " Chapter 18 Inference for Regression In our penultimate chapter, we’ll revisit the regression models we first studied in Chapters 17 and 19. Armed with our knowledge of confidence intervals and hypothesis tests from Chapters 15 and 16, we’ll be able to apply statistical inference to further our understanding of relationships between outcome and explanatory variables. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section ?? that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section ?? for information on how to install and load R packages. library(tidyverse) library(moderndive) library(infer) 18.1 Regression refresher Before jumping into inference for regression, let’s remind ourselves of the University of Texas Austin teaching evaluations analysis in Section 17.1. 18.1.1 Teaching evaluations analysis Recall using simple linear regression we modeled the relationship between A numerical outcome variable \\(y\\) (the instructor’s teaching score) and A single numerical explanatory variable \\(x\\) (the instructor’s “beauty” score). We first created an evals_ch5 data frame that selected a subset of variables from the evals data frame included in the moderndive package. This evals_ch5 data frame contains only the variables of interest for our analysis, in particular the instructor’s teaching score and the “beauty” rating bty_avg: evals_ch5 &lt;- evals %&gt;% select(ID, score, bty_avg, age) glimpse(evals_ch5) Rows: 463 Columns: 4 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… $ score &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4.… $ bty_avg &lt;dbl&gt; 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, 3.17, 3.… $ age &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 40… In Subsection 17.1.1, we performed an exploratory data analysis of the relationship between these two variables of score and bty_avg. We saw there that a weakly positive correlation of 0.187 existed between the two variables. This was evidenced in Figure 18.1 of the scatterplot along with the “best-fitting” regression line that summarizes the linear relationship between the two variables of score and bty_avg. Recall in Subsection 17.3.2 that we defined a “best-fitting” line as the line that minimizes the sum of squared residuals. ggplot(evals_ch5, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship between teaching and beauty scores&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 18.1: Relationship with regression line. Looking at this plot again, you might be asking, “Does that line really have all that positive of a slope?”. It does increase from left to right as the bty_avg variable increases, but by how much? To get to this information, recall that we followed a two-step procedure: We first “fit” the linear regression model using the lm() function with the formula score ~ bty_avg. We saved this model in score_model. We get the regression table by applying the get_regression_table() function from the moderndive package to score_model. # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch5) # Get regression table: get_regression_table(score_model) TABLE 18.1: Previously seen linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Using the values in the estimate column of the resulting regression table in Table 18.1, we could then obtain the equation of the “best-fitting” regression line in Figure 18.1: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x\\\\ \\widehat{\\text{score}} &amp;= b_0 + b_{\\text{bty}\\_\\text{avg}} \\cdot\\text{bty}\\_\\text{avg}\\\\ &amp;= 3.880 + 0.067\\cdot\\text{bty}\\_\\text{avg} \\end{aligned} \\] where \\(b_0\\) is the fitted intercept and \\(b_1\\) is the fitted slope for bty_avg. Recall the interpretation of the \\(b_1\\) = 0.067 value of the fitted slope: For every increase of one unit in “beauty” rating, there is an associated increase, on average, of 0.067 units of evaluation score. Thus, the slope value quantifies the relationship between the \\(y\\) variable score and the \\(x\\) variable bty_avg. We also discussed the intercept value of \\(b_0\\) = 3.88 and its lack of practical interpretation, since the range of possible “beauty” scores does not include 0. 18.1.2 Sampling scenario Let’s now revisit this study in terms of the terminology and notation related to sampling we studied in Subsection 14.3.1. First, let’s view the instructors for these 463 courses as a representative sample from a greater study population. In our case, let’s assume that the study population is all instructors at UT Austin and that the sample of instructors who taught these 463 courses is a representative sample. Unfortunately, we can only assume these two facts without more knowledge of the sampling methodology used by the researchers. Since we are viewing these \\(n\\) = 463 courses as a sample, we can view our fitted slope \\(b_1\\) = 0.067 as a point estimate of the population slope \\(\\beta_1\\). In other words, \\(\\beta_1\\) quantifies the relationship between teaching score and “beauty” average bty_avg for all instructors at UT Austin. Similarly, we can view our fitted intercept \\(b_0\\) = 3.88 as a point estimate of the population intercept \\(\\beta_0\\) for all instructors at UT Austin. Putting these two ideas together, we can view the equation of the fitted line \\(\\widehat{y}\\) = \\(b_0 + b_1 \\cdot x\\) = \\(3.880 + 0.067 \\cdot \\text{bty}\\_\\text{avg}\\) as an estimate of some true and unknown population line \\(y = \\beta_0 + \\beta_1 \\cdot x\\). Thus we can draw parallels between our teaching evaluations analysis and all the sampling scenarios we’ve seen previously. In this chapter, we’ll focus on the final scenario of regression slopes as shown in Table 18.2. TABLE 18.2: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) or \\(\\widehat{\\mu}_1 - \\widehat{\\mu}_2\\) 5 Population regression slope \\(\\beta_1\\) Fitted regression slope \\(b_1\\) or \\(\\widehat{\\beta}_1\\) Since we are now viewing our fitted slope \\(b_1\\) and fitted intercept \\(b_0\\) as point estimates based on a sample, these estimates will again be subject to sampling variability. In other words, if we collected a new sample of data on a different set of \\(n\\) = 463 courses and their instructors, the new fitted slope \\(b_1\\) will likely differ from 0.067. The same goes for the new fitted intercept \\(b_0\\). But by how much will these estimates vary? This information is in the remaining columns of the regression table in Table 18.1. Our knowledge of sampling from Chapter 14, confidence intervals from Chapter 15, and hypothesis tests from Chapter 16 will help us interpret these remaining columns. 18.2 Interpreting regression tables We’ve so far focused only on the two leftmost columns of the regression table in Table 18.1: term and estimate. Let’s now shift our attention to the remaining columns: std_error, statistic, p_value, lower_ci and upper_ci in Table 18.3. TABLE 18.3: Previously seen regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Given the lack of practical interpretation for the fitted intercept \\(b_0\\), in this section we’ll focus only on the second row of the table corresponding to the fitted slope \\(b_1\\). We’ll first interpret the std_error, statistic, p_value, lower_ci and upper_ci columns. Afterwards in the upcoming Subsection 18.2.5, we’ll discuss how R computes these values. 18.2.1 Standard error The third column of the regression table in Table 18.1 std_error corresponds to the standard error of our estimates. Recall the definition of standard error we saw in Subsection 14.3.2: The standard error is the standard deviation of any point estimate computed from a sample. So what does this mean in terms of the fitted slope \\(b_1\\) = 0.067? This value is just one possible value of the fitted slope resulting from this particular sample of \\(n\\) = 463 pairs of teaching and “beauty” scores. However, if we collected a different sample of \\(n\\) = 463 pairs of teaching and “beauty” scores, we will almost certainly obtain a different fitted slope \\(b_1\\). This is due to sampling variability. Say we hypothetically collected 1000 such samples of pairs of teaching and “beauty” scores, computed the 1000 resulting values of the fitted slope \\(b_1\\), and visualized them in a histogram. This would be a visualization of the sampling distribution of \\(b_1\\), which we defined in Subsection 14.3.2. Further recall that the standard deviation of the sampling distribution of \\(b_1\\) has a special name: the standard error. Recall that we constructed three sampling distributions for the sample proportion \\(\\widehat{p}\\) using shovels of size 25, 50, and 100 in Figure 14.12. We observed that as the sample size increased, the standard error decreased as evidenced by the narrowing sampling distribution. The standard error of \\(b_1\\) similarly quantifies how much variation in the fitted slope \\(b_1\\) one would expect between different samples. So in our case, we can expect about 0.016 units of variation in the bty_avg slope variable. Recall that the estimate and std_error values play a key role in inferring the value of the unknown population slope \\(\\beta_1\\) relating to all instructors. In Section 18.4, we’ll perform a simulation using the infer package to construct the bootstrap distribution for \\(b_1\\) in this case. Recall from Subsection 15.7.1 that the bootstrap distribution is an approximation to the sampling distribution in that they have a similar shape. Since they have a similar shape, they have similar standard errors. However, unlike the sampling distribution, the bootstrap distribution is constructed from a single sample, which is a practice more aligned with what’s done in real life. 18.2.2 Test statistic The fourth column of the regression table in Table 18.1 statistic corresponds to a test statistic relating to the following hypothesis test: \\[ \\begin{aligned} H_0 &amp;: \\beta_1 = 0\\\\ \\text{vs } H_A&amp;: \\beta_1 \\neq 0. \\end{aligned} \\] Recall our terminology, notation, and definitions related to hypothesis tests we introduced in Section 16.2. A hypothesis test consists of a test between two competing hypotheses: (1) a null hypothesis \\(H_0\\) versus (2) an alternative hypothesis \\(H_A\\). A test statistic is a point estimate/sample statistic formula used for hypothesis testing. Here, our null hypothesis \\(H_0\\) assumes that the population slope \\(\\beta_1\\) is 0. If the population slope \\(\\beta_1\\) is truly 0, then this is saying that there is no true relationship between teaching and “beauty” scores for all the instructors in our population. In other words, \\(x\\) = “beauty” score would have no associated effect on \\(y\\) = teaching score. The alternative hypothesis \\(H_A\\), on the other hand, assumes that the population slope \\(\\beta_1\\) is not 0, meaning it could be either positive or negative. This suggests either a positive or negative relationship between teaching and “beauty” scores. Recall we called such alternative hypotheses two-sided. By convention, all hypothesis testing for regression assumes two-sided alternatives. Recall our “hypothesized universe” of no gender discrimination we assumed in our promotions activity in Section 16.1. Similarly here when conducting this hypothesis test, we’ll assume a “hypothesized universe” where there is no relationship between teaching and “beauty” scores. In other words, we’ll assume the null hypothesis \\(H_0: \\beta_1 = 0\\) is true. The statistic column in the regression table is a tricky one, however. It corresponds to a standardized t-test statistic, much like the two-sample \\(t\\) statistic we saw in Subsection 16.6.1 where we used a theory-based method for conducting hypothesis tests. In both these cases, the null distribution can be mathematically proven to be a \\(t\\)-distribution. Since such test statistics are tricky for individuals new to statistical inference to study, we’ll skip this and jump into interpreting the \\(p\\)-value. If you’re curious, we have included a discussion of this standardized t-test statistic in Subsection 18.5.1. 18.2.3 p-value The fifth column of the regression table in Table 18.1 p_value corresponds to the p-value of the hypothesis test \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\). Again recalling our terminology, notation, and definitions related to hypothesis tests we introduced in Section 16.2, let’s focus on the definition of the \\(p\\)-value: A p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. Recall that you can intuitively think of the \\(p\\)-value as quantifying how “extreme” the observed fitted slope of \\(b_1\\) = 0.067 is in a “hypothesized universe” where there is no relationship between teaching and “beauty” scores. Following the hypothesis testing procedure we outlined in Section 16.4, since the \\(p\\)-value in this case is 0, for any choice of significance level \\(\\alpha\\) we would reject \\(H_0\\) in favor of \\(H_A\\). Using non-statistical language, this is saying: we reject the hypothesis that there is no relationship between teaching and “beauty” scores in favor of the hypothesis that there is. That is to say, the evidence suggests there is a significant relationship, one that is positive. More precisely, however, the \\(p\\)-value corresponds to how extreme the observed test statistic of 4.09 is when compared to the appropriate null distribution. In Section 18.4, we’ll perform a simulation using the infer package to construct the null distribution in this case. An extra caveat here is that the results of this hypothesis test are only valid if certain “conditions for inference for regression” are met, which we’ll introduce shortly in Section 18.3. 18.2.4 Confidence interval The two rightmost columns of the regression table in Table 18.1 (lower_ci and upper_ci) correspond to the endpoints of the 95% confidence interval for the population slope \\(\\beta_1\\). Recall our analogy of “nets are to fish” what “confidence intervals are to population parameters” from Section 15.3. The resulting 95% confidence interval for \\(\\beta_1\\) of (0.035, 0.099) can be thought of as a range of plausible values for the population slope \\(\\beta_1\\) of the linear relationship between teaching and “beauty” scores. As we introduced in Subsection 15.5.2 on the precise and shorthand interpretation of confidence intervals, the statistically precise interpretation of this confidence interval is: “if we repeated this sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population slope \\(\\beta_1\\).” However, we’ll summarize this using our shorthand interpretation that “we’re 95% ‘confident’ that the true population slope \\(\\beta_1\\) lies between 0.035 and 0.099.” Notice in this case that the resulting 95% confidence interval for \\(\\beta_1\\) of \\((0.035, \\, 0.099)\\) does not contain a very particular value: \\(\\beta_1\\) equals 0. Recall we mentioned that if the population regression slope \\(\\beta_1\\) is 0, this is equivalent to saying there is no relationship between teaching and “beauty” scores. Since \\(\\beta_1\\) = 0 is not in our plausible range of values for \\(\\beta_1\\), we are inclined to believe that there, in fact, is a relationship between teaching and “beauty” scores and a positive one at that. So in this case, the conclusion about the population slope \\(\\beta_1\\) from the 95% confidence interval matches the conclusion from the hypothesis test: evidence suggests that there is a meaningful relationship between teaching and “beauty” scores. Recall from Subsection 15.5.3, however, that the confidence level is one of many factors that determine confidence interval widths. So for example, say we used a higher confidence level of 99% instead of 95%. The resulting confidence interval for \\(\\beta_1\\) would be wider and thus might now include 0. The lesson to remember here is that any confidence-interval-based conclusion depends highly on the confidence level used. What are the calculations that went into computing the two endpoints of the 95% confidence interval for \\(\\beta_1\\)? Recall our sampling bowl example from Subsection 15.7.2 discussing lower_ci and upper_ci. Since the sampling and bootstrap distributions of the sample proportion \\(\\widehat{p}\\) were roughly normal, we could use the rule of thumb for bell-shaped distributions from Appendix ?? to create a 95% confidence interval for \\(p\\) with the following equation: \\[\\widehat{p} \\pm \\text{MoE}_{\\widehat{p}} = \\widehat{p} \\pm 1.96 \\cdot \\text{SE}_{\\widehat{p}} = \\widehat{p} \\pm 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] We can generalize this to other point estimates that have roughly normally shaped sampling and/or bootstrap distributions: \\[\\text{point estimate} \\pm \\text{MoE} = \\text{point estimate} \\pm 1.96 \\cdot \\text{SE}.\\] We’ll show in Section 18.4 that the sampling/bootstrap distribution for the fitted slope \\(b_1\\) is in fact bell-shaped as well. Thus we can construct a 95% confidence interval for \\(\\beta_1\\) with the following equation: \\[b_1 \\pm \\text{MoE}_{b_1} = b_1 \\pm 1.96 \\cdot \\text{SE}_{b_1}.\\] What is the value of the standard error \\(\\text{SE}_{b_1}\\)? It is in fact in the third column of the regression table in Table 18.1: 0.016. Thus \\[ \\begin{aligned} b_1 \\pm 1.96 \\cdot \\text{SE}_{b_1} &amp;= 0.067 \\pm 1.96 \\cdot 0.016 = 0.067 \\pm 0.031\\\\ &amp;= (0.036, 0.098) \\end{aligned} \\] This closely matches the \\((0.035, 0.099)\\) confidence interval in the last two columns of Table 18.1. Much like hypothesis tests, however, the results of this confidence interval also are only valid if the “conditions for inference for regression” to be discussed in Section 18.3 are met. 18.2.5 How does R compute the table? Since we didn’t perform the simulation to get the values of the standard error, test statistic, \\(p\\)-value, and endpoints of the 95% confidence interval in Table 18.1, you might be wondering how were these values computed. What did R do behind the scenes? Does R run simulations like we did using the infer package in Chapters 15 and 16 on confidence intervals and hypothesis testing? The answer is no! Much like the theory-based method for constructing confidence intervals you saw in Subsection 15.7.2 and the theory-based hypothesis test you saw in Subsection 16.6.1, there exist mathematical formulas that allow you to construct confidence intervals and conduct hypothesis tests for inference for regression. These formulas were derived in a time when computers didn’t exist, so it would’ve been impossible to run the extensive computer simulations we have in this book. We present these formulas in Subsection 18.5.1 on “theory-based inference for regression.” In Section 18.4, we’ll go over a simulation-based approach to constructing confidence intervals and conducting hypothesis tests using the infer package. In particular, we’ll convince you that the bootstrap distribution of the fitted slope \\(b_1\\) is indeed bell-shaped. 18.3 Conditions for inference for regression Recall in Subsection 15.3.2 we stated that we could only use the standard-error-based method for constructing confidence intervals if the bootstrap distribution was bell shaped. Similarly, there are certain conditions that need to be met in order for the results of our hypothesis tests and confidence intervals we described in Section 18.2 to have valid meaning. These conditions must be met for the assumed underlying mathematical and probability theory to hold true. For inference for regression, there are four conditions that need to be met. Note the first four letters of these conditions are highlighted in bold in what follows: LINE. This can serve as a nice reminder of what to check for whenever you perform linear regression. Linearity of relationship between variables Independence of the residuals Normality of the residuals Equality of variance of the residuals Conditions L, N, and E can be verified through what is known as a residual analysis. Condition I can only be verified through an understanding of how the data was collected. In this section, we’ll go over a refresher on residuals, verify whether each of the four LINE conditions hold true, and then discuss the implications. 18.3.1 Residuals refresher Recall our definition of a residual from Subsection 17.1.3: it is the observed value minus the fitted value denoted by \\(y - \\widehat{y}\\). Recall that residuals can be thought of as the error or the “lack-of-fit” between the observed value \\(y\\) and the fitted value \\(\\widehat{y}\\) on the regression line in Figure 18.1. In Figure 18.2, we illustrate one particular residual out of 463 using an arrow, as well as its corresponding observed and fitted values using a circle and a square, respectively. FIGURE 18.2: Example of observed value, fitted value, and residual. Furthermore, we can automate the calculation of all \\(n\\) = 463 residuals by applying the get_regression_points() function to our saved regression model in score_model. Observe how the resulting values of residual are roughly equal to score - score_hat (there is potentially a slight difference due to rounding error). # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch5) # Get regression points: regression_points &lt;- get_regression_points(score_model) regression_points # A tibble: 463 × 5 ID score bty_avg score_hat residual &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 4.7 5 4.214 0.486 2 2 4.1 5 4.214 -0.114 3 3 3.9 5 4.214 -0.314 4 4 4.8 5 4.214 0.586 5 5 4.6 3 4.08 0.52 6 6 4.3 3 4.08 0.22 7 7 2.8 3 4.08 -1.28 8 8 4.1 3.333 4.102 -0.002 9 9 3.4 3.333 4.102 -0.702 10 10 4.5 3.167 4.091 0.409 # ℹ 453 more rows A residual analysis is used to verify conditions L, N, and E and can be performed using appropriate data visualizations. While there are more sophisticated statistical approaches that can also be done, we’ll focus on the much simpler approach of looking at plots. 18.3.2 Linearity of relationship The first condition is that the relationship between the outcome variable \\(y\\) and the explanatory variable \\(x\\) must be Linear. Recall the scatterplot in Figure 18.1 where we had the explanatory variable \\(x\\) as “beauty” score and the outcome variable \\(y\\) as teaching score. Would you say that the relationship between \\(x\\) and \\(y\\) is linear? It’s hard to say because of the scatter of the points about the line. In the authors’ opinions, we feel this relationship is “linear enough.” Let’s present an example where the relationship between \\(x\\) and \\(y\\) is clearly not linear in Figure 18.3. In this case, the points clearly do not form a line, but rather a U-shaped polynomial curve. In this case, any results from an inference for regression would not be valid. FIGURE 18.3: Example of a clearly non-linear relationship. 18.3.3 Independence of residuals The second condition is that the residuals must be Independent. In other words, the different observations in our data must be independent of one another. For our UT Austin data, while there is data on 463 courses, these 463 courses were actually taught by 94 unique instructors. In other words, the same professor is often included more than once in our data. The original evals data frame that we used to construct the evals_ch5 data frame has a variable prof_ID, which is an anonymized identification variable for the professor: evals %&gt;% select(ID, prof_ID, score, bty_avg) # A tibble: 463 × 4 ID prof_ID score bty_avg &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 1 4.7 5 2 2 1 4.1 5 3 3 1 3.9 5 4 4 1 4.8 5 5 5 2 4.6 3 6 6 2 4.3 3 7 7 2 2.8 3 8 8 3 4.1 3.333 9 9 3 3.4 3.333 10 10 4 4.5 3.167 # ℹ 453 more rows For example, the professor with prof_ID equal to 1 taught the first 4 courses in the data, the professor with prof_ID equal to 2 taught the next 3, and so on. Given that the same professor taught these first four courses, it is reasonable to expect that these four teaching scores are related to each other. If a professor gets a high score in one class, chances are fairly good they’ll get a high score in another. This dataset thus provides different information than if we had 463 unique instructors teaching the 463 courses. In this case, we say there exists dependence between observations. The first four courses taught by professor 1 are dependent, the next 3 courses taught by professor 2 are related, and so on. Any proper analysis of this data needs to take into account that we have repeated measures for the same profs. So in this case, the independence condition is not met. What does this mean for our analysis? We’ll address this in Subsection 18.3.6 coming up, after we check the remaining two conditions. 18.3.4 Normality of residuals The third condition is that the residuals should follow a Normal distribution. Furthermore, the center of this distribution should be 0. In other words, sometimes the regression model will make positive errors: \\(y - \\widehat{y} &gt; 0\\). Other times, the regression model will make equally negative errors: \\(y - \\widehat{y} &lt; 0\\). However, on average the errors should equal 0 and their shape should be similar to that of a bell. The simplest way to check the normality of the residuals is to look at a histogram, which we visualize in Figure 18.4. ggplot(regression_points, aes(x = residual)) + geom_histogram(binwidth = 0.25, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) FIGURE 18.4: Histogram of residuals. This histogram shows that we have more positive residuals than negative. Since the residual \\(y-\\widehat{y}\\) is positive when \\(y &gt; \\widehat{y}\\), it seems our regression model’s fitted teaching scores \\(\\widehat{y}\\) tend to underestimate the true teaching scores \\(y\\). Furthermore, this histogram has a slight left-skew in that there is a tail on the left. This is another way to say the residuals exhibit a negative skew. Is this a problem? Again, there is a certain amount of subjectivity in the response. In the authors’ opinion, while there is a slight skew to the residuals, we feel it isn’t drastic. On the other hand, others might disagree with our assessment. Let’s present examples where the residuals clearly do and don’t follow a normal distribution in Figure 18.5. In this case of the model yielding the clearly non-normal residuals on the right, any results from an inference for regression would not be valid. FIGURE 18.5: Example of clearly normal and clearly not normal residuals. 18.3.5 Equality of variance The fourth and final condition is that the residuals should exhibit Equal variance across all values of the explanatory variable \\(x\\). In other words, the value and spread of the residuals should not depend on the value of the explanatory variable \\(x\\). Recall the scatterplot in Figure 18.1: we had the explanatory variable \\(x\\) of “beauty” score on the x-axis and the outcome variable \\(y\\) of teaching score on the y-axis. Instead, let’s create a scatterplot that has the same values on the x-axis, but now with the residual \\(y-\\widehat{y}\\) on the y-axis as seen in Figure 18.6. ggplot(regression_points, aes(x = bty_avg, y = residual)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Residual&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;, size = 1) FIGURE 18.6: Plot of residuals over beauty score. You can think of Figure 18.6 as a modified version of the plot with the regression line in Figure 18.1, but with the regression line flattened out to \\(y=0\\). Looking at this plot, would you say that the spread of the residuals around the line at \\(y=0\\) is constant across all values of the explanatory variable \\(x\\) of “beauty” score? This question is rather qualitative and subjective in nature, thus different people may respond with different answers. For example, some people might say that there is slightly more variation in the residuals for smaller values of \\(x\\) than for higher ones. However, it can be argued that there isn’t a drastic non-constancy. In Figure 18.7 let’s present an example where the residuals clearly do not have equal variance across all values of the explanatory variable \\(x\\). FIGURE 18.7: Example of clearly non-equal variance. Observe how the spread of the residuals increases as the value of \\(x\\) increases. This is a situation known as heteroskedasticity. Any inference for regression based on a model yielding such a pattern in the residuals would not be valid. 18.3.6 What’s the conclusion? Let’s list our four conditions for inference for regression again and indicate whether or not they were satisfied in our analysis: Linearity of relationship between variables: Yes Independence of residuals: No Normality of residuals: Somewhat Equality of variance: Yes So what does this mean for the results of our confidence intervals and hypothesis tests in Section 18.2? First, the Independence condition. The fact that there exist dependencies between different rows in evals_ch5 must be addressed. In more advanced statistics courses, you’ll learn how to incorporate such dependencies into your regression models. One such technique is called hierarchical/multilevel modeling. Second, when conditions L, N, E are not met, it often means there is a shortcoming in our model. For example, it may be the case that using only a single explanatory variable is insufficient, as we did with “beauty” score. We may need to incorporate more explanatory variables in a multiple regression model as we did in Chapter 19, or perhaps use a transformation of one or more of your variables, or use an entirely different modeling technique. To learn more about addressing such shortcomings, you’ll have to take a class on or read up on more advanced regression modeling methods. In our case, the best we can do is view the results suggested by our confidence intervals and hypothesis tests as preliminary. While a preliminary analysis suggests that there is a significant relationship between teaching and “beauty” scores, further investigation is warranted; in particular, by improving the preliminary score ~ bty_avg model so that the four conditions are met. When the four conditions are roughly met, then we can put more faith into our confidence intervals and \\(p\\)-values. The conditions for inference in regression problems are a key part of regression analysis that are of vital importance to the processes of constructing confidence intervals and conducting hypothesis tests. However, it is often the case with regression analysis in the real world that not all the conditions are completely met. Furthermore, as you saw, there is a level of subjectivity in the residual analyses to verify the L, N, and E conditions. So what can you do? We as authors advocate for transparency in communicating all results. This lets the stakeholders of any analysis know about a model’s shortcomings or whether the model is “good enough.” So while this checking of assumptions has lead to some fuzzy “it depends” results, we decided as authors to show you these scenarios to help prepare you for difficult statistical decisions you may need to make down the road. Learning check (LC10.1) Continuing with our regression using age as the explanatory variable and teaching score as the outcome variable. Use the get_regression_points() function to get the observed values, fitted values, and residuals for all 463 instructors. Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here. 18.4 Simulation-based inference for regression Recall in Subsection 18.2.5 when we interpreted the third through seventh columns of a regression table, we stated that R doesn’t do simulations to compute these values. Rather R uses theory-based methods that involve mathematical formulas. In this section, we’ll use the simulation-based methods you previously learned in Chapters 15 and 16 to recreate the values in the regression table in Table 18.1. In particular, we’ll use the infer package workflow to Construct a 95% confidence interval for the population slope \\(\\beta_1\\) using bootstrap resampling with replacement. We did this previously in Sections 15.4 with the pennies data and 15.6 with the mythbusters_yawn data. Conduct a hypothesis test of \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\) using a permutation test. We did this previously in Sections 16.3 with the promotions data and 16.5 with the movies_sample IMDb data. 18.4.1 Confidence interval for slope We’ll construct a 95% confidence interval for \\(\\beta_1\\) using the infer workflow outlined in Subsection 15.4.2. Specifically, we’ll first construct the bootstrap distribution for the fitted slope \\(b_1\\) using our single sample of 463 courses: specify() the variables of interest in evals_ch5 with the formula: score ~ bty_avg. generate() replicates by using bootstrap resampling with replacement from the original sample of 463 courses. We generate reps = 1000 replicates using type = \"bootstrap\". calculate() the summary statistic of interest: the fitted slope \\(b_1\\). Using this bootstrap distribution, we’ll construct the 95% confidence interval using the percentile method and (if appropriate) the standard error method as well. It is important to note in this case that the bootstrapping with replacement is done row-by-row. Thus, the original pairs of score and bty_avg values are always kept together, but different pairs of score and bty_avg values may be resampled multiple times. The resulting confidence interval will denote a range of plausible values for the unknown population slope \\(\\beta_1\\) quantifying the relationship between teaching and “beauty” scores for all professors at UT Austin. Let’s first construct the bootstrap distribution for the fitted slope \\(b_1\\): bootstrap_distn_slope &lt;- evals_ch5 %&gt;% specify(formula = score ~ bty_avg) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;slope&quot;) bootstrap_distn_slope # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.0651055 2 2 0.0382313 3 3 0.108056 4 4 0.0666601 5 5 0.0715932 6 6 0.0854565 7 7 0.0624868 8 8 0.0412859 9 9 0.0796269 10 10 0.0761299 # ℹ 990 more rows Observe how we have 1000 values of the bootstrapped slope \\(b_1\\) in the stat column. Let’s visualize the 1000 bootstrapped values in Figure 18.8. visualize(bootstrap_distn_slope) FIGURE 18.8: Bootstrap distribution of slope. Observe how the bootstrap distribution is roughly bell-shaped. Recall from Subsection 15.7.1 that the shape of the bootstrap distribution of \\(b_1\\) closely approximates the shape of the sampling distribution of \\(b_1\\). Percentile-method First, let’s compute the 95% confidence interval for \\(\\beta_1\\) using the percentile method. We’ll do so by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped. percentile_ci &lt;- bootstrap_distn_slope %&gt;% get_confidence_interval(type = &quot;percentile&quot;, level = 0.95) percentile_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.0323411 0.0990027 The resulting percentile-based 95% confidence interval for \\(\\beta_1\\) of (0.032, 0.099) is similar to the confidence interval in the regression Table 18.1 of (0.035, 0.099). Standard error method Since the bootstrap distribution in Figure 18.8 appears to be roughly bell-shaped, we can also construct a 95% confidence interval for \\(\\beta_1\\) using the standard error method. In order to do this, we need to first compute the fitted slope \\(b_1\\), which will act as the center of our standard error-based confidence interval. While we saw in the regression table in Table 18.1 that this was \\(b_1\\) = 0.067, we can also use the infer pipeline with the generate() step removed to calculate it: observed_slope &lt;- evals %&gt;% specify(score ~ bty_avg) %&gt;% calculate(stat = &quot;slope&quot;) observed_slope Response: score (numeric) Explanatory: bty_avg (numeric) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 0.0666370 We then use the get_ci() function with level = 0.95 to compute the 95% confidence interval for \\(\\beta_1\\). Note that setting the point_estimate argument to the observed_slope of 0.067 sets the center of the confidence interval. se_ci &lt;- bootstrap_distn_slope %&gt;% get_ci(level = 0.95, type = &quot;se&quot;, point_estimate = observed_slope) se_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.0333767 0.0998974 The resulting standard error-based 95% confidence interval for \\(\\beta_1\\) of \\((0.033, 0.1)\\) is slightly different than the confidence interval in the regression Table 18.1 of \\((0.035, 0.099)\\). Comparing all three Let’s compare all three confidence intervals in Figure 18.9, where the percentile-based confidence interval is marked with solid lines, the standard error based confidence interval is marked with dashed lines, and the theory-based confidence interval (0.035, 0.099) from the regression table in Table 18.1 is marked with dotted lines. visualize(bootstrap_distn_slope) + shade_confidence_interval(endpoints = percentile_ci, fill = NULL, linetype = &quot;solid&quot;, color = &quot;grey90&quot;) + shade_confidence_interval(endpoints = se_ci, fill = NULL, linetype = &quot;dashed&quot;, color = &quot;grey60&quot;) + shade_confidence_interval(endpoints = c(0.035, 0.099), fill = NULL, linetype = &quot;dotted&quot;, color = &quot;black&quot;) FIGURE 18.9: Comparing three confidence intervals for the slope. Observe that all three are quite similar! Furthermore, none of the three confidence intervals for \\(\\beta_1\\) contain 0 and are entirely located above 0. This is suggesting that there is in fact a meaningful positive relationship between teaching and “beauty” scores. 18.4.2 Hypothesis test for slope Let’s now conduct a hypothesis test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\neq 0\\). We will use the infer package, which follows the hypothesis testing paradigm in the “There is only one test” diagram in Figure 16.14. Let’s first think about what it means for \\(\\beta_1\\) to be zero as assumed in the null hypothesis \\(H_0\\). Recall we said if \\(\\beta_1 = 0\\), then this is saying there is no relationship between the teaching and “beauty” scores. Thus assuming this particular null hypothesis \\(H_0\\) means that in our “hypothesized universe” there is no relationship between score and bty_avg. We can therefore shuffle/permute the bty_avg variable to no consequence. We construct the null distribution of the fitted slope \\(b_1\\) by performing the steps that follow. Recall from Section 16.2 on terminology, notation, and definitions related to hypothesis testing where we defined the null distribution: the sampling distribution of our test statistic \\(b_1\\) assuming the null hypothesis \\(H_0\\) is true. specify() the variables of interest in evals_ch5 with the formula: score ~ bty_avg. hypothesize() the null hypothesis of independence. Recall from Section 16.3 that this is an additional step that needs to be added for hypothesis testing. generate() replicates by permuting/shuffling values from the original sample of 463 courses. We generate reps = 1000 replicates using type = \"permute\" here. calculate() the test statistic of interest: the fitted slope \\(b_1\\). In this case, we permute the values of bty_avg across the values of score 1000 times. We can do this shuffling/permuting since we assumed a “hypothesized universe” of no relationship between these two variables. Then we calculate the \"slope\" coefficient for each of these 1000 generated samples. null_distn_slope &lt;- evals %&gt;% specify(score ~ bty_avg) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;slope&quot;) Observe the resulting null distribution for the fitted slope \\(b_1\\) in Figure 18.10. FIGURE 18.10: Null distribution of slopes. Notice how it is centered at \\(b_1\\) = 0. This is because in our hypothesized universe, there is no relationship between score and bty_avg and so \\(\\beta_1 = 0\\). Thus, the most typical fitted slope \\(b_1\\) we observe across our simulations is 0. Observe, furthermore, how there is variation around this central value of 0. Let’s visualize the \\(p\\)-value in the null distribution by comparing it to the observed test statistic of \\(b_1\\) = 0.067 in Figure 18.11. We’ll do this by adding a shade_p_value() layer to the previous visualize() code. FIGURE 18.11: Null distribution and \\(p\\)-value. Since the observed fitted slope 0.067 falls far to the right of this null distribution and thus the shaded region doesn’t overlap it, we’ll have a \\(p\\)-value of 0. For completeness, however, let’s compute the numerical value of the \\(p\\)-value anyways using the get_p_value() function. Recall that it takes the same inputs as the shade_p_value() function: null_distn_slope %&gt;% get_p_value(obs_stat = observed_slope, direction = &quot;both&quot;) # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0 This matches the \\(p\\)-value of 0 in the regression table in Table 18.1. We therefore reject the null hypothesis \\(H_0: \\beta_1 = 0\\) in favor of the alternative hypothesis \\(H_A: \\beta_1 \\neq 0\\). We thus have evidence that suggests there is a significant relationship between teaching and “beauty” scores for all instructors at UT Austin. When the conditions for inference for regression are met and the null distribution has a bell shape, we are likely to see similar results between the simulation-based results we just demonstrated and the theory-based results shown in the regression table in Table 18.1. Learning check (LC10.2) Repeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of stat = \"correlation\" in the calculate() function of the infer package. 18.5 Conclusion 18.5.1 Theory-based inference for regression Recall in Subsection 18.2.5 when we interpreted the regression table in Table 18.1, we mentioned that R does not compute its values using simulation-based methods for constructing confidence intervals and conducting hypothesis tests as we did in Chapters 15 and 16 using the infer package. Rather, R uses a theory-based approach using mathematical formulas, much like the theory-based confidence intervals you saw in Subsection 15.7.2 and the theory-based hypothesis tests you saw in Subsection 16.6.1. These formulas were derived in a time when computers didn’t exist, so it would’ve been incredibly labor intensive to run extensive simulations. In particular, much like the formula for the standard error for the sample proportion \\(\\widehat{p}\\) we saw in Subsection 14.6.2 and the formula for the standard error for the difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) we saw in Subsection 16.6.1, there is a formula for the standard error of the fitted slope \\(b_1\\): \\[\\text{SE}_{b_1} = \\dfrac{\\dfrac{s_y}{s_x} \\cdot \\sqrt{1-r^2}}{\\sqrt{n-2}}\\] As with many formulas in statistics, there’s a lot going on here, so let’s first break down what each symbol represents. First \\(s_x\\) and \\(s_y\\) are the sample standard deviations of the explanatory variable bty_avg and the response variable score, respectively. Second, \\(r\\) is the sample correlation coefficient between score and bty_avg. This was computed as 0.187 in Chapter 17. Lastly, \\(n\\) is the number of pairs of points in the evals_ch5 data frame, here 463. To put this formula into words, the standard error of \\(b_1\\) depends on the relationship between the variability of the response variable and the variability of the explanatory variable as measured in the \\(s_y / s_x\\) term. Next, it looks into how the two variables relate to each other in the \\(\\sqrt{1-r^2}\\) term. However, the most important observation to make in the previous formula is that there is an \\(n - 2\\) in the denominator. In other words, as the sample size \\(n\\) increases, the standard error \\(\\text{SE}_{b_1}\\) decreases. Just as we demonstrated in Subsection 14.3.3 when we used shovels with \\(n\\) = 25, 50, and 100 slots, the amount of sampling variation of the fitted slope \\(b_1\\) will depend on the sample size \\(n\\). In particular, as the sample size increases, both the sampling and bootstrap distributions narrow and the standard error \\(\\text{SE}_{b_1}\\) decreases. Hence, our estimates of \\(b_1\\) for the true population slope \\(\\beta_1\\) get more and more precise. R then uses this formula for the standard error of \\(b_1\\) in the third column of the regression table and subsequently to construct 95% confidence intervals. But what about the hypothesis test? Much like with our theory-based hypothesis test in Subsection 16.6.1, R uses the following \\(t\\)-statistic as the test statistic for hypothesis testing: \\[ t = \\dfrac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} \\] And since the null hypothesis \\(H_0: \\beta_1 = 0\\) is assumed during the hypothesis test, the \\(t\\)-statistic becomes \\[ t = \\dfrac{ b_1 - 0}{ \\text{SE}_{b_1}} = \\dfrac{ b_1 }{ \\text{SE}_{b_1}} \\] What are the values of \\(b_1\\) and \\(\\text{SE}_{b_1}\\)? They are in the estimate and std_error column of the regression table in Table 18.1. Thus the value of 4.09 in the table is computed as 0.067/0.016 = 4.188. Note there is a difference due to some rounding error here. Lastly, to compute the \\(p\\)-value, we need to compare the observed test statistic of 4.09 to the appropriate null distribution. Recall from Section 16.2, that a null distribution is the sampling distribution of the test statistic assuming the null hypothesis \\(H_0\\) is true. Much like in our theory-based hypothesis test in Subsection 16.6.1, it can be mathematically proven that this distribution is a \\(t\\)-distribution with degrees of freedom equal to \\(df = n - 2 = 463 - 2 = 461\\). Don’t worry if you’re feeling a little overwhelmed at this point. There is a lot of background theory to understand before you can fully make sense of the equations for theory-based methods. That being said, theory-based methods and simulation-based methods for constructing confidence intervals and conducting hypothesis tests often yield consistent results. As mentioned before, in our opinion, two large benefits of simulation-based methods over theory-based are that (1) they are easier for people new to statistical inference to understand, and (2) they also work in situations where theory-based methods and mathematical formulas don’t exist. 18.5.2 Summary of statistical inference We’ve finished the last two scenarios from the “Scenarios of sampling for inference” table in Subsection 14.6.1, which we re-display in Table 18.4. TABLE 18.4: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) or \\(\\widehat{\\mu}_1 - \\widehat{\\mu}_2\\) 5 Population regression slope \\(\\beta_1\\) Fitted regression slope \\(b_1\\) or \\(\\widehat{\\beta}_1\\) Armed with the regression modeling techniques you learned in Chapters 17 and 19, your understanding of sampling for inference in Chapter 14, and the tools for statistical inference like confidence intervals and hypothesis tests in Chapters 15 and 16, you’re now equipped to study the significance of relationships between variables in a wide array of data! Many of the ideas presented here can be extended into multiple regression and other more advanced modeling techniques. 18.5.3 Additional resources An R script file of all R code used in this chapter is available here. 18.5.4 What’s to come You’ve now concluded the last major part of the book on “Statistical Inference with infer.” The closing Chapter ?? concludes this book with various short case studies involving real data, such as house prices in the city of Seattle, Washington in the US. You’ll see how the principles in this book can help you become a great storyteller with data! "],["19-multiple-regression.html", "Chapter 19 Multiple Regression 19.1 One numerical and one categorical explanatory variable 19.2 Two numerical explanatory variables 19.3 Related topics 19.4 Conclusion", " Chapter 19 Multiple Regression In Chapter 17 we introduced ideas related to modeling for explanation, in particular that the goal of modeling is to make explicit the relationship between some outcome variable \\(y\\) and some explanatory variable \\(x\\). While there are many approaches to modeling, we focused on one particular technique: linear regression, one of the most commonly used and easy-to-understand approaches to modeling. Furthermore to keep things simple, we only considered models with one explanatory \\(x\\) variable that was either numerical in Section 17.1 or categorical in Section 17.2. In this chapter on multiple regression, we’ll start considering models that include more than one explanatory variable \\(x\\). You can imagine when trying to model a particular outcome variable, like teaching evaluation scores as in Section 17.1 or life expectancy as in Section 17.2, that it would be useful to include more than just one explanatory variable’s worth of information. Since our regression models will now consider more than one explanatory variable, the interpretation of the associated effect of any one explanatory variable must be made in conjunction with the other explanatory variables included in your model. Let’s begin! Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section ?? that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section ?? for information on how to install and load R packages. library(tidyverse) library(moderndive) library(skimr) library(ISLR) 19.1 One numerical and one categorical explanatory variable Let’s revisit the instructor evaluation data from UT Austin we introduced in Section 17.1. We studied the relationship between teaching evaluation scores as given by students and “beauty” scores. The variable teaching score was the numerical outcome variable \\(y\\), and the variable “beauty” score (bty_avg) was the numerical explanatory \\(x\\) variable. In this section, we are going to consider a different model. Our outcome variable will still be teaching score, but we’ll now include two different explanatory variables: age and (binary) gender. Could it be that instructors who are older receive better teaching evaluations from students? Or could it instead be that younger instructors receive better evaluations? Are there differences in evaluations given by students for instructors of different genders? We’ll answer these questions by modeling the relationship between these variables using multiple regression, where we have: A numerical outcome variable \\(y\\), the instructor’s teaching score, and Two explanatory variables: A numerical explanatory variable \\(x_1\\), the instructor’s age. A categorical explanatory variable \\(x_2\\), the instructor’s (binary) gender. It is important to note that at the time of this study due to then commonly held beliefs about gender, this variable was often recorded as a binary variable. While the results of a model that oversimplifies gender this way may be imperfect, we still found the results to be pertinent and relevant today. 19.1.1 Exploratory data analysis Recall that data on the 463 courses at UT Austin can be found in the evals data frame included in the moderndive package. However, to keep things simple, let’s select() only the subset of the variables we’ll consider in this chapter, and save this data in a new data frame called evals_ch6. Note that these are different than the variables chosen in Chapter 17. evals_ch6 &lt;- evals %&gt;% select(ID, score, age, gender) Recall the three common steps in an exploratory data analysis we saw in Subsection 17.1.1: Looking at the raw data values. Computing summary statistics. Creating data visualizations. Let’s first look at the raw data values by either looking at evals_ch6 using RStudio’s spreadsheet viewer or by using the glimpse() function from the dplyr package: glimpse(evals_ch6) Rows: 463 Columns: 4 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, … $ score &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4.6… $ age &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 40,… $ gender &lt;fct&gt; female, female, female, female, male, male, male, male, male, f… Let’s also display a random sample of 5 rows of the 463 rows corresponding to different courses in Table 19.1. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows. evals_ch6 %&gt;% sample_n(size = 5) TABLE 19.1: A random sample of 5 out of the 463 courses at UT Austin ID score age gender 129 3.7 62 male 109 4.7 46 female 28 4.8 62 male 434 2.8 62 male 330 4.0 64 male Now that we’ve looked at the raw values in our evals_ch6 data frame and got a sense of the data, let’s compute summary statistics. As we did in our exploratory data analyses in Sections 17.1.1 and 17.2.1 from the previous chapter, let’s use the skim() function from the skimr package, being sure to only select() the variables of interest in our model: evals_ch6 %&gt;% select(score, age, gender) %&gt;% skim() Skim summary statistics n obs: 463 n variables: 3 ── Variable type:factor variable missing complete n n_unique top_counts ordered gender 0 463 463 2 mal: 268, fem: 195, NA: 0 FALSE ── Variable type:integer variable missing complete n mean sd p0 p25 p50 p75 p100 age 0 463 463 48.37 9.8 29 42 48 57 73 ── Variable type:numeric variable missing complete n mean sd p0 p25 p50 p75 p100 score 0 463 463 4.17 0.54 2.3 3.8 4.3 4.6 5 Observe that we have no missing data, that there are 268 courses taught by male instructors and 195 courses taught by female instructors, and that the average instructor age is 48.37. Recall that each row represents a particular course and that the same instructor often teaches more than one course. Therefore, the average age of the unique instructors may differ. Furthermore, let’s compute the correlation coefficient between our two numerical variables: score and age. Recall from Subsection 17.1.1 that correlation coefficients only exist between numerical variables. We observe that they are “weakly negatively” correlated. evals_ch6 %&gt;% get_correlation(formula = score ~ age) # A tibble: 1 × 1 cor &lt;dbl&gt; 1 -0.107032 Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations. Given that the outcome variable score and explanatory variable age are both numerical, we’ll use a scatterplot to display their relationship. How can we incorporate the categorical variable gender, however? By mapping the variable gender to the color aesthetic, thereby creating a colored scatterplot. The following code is similar to the code that created the scatterplot of teaching score over “beauty” score in Figure 17.2, but with color = gender added to the aes()thetic mapping. ggplot(evals_ch6, aes(x = age, y = score, color = gender)) + geom_point() + labs(x = &quot;Age&quot;, y = &quot;Teaching Score&quot;, color = &quot;Gender&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 19.1: Colored scatterplot of relationship of teaching score and age. In the resulting Figure 19.1, observe that ggplot() assigns a default in red/blue color scheme to the points and to the lines associated with the two levels of gender: female and male. Furthermore, the geom_smooth(method = \"lm\", se = FALSE) layer automatically fits a different regression line for each group. We notice some interesting trends. First, there are almost no women faculty over the age of 60 as evidenced by lack of red dots above \\(x\\) = 60. Second, while both regression lines are negatively sloped with age (i.e., older instructors tend to have lower scores), the slope for age for the female instructors is more negative. In other words, female instructors are paying a harsher penalty for advanced age than the male instructors. 19.1.2 Interaction model Let’s now quantify the relationship of our outcome variable \\(y\\) and the two explanatory variables using one type of multiple regression model known as an interaction model. We’ll explain where the term “interaction” comes from at the end of this section. In particular, we’ll write out the equation of the two regression lines in Figure 19.1 using the values from a regression table. Before we do this, however, let’s go over a brief refresher of regression when you have a categorical explanatory variable \\(x\\). Recall in Subsection 17.2.2 we fit a regression model for countries’ life expectancies as a function of which continent the country was in. In other words, we had a numerical outcome variable \\(y\\) = lifeExp and a categorical explanatory variable \\(x\\) = continent which had 5 levels: Africa, Americas, Asia, Europe, and Oceania. Let’s re-display the regression table you saw in Table 17.8: TABLE 19.2: Regression table for life expectancy as a function of continent term estimate std_error statistic p_value lower_ci upper_ci intercept 54.8 1.02 53.45 0 52.8 56.8 continent: Americas 18.8 1.80 10.45 0 15.2 22.4 continent: Asia 15.9 1.65 9.68 0 12.7 19.2 continent: Europe 22.8 1.70 13.47 0 19.5 26.2 continent: Oceania 25.9 5.33 4.86 0 15.4 36.5 Recall our interpretation of the estimate column. Since Africa was the “baseline for comparison” group, the intercept term corresponds to the mean life expectancy for all countries in Africa of 54.8 years. The other four values of estimate correspond to “offsets” relative to the baseline group. So, for example, the “offset” corresponding to the Americas is +18.8 as compared to the baseline for comparison group Africa. In other words, the average life expectancy for countries in the Americas is 18.8 years higher. Thus the mean life expectancy for all countries in the Americas is 54.8 + 18.8 = 73.6. The same interpretation holds for Asia, Europe, and Oceania. Going back to our multiple regression model for teaching score using age and gender in Figure 19.1, we generate the regression table using the same two-step approach from Chapter 17: we first “fit” the model using the lm() “linear model” function and then we apply the get_regression_table() function. This time, however, our model formula won’t be of the form y ~ x, but rather of the form y ~ x1 * x2. In other words, our two explanatory variables x1 and x2 are separated by a * sign: # Fit regression model: score_model_interaction &lt;- lm(score ~ age * gender, data = evals_ch6) # Get regression table: get_regression_table(score_model_interaction) TABLE 19.3: Regression table for interaction model term estimate std_error statistic p_value lower_ci upper_ci intercept 4.883 0.205 23.80 0.000 4.480 5.286 age -0.018 0.004 -3.92 0.000 -0.026 -0.009 gender: male -0.446 0.265 -1.68 0.094 -0.968 0.076 age:gendermale 0.014 0.006 2.45 0.015 0.003 0.024 Looking at the regression table output in Table 19.3, there are four rows of values in the estimate column. While it is not immediately apparent, using these four values we can write out the equations of both lines in Figure 19.1. First, since the word female comes alphabetically before male, female instructors are the “baseline for comparison” group. Thus, intercept is the intercept for only the female instructors. This holds similarly for age. It is the slope for age for only the female instructors. Thus, the red regression line in Figure 19.1 has an intercept of 4.883 and slope for age of -0.018. Remember that for this data, while the intercept has a mathematical interpretation, it has no practical interpretation since instructors can’t have zero age. What about the intercept and slope for age of the male instructors in the blue line in Figure 19.1? This is where our notion of “offsets” comes into play once again. The value for gender: male of -0.446 is not the intercept for the male instructors, but rather the offset in intercept for male instructors relative to female instructors. The intercept for the male instructors is intercept + gender: male = 4.883 + (-0.446) = 4.883 - 0.446 = 4.437. Similarly, age:gendermale = 0.014 is not the slope for age for the male instructors, but rather the offset in slope for the male instructors. Therefore, the slope for age for the male instructors is age + age:gendermale \\(= -0.018 + 0.014 = -0.004\\). Thus, the blue regression line in Figure 19.1 has intercept 4.437 and slope for age of -0.004. Let’s summarize these values in Table 19.4 and focus on the two slopes for age: TABLE 19.4: Comparison of intercepts and slopes for interaction model Gender Intercept Slope for age Female instructors 4.883 -0.018 Male instructors 4.437 -0.004 Since the slope for age for the female instructors was -0.018, it means that on average, a female instructor who is a year older would have a teaching score that is 0.018 units lower. For the male instructors, however, the corresponding associated decrease was on average only 0.004 units. While both slopes for age were negative, the slope for age for the female instructors is more negative. This is consistent with our observation from Figure 19.1, that this model is suggesting that age impacts teaching scores for female instructors more than for male instructors. Let’s now write the equation for our regression lines, which we can use to compute our fitted values \\(\\widehat{y} = \\widehat{\\text{score}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= b_0 + b_{\\text{age}} \\cdot \\text{age} + b_{\\text{male}} \\cdot \\mathbb{1}_{\\text{is male}}(x) + b_{\\text{age,male}} \\cdot \\text{age} \\cdot \\mathbb{1}_{\\text{is male}}(x)\\\\ &amp;= 4.883 -0.018 \\cdot \\text{age} - 0.446 \\cdot \\mathbb{1}_{\\text{is male}}(x) + 0.014 \\cdot \\text{age} \\cdot \\mathbb{1}_{\\text{is male}}(x) \\end{aligned} \\] Whoa! That’s even more daunting than the equation you saw for the life expectancy as a function of continent in Subsection 17.2.2! However, if you recall what an “indicator function” does, the equation simplifies greatly. In the previous equation, we have one indicator function of interest: \\[ \\mathbb{1}_{\\text{is male}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{instructor } x \\text{ is male} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\] Second, let’s match coefficients in the previous equation with values in the estimate column in our regression table in Table 19.3: \\(b_0\\) is the intercept = 4.883 for the female instructors \\(b_{\\text{age}}\\) is the slope for age = -0.018 for the female instructors \\(b_{\\text{male}}\\) is the offset in intercept = -0.446 for the male instructors \\(b_{\\text{age,male}}\\) is the offset in slope for age = 0.014 for the male instructors Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{score}}\\) for female instructors. Since for female instructors \\(\\mathbb{1}_{\\text{is male}}(x)\\) = 0, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= 4.883 - 0.018 \\cdot \\text{age} - 0.446 \\cdot 0 + 0.014 \\cdot \\text{age} \\cdot 0\\\\ &amp;= 4.883 - 0.018 \\cdot \\text{age} - 0 + 0\\\\ &amp;= 4.883 - 0.018 \\cdot \\text{age}\\\\ \\end{aligned} \\] which is the equation of the red regression line in Figure 19.1 corresponding to the female instructors in Table 19.4. Correspondingly, since for male instructors \\(\\mathbb{1}_{\\text{is male}}(x)\\) = 1, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= 4.883 - 0.018 \\cdot \\text{age} - 0.446 + 0.014 \\cdot \\text{age}\\\\ &amp;= (4.883 - 0.446) + (- 0.018 + 0.014) * \\text{age}\\\\ &amp;= 4.437 - 0.004 \\cdot \\text{age}\\\\ \\end{aligned} \\] which is the equation of the blue regression line in Figure 19.1 corresponding to the male instructors in Table 19.4. Phew! That was a lot of arithmetic! Don’t fret, however, this is as hard as modeling will get in this book. If you’re still a little unsure about using indicator functions and using categorical explanatory variables in a regression model, we highly suggest you re-read Subsection 17.2.2. This involves only a single categorical explanatory variable and thus is much simpler. Before we end this section, we explain why we refer to this type of model as an “interaction model.” The \\(b_{\\text{age,male}}\\) term in the equation for the fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) is what’s known in statistical modeling as an “interaction effect.” The interaction term corresponds to the age:gendermale = 0.014 in the final row of the regression table in Table 19.3. We say there is an interaction effect if the associated effect of one variable depends on the value of another variable. That is to say, the two variables are “interacting” with each other. Here, the associated effect of the variable age depends on the value of the other variable gender. The difference in slopes for age of +0.014 of male instructors relative to female instructors shows this. Another way of thinking about interaction effects on teaching scores is as follows. For a given instructor at UT Austin, there might be an associated effect of their age by itself, there might be an associated effect of their gender by itself, but when age and gender are considered together there might be an additional effect above and beyond the two individual effects. 19.1.3 Parallel slopes model When creating regression models with one numerical and one categorical explanatory variable, we are not just limited to interaction models as we just saw. Another type of model we can use is known as a parallel slopes model. Unlike interaction models where the regression lines can have different intercepts and different slopes, parallel slopes models still allow for different intercepts but force all lines to have the same slope. The resulting regression lines are thus parallel. Let’s visualize the best-fitting parallel slopes model to evals_ch6. Unfortunately, the geom_smooth() function in the ggplot2 package does not have a convenient way to plot parallel slopes models. Evgeni Chasnovski thus created a special purpose function called geom_parallel_slopes() that is included in the moderndive package. You won’t find geom_parallel_slopes() in the ggplot2 package, but rather the moderndive package. Thus, if you want to be able to use it, you will need to load both the ggplot2 and moderndive packages. Using this function, let’s now plot the parallel slopes model for teaching score. Notice how the code is identical to the code that produced the visualization of the interaction model in Figure 19.1, but now the geom_smooth(method = \"lm\", se = FALSE) layer is replaced with geom_parallel_slopes(se = FALSE). ggplot(evals_ch6, aes(x = age, y = score, color = gender)) + geom_point() + labs(x = &quot;Age&quot;, y = &quot;Teaching Score&quot;, color = &quot;Gender&quot;) + geom_parallel_slopes(se = FALSE) FIGURE 19.2: Parallel slopes model of score with age and gender. Observe in Figure 19.2 that we now have parallel lines corresponding to the female and male instructors, respectively: here they have the same negative slope. This is telling us that instructors who are older will tend to receive lower teaching scores than instructors who are younger. Furthermore, since the lines are parallel, the associated penalty for being older is assumed to be the same for both female and male instructors. However, observe also in Figure 19.2 that these two lines have different intercepts as evidenced by the fact that the blue line corresponding to the male instructors is higher than the red line corresponding to the female instructors. This is telling us that irrespective of age, female instructors tended to receive lower teaching scores than male instructors. In order to obtain the precise numerical values of the two intercepts and the single common slope, we once again “fit” the model using the lm() “linear model” function and then apply the get_regression_table() function. However, unlike the interaction model which had a model formula of the form y ~ x1 * x2, our model formula is now of the form y ~ x1 + x2. In other words, our two explanatory variables x1 and x2 are separated by a + sign: # Fit regression model: score_model_parallel_slopes &lt;- lm(score ~ age + gender, data = evals_ch6) # Get regression table: get_regression_table(score_model_parallel_slopes) TABLE 19.5: Regression table for parallel slopes model term estimate std_error statistic p_value lower_ci upper_ci intercept 4.484 0.125 35.79 0.000 4.238 4.730 age -0.009 0.003 -3.28 0.001 -0.014 -0.003 gender: male 0.191 0.052 3.63 0.000 0.087 0.294 Similarly to the regression table for the interaction model from Table 19.3, we have an intercept term corresponding to the intercept for the “baseline for comparison” female instructor group and a gender: male term corresponding to the offset in intercept for the male instructors relative to female instructors. In other words, in Figure 19.2 the red regression line corresponding to the female instructors has an intercept of 4.484 while the blue regression line corresponding to the male instructors has an intercept of 4.484 + 0.191 = 4.675. Once again, since there aren’t any instructors of age 0, the intercepts only have a mathematical interpretation but no practical one. Unlike in Table 19.3, however, we now only have a single slope for age of -0.009. This is because the model dictates that both the female and male instructors have a common slope for age. This is telling us that an instructor who is a year older than another instructor received a teaching score that is on average 0.009 units lower. This penalty for being of advanced age applies equally to both female and male instructors. Let’s summarize these values in Table 19.6, noting the different intercepts but common slopes: TABLE 19.6: Comparison of intercepts and slope for parallel slopes model Gender Intercept Slope for age Female instructors 4.484 -0.009 Male instructors 4.675 -0.009 Let’s now write the equation for our regression lines, which we can use to compute our fitted values \\(\\widehat{y} = \\widehat{\\text{score}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= b_0 + b_{\\text{age}} \\cdot \\text{age} + b_{\\text{male}} \\cdot \\mathbb{1}_{\\text{is male}}(x)\\\\ &amp;= 4.484 -0.009 \\cdot \\text{age} + 0.191 \\cdot \\mathbb{1}_{\\text{is male}}(x) \\end{aligned} \\] Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{score}}\\) for female instructors. Since for female instructors the indicator function \\(\\mathbb{1}_{\\text{is male}}(x)\\) = 0, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= 4.484 -0.009 \\cdot \\text{age} + 0.191 \\cdot 0\\\\ &amp;= 4.484 -0.009 \\cdot \\text{age} \\end{aligned} \\] which is the equation of the red regression line in Figure 19.2 corresponding to the female instructors. Correspondingly, since for male instructors the indicator function \\(\\mathbb{1}_{\\text{is male}}(x)\\) = 1, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= 4.484 -0.009 \\cdot \\text{age} + 0.191 \\cdot 1\\\\ &amp;= (4.484 + 0.191) - 0.009 \\cdot \\text{age}\\\\ &amp;= 4.675 -0.009 \\cdot \\text{age} \\end{aligned} \\] which is the equation of the blue regression line in Figure 19.2 corresponding to the male instructors. Great! We’ve considered both an interaction model and a parallel slopes model for our data. Let’s compare the visualizations for both models side-by-side in Figure 19.3. FIGURE 19.3: Comparison of interaction and parallel slopes models. At this point, you might be asking yourself: “Why would we ever use a parallel slopes model?”. Looking at the left-hand plot in Figure 19.3, the two lines definitely do not appear to be parallel, so why would we force them to be parallel? For this data, we agree! It can easily be argued that the interaction model on the left is more appropriate. However, in the upcoming Subsection 19.3.1 on model selection, we’ll present an example where it can be argued that the case for a parallel slopes model might be stronger. 19.1.4 Observed/fitted values and residuals For brevity’s sake, in this section we’ll only compute the observed values, fitted values, and residuals for the interaction model which we saved in score_model_interaction. You’ll have an opportunity to study the corresponding values for the parallel slopes model in the upcoming Learning check. Say, you have an instructor who identifies as female and is 36 years old. What fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) would our model yield? Say, you have another instructor who identifies as male and is 59 years old. What would their fitted value \\(\\widehat{y}\\) be? We answer this question visually first for the female instructor by finding the intersection of the red regression line and the vertical line at \\(x\\) = age = 36. We mark this value with a large red dot in Figure 19.4. Similarly, we can identify the fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) for the male instructor by finding the intersection of the blue regression line and the vertical line at \\(x\\) = age = 59. We mark this value with a large blue dot in Figure 19.4. FIGURE 19.4: Fitted values for two new professors. What are these two values of \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) precisely? We can use the equations of the two regression lines we computed in Subsection 19.1.2, which in turn were based on values from the regression table in Table 19.3: For all female instructors: \\(\\widehat{y} = \\widehat{\\text{score}} = 4.883 - 0.018 \\cdot \\text{age}\\) For all male instructors: \\(\\widehat{y} = \\widehat{\\text{score}} = 4.437 - 0.004 \\cdot \\text{age}\\) So our fitted values would be: \\(4.883 - 0.018 \\cdot 36 = 4.24\\) and \\(4.437 - 0.004 \\cdot 59 = 4.20\\), respectively. Now what if we want the fitted values not just for these two instructors, but for the instructors of all 463 courses included in the evals_ch6 data frame? Doing this by hand would be long and tedious! This is where the get_regression_points() function from the moderndive package can help: it will quickly automate the above calculations for all 463 courses. We present a preview of just the first 10 rows out of 463 in Table 19.7. regression_points &lt;- get_regression_points(score_model_interaction) regression_points TABLE 19.7: Regression points (First 10 out of 463 courses) ID score age gender score_hat residual 1 4.7 36 female 4.25 0.448 2 4.1 36 female 4.25 -0.152 3 3.9 36 female 4.25 -0.352 4 4.8 36 female 4.25 0.548 5 4.6 59 male 4.20 0.399 6 4.3 59 male 4.20 0.099 7 2.8 59 male 4.20 -1.401 8 4.1 51 male 4.23 -0.133 9 3.4 51 male 4.23 -0.833 10 4.5 40 female 4.18 0.318 It turns out that the female instructor of age 36 taught the first four courses, while the male instructor taught the next 3. The resulting \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) fitted values are in the score_hat column. Furthermore, the get_regression_points() function also returns the residuals \\(y-\\widehat{y}\\). Notice, for example, the first and fourth courses the female instructor of age 36 taught had positive residuals, indicating that the actual teaching scores they received from students were greater than their fitted score of 4.25. On the other hand, the second and third courses this instructor taught had negative residuals, indicating that the actual teaching scores they received from students were less than 4.25. Learning check (LC6.1) Compute the observed values, fitted values, and residuals not for the interaction model as we just did, but rather for the parallel slopes model we saved in score_model_parallel_slopes. 19.2 Two numerical explanatory variables Let’s now switch gears and consider multiple regression models where instead of one numerical and one categorical explanatory variable, we now have two numerical explanatory variables. The dataset we’ll use is from An Introduction to Statistical Learning with Applications in R (ISLR), an intermediate-level textbook on statistical and machine learning (James et al. 2017). Its accompanying ISLR R package contains the datasets to which the authors apply various machine learning methods. One frequently used dataset in this book is the Credit dataset, where the outcome variable of interest is the credit card debt of 400 individuals. Other variables like income, credit limit, credit rating, and age are included as well. Note that the Credit data is not based on real individuals’ financial information, but rather is a simulated dataset used for educational purposes. In this section, we’ll fit a regression model where we have A numerical outcome variable \\(y\\), the cardholder’s credit card debt Two explanatory variables: One numerical explanatory variable \\(x_1\\), the cardholder’s credit limit Another numerical explanatory variable \\(x_2\\), the cardholder’s income (in thousands of dollars). 19.2.1 Exploratory data analysis Let’s load the Credit dataset. To keep things simple let’s select() the subset of the variables we’ll consider in this chapter, and save this data in the new data frame credit_ch6. Notice our slightly different use of the select() verb here than we introduced in Subsection 13.8.1. For example, we’ll select the Balance variable from Credit but then save it with a new variable name debt. We do this because here the term “debt” is easier to interpret than “balance.” library(ISLR) credit_ch6 &lt;- Credit %&gt;% as_tibble() %&gt;% select(ID, debt = Balance, credit_limit = Limit, income = Income, credit_rating = Rating, age = Age) You can observe the effect of our use of select() in the first common step of an exploratory data analysis: looking at the raw values either in RStudio’s spreadsheet viewer or by using glimpse(). glimpse(credit_ch6) Rows: 400 Columns: 6 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… $ debt &lt;int&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407… $ credit_limit &lt;int&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 68… $ income &lt;dbl&gt; 14.9, 106.0, 104.6, 148.9, 55.9, 80.2, 21.0, 71.4, 15.1,… $ credit_rating &lt;int&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 1… $ age &lt;int&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, … Furthermore, let’s look at a random sample of five out of the 400 credit card holders in Table 19.8. Once again, note that due to the random nature of the sampling, you will likely end up with a different subset of five rows. credit_ch6 %&gt;% sample_n(size = 5) TABLE 19.8: Random sample of 5 credit card holders ID debt credit_limit income credit_rating age 272 436 4866 45.0 347 30 239 52 2910 26.5 236 58 87 815 6340 55.4 448 33 108 0 3189 39.1 263 72 149 0 2420 15.2 192 69 Now that we’ve looked at the raw values in our credit_ch6 data frame and got a sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s use the skim() function from the skimr package, being sure to only select() the columns of interest for our model: credit_ch6 %&gt;% select(debt, credit_limit, income) %&gt;% skim() Skim summary statistics n obs: 400 n variables: 3 ── Variable type:integer variable missing complete n mean sd p0 p25 p50 p75 p100 credit_limit 0 400 400 4735.6 2308.2 855 3088 4622.5 5872.75 13913 debt 0 400 400 520.01 459.76 0 68.75 459.5 863 1999 ── Variable type:numeric variable missing complete n mean sd p0 p25 p50 p75 p100 income 0 400 400 45.22 35.24 10.35 21.01 33.12 57.47 186.63 Observe the summary statistics for the outcome variable debt: the mean and median credit card debt are $520.01 and $459.50, respectively, and that 25% of card holders had debts of $68.75 or less. Let’s now look at one of the explanatory variables credit_limit: the mean and median credit card limit are $4735.6 and $4622.50, respectively, while 75% of card holders had incomes of $57,470 or less. Since our outcome variable debt and the explanatory variables credit_limit and income are numerical, we can compute the correlation coefficient between the different possible pairs of these variables. First, we can run the get_correlation() command as seen in Subsection 17.1.1 twice, once for each explanatory variable: credit_ch6 %&gt;% get_correlation(debt ~ credit_limit) credit_ch6 %&gt;% get_correlation(debt ~ income) Or we can simultaneously compute them by returning a correlation matrix which we display in Table 19.9. We can see the correlation coefficient for any pair of variables by looking them up in the appropriate row/column combination. credit_ch6 %&gt;% select(debt, credit_limit, income) %&gt;% cor() TABLE 19.9: Correlation coefficients between credit card debt, credit limit, and income debt credit_limit income debt 1.000 0.862 0.464 credit_limit 0.862 1.000 0.792 income 0.464 0.792 1.000 For example, the correlation coefficient of: debt with itself is 1 as we would expect based on the definition of the correlation coefficient. debt with credit_limit is 0.862. This indicates a strong positive linear relationship, which makes sense as only individuals with large credit limits can accrue large credit card debts. debt with income is 0.464. This is suggestive of another positive linear relationship, although not as strong as the relationship between debt and credit_limit. As an added bonus, we can read off the correlation coefficient between the two explanatory variables of credit_limit and income as 0.792. We say there is a high degree of collinearity between the credit_limit and income explanatory variables. Collinearity (or multicollinearity) is a phenomenon where one explanatory variable in a multiple regression model is highly correlated with another. So in our case since credit_limit and income are highly correlated, if we knew someone’s credit_limit, we could make pretty good guesses about their income as well. Thus, these two variables provide somewhat redundant information. However, we’ll leave discussion on how to work with collinear explanatory variables to a more intermediate-level book on regression modeling. Let’s visualize the relationship of the outcome variable with each of the two explanatory variables in two separate plots in Figure 19.5. ggplot(credit_ch6, aes(x = credit_limit, y = debt)) + geom_point() + labs(x = &quot;Credit limit (in $)&quot;, y = &quot;Credit card debt (in $)&quot;, title = &quot;Debt and credit limit&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(credit_ch6, aes(x = income, y = debt)) + geom_point() + labs(x = &quot;Income (in $1000)&quot;, y = &quot;Credit card debt (in $)&quot;, title = &quot;Debt and income&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 19.5: Relationship between credit card debt and credit limit/income. Observe there is a positive relationship between credit limit and credit card debt: as credit limit increases so also does credit card debt. This is consistent with the strongly positive correlation coefficient of 0.862 we computed earlier. In the case of income, the positive relationship doesn’t appear as strong, given the weakly positive correlation coefficient of 0.464. However, the two plots in Figure 19.5 only focus on the relationship of the outcome variable with each of the two explanatory variables separately. To visualize the joint relationship of all three variables simultaneously, we need a 3-dimensional (3D) scatterplot as seen in Figure 19.6. Each of the 400 observations in the credit_ch6 data frame are marked with a blue point where The numerical outcome variable \\(y\\) debt is on the vertical axis. The two numerical explanatory variables, \\(x_1\\) income and \\(x_2\\) credit_limit, are on the two axes that form the bottom plane. FIGURE 19.6: 3D scatterplot and regression plane. Furthermore, we also include the regression plane. Recall from Subsection 17.3.2 that regression lines are “best-fitting” in that of all possible lines we can draw through a cloud of points, the regression line minimizes the sum of squared residuals. This concept also extends to models with two numerical explanatory variables. The difference is instead of a “best-fitting” line, we now have a “best-fitting” plane that similarly minimizes the sum of squared residuals. Head to this website to open an interactive version of this plot in your browser. Learning check (LC6.2) Conduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age? 19.2.2 Regression plane Let’s now fit a regression model and get the regression table corresponding to the regression plane in Figure 19.6. To keep things brief in this subsection, we won’t consider an interaction model for the two numerical explanatory variables income and credit_limit like we did in Subsection 19.1.2 using the model formula score ~ age * gender. Rather we’ll only consider a model fit with a formula of the form y ~ x1 + x2. Confusingly, however, since we now have a regression plane instead of multiple lines, the label “parallel slopes” doesn’t apply when you have two numerical explanatory variables. Just as we have done multiple times throughout Chapters 17 and this chapter, the regression table for this model using our two-step process is in Table 19.10. # Fit regression model: debt_model &lt;- lm(debt ~ credit_limit + income, data = credit_ch6) # Get regression table: get_regression_table(debt_model) TABLE 19.10: Multiple regression table term estimate std_error statistic p_value lower_ci upper_ci intercept -385.179 19.465 -19.8 0 -423.446 -346.912 credit_limit 0.264 0.006 45.0 0 0.253 0.276 income -7.663 0.385 -19.9 0 -8.420 -6.906 We first “fit” the linear regression model using the lm(y ~ x1 + x2, data) function and save it in debt_model. We get the regression table by applying the get_regression_table() function from the moderndive package to debt_model. Let’s interpret the three values in the estimate column. First, the intercept value is -$385.179. This intercept represents the credit card debt for an individual who has credit_limit of $0 and income of $0. In our data, however, the intercept has no practical interpretation since no individuals had credit_limit or income values of $0. Rather, the intercept is used to situate the regression plane in 3D space. Second, the credit_limit value is $0.264. Taking into account all the other explanatory variables in our model, for every increase of one dollar in credit_limit, there is an associated increase of on average $0.26 in credit card debt. Just as we did in Subsection 17.1.2, we are cautious not to imply causality as we saw in Subsection 17.3.1 that “correlation is not necessarily causation.” We do this merely stating there was an associated increase. Furthermore, we preface our interpretation with the statement, “taking into account all the other explanatory variables in our model.” Here, by all other explanatory variables we mean income. We do this to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model at the same time. Third, income = -$7.66. Taking into account all other explanatory variables in our model, for every increase of one unit of income ($1000 in actual income), there is an associated decrease of, on average, $7.66 in credit card debt. Putting these results together, the equation of the regression plane that gives us fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{debt}}\\) is: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2\\\\ \\widehat{\\text{debt}} &amp;= b_0 + b_{\\text{limit}} \\cdot \\text{limit} + b_{\\text{income}} \\cdot \\text{income}\\\\ &amp;= -385.179 + 0.263 \\cdot\\text{limit} - 7.663 \\cdot\\text{income} \\end{aligned} \\] Recall however in the right-hand plot of Figure 19.5 that when plotting the relationship between debt and income in isolation, there appeared to be a positive relationship. In the last discussed multiple regression, however, when jointly modeling the relationship between debt, credit_limit, and income, there appears to be a negative relationship of debt and income as evidenced by the negative slope for income of -$7.663. What explains these contradictory results? A phenomenon known as Simpson’s Paradox, whereby overall trends that exist in aggregate either disappear or reverse when the data are broken down into groups. In Subsection 19.3.4 we elaborate on this idea by looking at the relationship between credit_limit and credit card debt, but split along different income brackets. Learning check (LC6.3) Fit a new simple linear regression using lm(debt ~ credit_rating + age, data = credit_ch6) where credit_rating and age are the new numerical explanatory variables \\(x_1\\) and \\(x_2\\). Get information about the “best-fitting” regression plane from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis? 19.2.3 Observed/fitted values and residuals Let’s also compute all fitted values and residuals for our regression model using the get_regression_points() function and present only the first 10 rows of output in Table 19.11. Remember that the coordinates of each of the blue points in our 3D scatterplot in Figure 19.6 can be found in the income, credit_limit, and debt columns. The fitted values on the regression plane are found in the debt_hat column and are computed using our equation for the regression plane in the previous section: \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{debt}} &amp;= -385.179 + 0.263 \\cdot \\text{limit} - 7.663 \\cdot \\text{income} \\end{aligned} \\] get_regression_points(debt_model) TABLE 19.11: Regression points (First 10 credit card holders out of 400) ID debt credit_limit income debt_hat residual 1 333 3606 14.9 454 -120.8 2 903 6645 106.0 559 344.3 3 580 7075 104.6 683 -103.4 4 964 9504 148.9 986 -21.7 5 331 4897 55.9 481 -150.0 6 1151 8047 80.2 1127 23.6 7 203 3388 21.0 349 -146.4 8 872 7114 71.4 948 -76.0 9 279 3300 15.1 371 -92.2 10 1350 6819 71.1 873 477.3 19.3 Related topics 19.3.1 Model selection using visualizations When should we use an interaction model versus a parallel slopes model? Recall in Sections 19.1.2 and 19.1.3 we fit both interaction and parallel slopes models for the outcome variable \\(y\\) (teaching score) using a numerical explanatory variable \\(x_1\\) (age) and a categorical explanatory variable \\(x_2\\) (gender recorded as a binary variable). We compared these models in Figure 19.3, which we display again now. FIGURE 19.7: Previously seen comparison of interaction and parallel slopes models. A lot of you might have asked yourselves: “Why would I force the lines to have parallel slopes (as seen in the right-hand plot) when they clearly have different slopes (as seen in the left-hand plot)?”. The answer lies in a philosophical principle known as “Occam’s Razor.” It states that, “all other things being equal, simpler solutions are more likely to be correct than complex ones.” When viewed in a modeling framework, Occam’s Razor can be restated as, “all other things being equal, simpler models are to be preferred over complex ones.” In other words, we should only favor the more complex model if the additional complexity is warranted. Let’s revisit the equations for the regression line for both the interaction and parallel slopes model: \\[ \\begin{aligned} \\text{Interaction} &amp;: \\widehat{y} = \\widehat{\\text{score}} = b_0 + b_{\\text{age}} \\cdot \\text{age} + b_{\\text{male}} \\cdot \\mathbb{1}_{\\text{is male}}(x) + \\\\ &amp; \\qquad b_{\\text{age,male}} \\cdot \\text{age} \\cdot \\mathbb{1}_{\\text{is male}}\\\\ \\text{Parallel slopes} &amp;: \\widehat{y} = \\widehat{\\text{score}} = b_0 + b_{\\text{age}} \\cdot \\text{age} + b_{\\text{male}} \\cdot \\mathbb{1}_{\\text{is male}}(x) \\end{aligned} \\] The interaction model is “more complex” in that there is an additional \\(b_{\\text{age,male}} \\cdot \\text{age} \\cdot \\mathbb{1}_{\\text{is male}}\\) interaction term in the equation not present for the parallel slopes model. Or viewed alternatively, the regression table for the interaction model in Table 19.3 has four rows, whereas the regression table for the parallel slopes model in Table 19.5 has three rows. The question becomes: “Is this additional complexity warranted?”. In this case, it can be argued that this additional complexity is warranted, as evidenced by the clear x-shaped pattern of the two regression lines in the left-hand plot of Figure 19.7. However, let’s consider an example where the additional complexity might not be warranted. Let’s consider the MA_schools data included in the moderndive package which contains 2017 data on Massachusetts public high schools provided by the Massachusetts Department of Education. For more details, read the help file for this data by running ?MA_schools in the console. Let’s model the numerical outcome variable \\(y\\), average SAT math score for a given high school, as a function of two explanatory variables: A numerical explanatory variable \\(x_1\\), the percentage of that high school’s student body that are economically disadvantaged and A categorical explanatory variable \\(x_2\\), the school size as measured by enrollment: small (13-341 students), medium (342-541 students), and large (542-4264 students). Let’s create visualizations of both the interaction and parallel slopes model once again and display the output in Figure 19.8. Recall from Subsection 19.1.3 that the geom_parallel_slopes() function is a special purpose function included in the moderndive package, since the geom_smooth() method in the ggplot2 package does not have a convenient way to plot parallel slopes models. # Interaction model ggplot(MA_schools, aes(x = perc_disadvan, y = average_sat_math, color = size)) + geom_point(alpha = 0.25) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Percent economically disadvantaged&quot;, y = &quot;Math SAT Score&quot;, color = &quot;School size&quot;, title = &quot;Interaction model&quot;) # Parallel slopes model ggplot(MA_schools, aes(x = perc_disadvan, y = average_sat_math, color = size)) + geom_point(alpha = 0.25) + geom_parallel_slopes(se = FALSE) + labs(x = &quot;Percent economically disadvantaged&quot;, y = &quot;Math SAT Score&quot;, color = &quot;School size&quot;, title = &quot;Parallel slopes model&quot;) FIGURE 19.8: Comparison of interaction and parallel slopes models for Massachusetts schools. Look closely at the left-hand plot of Figure 19.8 corresponding to an interaction model. While the slopes are indeed different, they do not differ by much and are nearly identical. Now compare the left-hand plot with the right-hand plot corresponding to a parallel slopes model. The two models don’t appear all that different. So in this case, it can be argued that the additional complexity of the interaction model is not warranted. Thus following Occam’s Razor, we should prefer the “simpler” parallel slopes model. Let’s explicitly define what “simpler” means in this case. Let’s compare the regression tables for the interaction and parallel slopes models in Tables 19.12 and 19.13. model_2_interaction &lt;- lm(average_sat_math ~ perc_disadvan * size, data = MA_schools) get_regression_table(model_2_interaction) TABLE 19.12: Interaction model regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 594.327 13.288 44.726 0.000 568.186 620.469 perc_disadvan -2.932 0.294 -9.961 0.000 -3.511 -2.353 size: medium -17.764 15.827 -1.122 0.263 -48.899 13.371 size: large -13.293 13.813 -0.962 0.337 -40.466 13.880 perc_disadvan:sizemedium 0.146 0.371 0.393 0.694 -0.585 0.877 perc_disadvan:sizelarge 0.189 0.323 0.586 0.559 -0.446 0.824 model_2_parallel_slopes &lt;- lm(average_sat_math ~ perc_disadvan + size, data = MA_schools) get_regression_table(model_2_parallel_slopes) TABLE 19.13: Parallel slopes regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 588.19 7.607 77.325 0.000 573.23 603.15 perc_disadvan -2.78 0.106 -26.120 0.000 -2.99 -2.57 size: medium -11.91 7.535 -1.581 0.115 -26.74 2.91 size: large -6.36 6.923 -0.919 0.359 -19.98 7.26 Observe how the regression table for the interaction model has 2 more rows (6 versus 4). This reflects the additional “complexity” of the interaction model over the parallel slopes model. Furthermore, note in Table 19.12 how the offsets for the slopes perc_disadvan:sizemedium being 0.146 and perc_disadvan:sizelarge being 0.189 are small relative to the slope for the baseline group of small schools of \\(-2.932\\). In other words, all three slopes are similarly negative: \\(-2.932\\) for small schools, \\(-2.786\\) \\((=-2.932 + 0.146)\\) for medium schools, and \\(-2.743\\) \\((=-2.932 + 0.189)\\) for large schools. These results are suggesting that irrespective of school size, the relationship between average math SAT scores and the percent of the student body that is economically disadvantaged is similar and, alas, quite negative. What you have just performed is a rudimentary model selection: choosing which model fits data best among a set of candidate models. The model selection we performed used the “eyeball test”: qualitatively looking at visualizations to choose a model. In the next subsection, you’ll once again perform the same model selection, but this time using a numerical approach via the \\(R^2\\) (pronounced “R-squared”) value. 19.3.2 Model selection using R-squared At the end of the previous section in Figure 19.8 you compared an interaction model with a parallel slopes model, where both models attempted to explain \\(y\\) = the average math SAT score for various high schools in Massachusetts. In Tables 19.12 and 19.13, we observed that the interaction model was “more complex” in that the regression table had 6 rows versus the 4 rows of the parallel slopes model. Most importantly however, when comparing the left and right-hand plots of Figure 19.8, we observed that the three lines corresponding to small, medium, and large high schools were not that different. Given this similarity, we stated it could be argued that the “simpler” parallel slopes model should be favored. In this section, we’ll mimic the model selection we just performed using the qualitative “eyeball test”, but this time using a numerical and quantitative approach. Specifically, we’ll use the \\(R^2\\) summary statistic (pronounced “R-squared”), also called the “coefficient of determination”. But first, we must introduce one new concept: the variance of a numerical variable. We’ve previously studied two summary statistics of the spread (or variation) of a numerical variable: the standard deviation when studying the normal distribution in ?? and the interquartile range (IQR) when studying boxplots in Section 12.7.1. We now introduce a third summary statistic of spread: the variance. The variance is merely the standard deviation squared and it can be computed in R using the var() summary function within summarize(). If you would like to see the formula, see ??. Recall that to get: 1) the observed values \\(y\\), 2) the fitted values \\(\\widehat{y}\\) from a regression model, and 3) the resulting residuals \\(y - \\widehat{y}\\), we can apply the get_regression_points() function our saved model, in this case model_2_interaction: get_regression_points(model_2_interaction) # A tibble: 332 × 6 ID average_sat_math perc_disadvan size average_sat_math_hat residual &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 516 21.5 medium 516.67 -0.67 2 2 514 22.7 large 518.771 -4.771 3 3 534 14.6 large 540.988 -6.988 4 4 581 6.3 large 563.754 17.246 5 5 592 10.3 large 552.783 39.217 6 6 576 10.3 large 552.783 23.217 7 7 504 25.6 large 510.816 -6.816 8 8 505 15.2 large 539.343 -34.343 9 9 481 23.8 small 524.548 -43.548 10 10 513 25.5 large 511.091 1.909 # ℹ 322 more rows Let’s now use the var() summary function within a summarize() to compute the variance of these three terms: get_regression_points(model_2_interaction) %&gt;% summarize(var_y = var(average_sat_math), var_y_hat = var(average_sat_math_hat), var_residual = var(residual)) # A tibble: 1 × 3 var_y var_y_hat var_residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3691.26 2580.47 1110.78 Observe that the variance of \\(y\\) is equal to the variance of \\(\\widehat{y}\\) plus the variance of the residuals. But what do these three terms tell us individually? First, the variance of \\(y\\) (denoted as \\(var(y)\\)) tells us how much do Massachusetts high schools differ in average math SAT scores. The goal of regression modeling is to fit a model that hopefully explains this variation. In other words, we want to understand what factors explain why certain schools have high math SAT scores, while others have low scores. This is independent of the model; this is just data. In other words, whether we fit an interaction or parallel slopes model, \\(var(y)\\) remains the same. Second, the variance of \\(\\widehat{y}\\) (denoted as \\(var(\\widehat{y})\\)) tells us how much the fitted values from our interaction model vary. That is to say, after accounting for (1) the percentage of the study body that is socioeconomically disadvantaged and (2) school size in an interaction model, how much do our model’s explanations of average math SAT scores vary? Third, the variance of the residuals tells us how much do “the left-overs” from the model vary. Observe how the points in the left-hand plot of Figure 19.8 scatter around the three lines. Say instead all the points fell exactly on one of the three lines. Then all residuals would be zero and hence the variance of the residuals would be zero. We’re now ready to introduce \\(R^2\\): \\[ R^2 = \\frac{var(\\widehat{y})}{var(y)} \\] It is the proportion of the spread/variation of the outcome variable \\(y\\) that is explained by our model, where our model’s explanatory power is embedded in the fitted values \\(\\widehat{y}\\). Furthermore, since it can be mathematically proven that \\(0 \\leq var(\\widehat{y}) \\leq var(y)\\) (a fact we leave for an advanced class on regression), we are guaranteed that: \\[ 0 \\leq R^2 \\leq 1 \\] \\(R^2\\) can be interpreted as follows: \\(R^2\\) values of 0 tell us that our model explains 0% of the variation in \\(y\\). Say we fit a model to the Massachusetts high school data and obtained \\(R^2 = 0\\). This would be telling us that the combination of explanatory variables \\(x\\) we used and model form we chose (interaction or parallel slopes) tell us nothing about average math SAT scores. The model is a poor fit. \\(R^2\\) values of 1 tell us that our model explains 100% of the variation in \\(y\\). Say we fit a model to the Massachusetts high school data and obtained \\(R^2 = 1\\). This would be telling us that the combination of explanatory variables \\(x\\) we used and model form we chose (interaction or parallel slopes) tell us everything we need to know about average math SAT scores. In practice however, \\(R^2\\) values of 1 almost never occur. Think about it in the context of Massachusetts high schools. There are an infinite number of factors that influence why certain high schools perform well on SAT’s on average while others don’t perform well. The idea that a human-designed statistical model can capture all the heterogeneity of all high school students in Massachusetts is bordering on hubris. However, even if such models are not perfect, they may still prove useful in determining educational policy. A general principle of modeling we should keep in mind is a famous quote by eminent statistician George Box: “All models are wrong, but some are useful.” Let’s repeat the above calculations for the parallel slopes model and compare them in Table 19.14. TABLE 19.14: Comparing variances from interaction and parallel slopes models for MA school data model var_y var_y_hat var_residual r_squared Interaction 3691 2580 1111 0.699 Parallel slopes 3691 2579 1112 0.699 Observe how the \\(R^2\\) values are near identical at around 0.699 = 69.9%. In other words, the additional complexity of the interaction model only improves our \\(R^2\\) value by a near zero amount. Thus, we are inclined to favor the “simpler” parallel slopes model. Now let’s repeat this \\(R^2\\) comparison between interaction and parallel slopes model for our models of \\(y\\) = teaching score for UT Austin professors which you visually compared in Figure 19.7. We compare these values in Table 19.15 TABLE 19.15: Comparing variances from interaction and parallel slopes models for UT Austin data model var_y var_y_hat var_residual r_squared Interaction 0.296 0.015 0.281 0.051 Parallel slopes 0.296 0.012 0.284 0.039 Observe how the \\(R^2\\) values are now very different! In other words, since the additional complexity of the interaction model over the parallel slopes model improves our \\(R^2\\) value by a relatively large amount (0.051 versus 0.039, which is an increase of about 31.5%), it could be argued that the additional complexity is warranted. As a final note, we can also use the third of our get_regression() wrapper functions, get_regression_summaries(), to quickly automate calculating \\(R^2\\) for both the interaction and parallels slopes models for \\(y\\) = average math SAT score for Massachusetts high schools. # R-squared for interaction model: get_regression_summaries(model_2_interaction) # A tibble: 1 × 9 r_squared adj_r_squared mse rmse sigma statistic p_value df nobs &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.699 0.694 1107.44 33.2782 33.583 151.467 0 5 332 # R-squared for parallel slopes model: get_regression_summaries(model_2_parallel_slopes) # A tibble: 1 × 9 r_squared adj_r_squared mse rmse sigma statistic p_value df nobs &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.699 0.696 1108.61 33.2958 33.498 253.609 0 3 332 19.3.3 Correlation coefficient Recall from Table 19.9 that the correlation coefficient between income in thousands of dollars and credit card debt was 0.464. What if instead we looked at the correlation coefficient between income and credit card debt, but where income was in dollars and not thousands of dollars? This can be done by multiplying income by 1000. credit_ch6 %&gt;% select(debt, income) %&gt;% mutate(income = income * 1000) %&gt;% cor() TABLE 19.16: Correlation between income (in dollars) and credit card debt debt income debt 1.000 0.464 income 0.464 1.000 We see it is the same! We say that the correlation coefficient is invariant to linear transformations. The correlation between \\(x\\) and \\(y\\) will be the same as the correlation between \\(a\\cdot x + b\\) and \\(y\\) for any numerical values \\(a\\) and \\(b\\). 19.3.4 Simpson’s Paradox Recall in Section 19.2, we saw the two seemingly contradictory results when studying the relationship between credit card debt and income. On the one hand, the right hand plot of Figure 19.5 suggested that the relationship between credit card debt and income was positive. We re-display this in Figure 19.9. FIGURE 19.9: Relationship between credit card debt and income. On the other hand, the multiple regression results in Table 19.10 suggested that the relationship between debt and income was negative. We re-display this information in Table 19.17. TABLE 19.17: Multiple regression results term estimate std_error statistic p_value lower_ci upper_ci intercept -385.179 19.465 -19.8 0 -423.446 -346.912 credit_limit 0.264 0.006 45.0 0 0.253 0.276 income -7.663 0.385 -19.9 0 -8.420 -6.906 Observe how the slope for income is \\(-7.663\\) and, most importantly for now, it is negative. This contradicts our observation in Figure 19.9 that the relationship is positive. How can this be? Recall the interpretation of the slope for income in the context of a multiple regression model: taking into account all the other explanatory variables in our model, for every increase of one unit in income (i.e., $1000), there is an associated decrease of on average $7.663 in debt. In other words, while in isolation, the relationship between debt and income may be positive, when taking into account credit_limit as well, this relationship becomes negative. These seemingly paradoxical results are due to a phenomenon aptly named Simpson’s Paradox. Simpson’s Paradox occurs when trends that exist for the data in aggregate either disappear or reverse when the data are broken down into groups. Let’s show how Simpson’s Paradox manifests itself in the credit_ch6 data. Let’s first visualize the distribution of the numerical explanatory variable credit_limit with a histogram in Figure 19.10. FIGURE 19.10: Histogram of credit limits and brackets. The vertical dashed lines are the quartiles that cut up the variable credit_limit into four equally sized groups. Let’s think of these quartiles as converting our numerical variable credit_limit into a categorical variable “credit_limit bracket” with four levels. This means that 25% of credit limits were between $0 and $3088. Let’s assign these 100 people to the “low” credit_limit bracket. 25% of credit limits were between $3088 and $4622. Let’s assign these 100 people to the “medium-low” credit_limit bracket. 25% of credit limits were between $4622 and $5873. Let’s assign these 100 people to the “medium-high” credit_limit bracket. 25% of credit limits were over $5873. Let’s assign these 100 people to the “high” credit_limit bracket. Now in Figure 19.11 let’s re-display two versions of the scatterplot of debt and income from Figure 19.9, but with a slight twist: The left-hand plot shows the regular scatterplot and the single regression line, just as you saw in Figure 19.9. The right-hand plot shows the colored scatterplot, where the color aesthetic is mapped to “credit_limit bracket.” Furthermore, there are now four separate regression lines. In other words, the location of the 400 points are the same in both scatterplots, but the right-hand plot shows an additional variable of information: credit_limit bracket. FIGURE 19.11: Relationship between credit card debt and income by credit limit bracket. The left-hand plot of Figure 19.11 focuses on the relationship between debt and income in aggregate. It is suggesting that overall there exists a positive relationship between debt and income. However, the right-hand plot of Figure 19.11 focuses on the relationship between debt and income broken down by credit_limit bracket. In other words, we focus on four separate relationships between debt and income: one for the “low” credit_limit bracket, one for the “medium-low” credit_limit bracket, and so on. Observe in the right-hand plot that the relationship between debt and income is clearly negative for the “medium-low” and “medium-high” credit_limit brackets, while the relationship is somewhat flat for the “low” credit_limit bracket. The only credit_limit bracket where the relationship remains positive is for the “high” credit_limit bracket. However, this relationship is less positive than in the relationship in aggregate, since the slope is shallower than the slope of the regression line in the left-hand plot. In this example of Simpson’s Paradox, the credit_limit is a confounding variable of the relationship between credit card debt and income as we defined in Subsection 17.3.1. Thus, credit_limit needs to be accounted for in any appropriate model for the relationship between debt and income. 19.4 Conclusion 19.4.1 Additional resources An R script file of all R code used in this chapter is available here. 19.4.2 What’s to come? Congratulations! We’ve completed the “Data Modeling with moderndive” portion of this book. We’re ready to proceed to Part III of this book: “Statistical Inference with infer.” Statistical inference is the science of inferring about some unknown quantity using sampling. The most well-known examples of sampling in practice involve polls. Because asking an entire population about their opinions would be a long and arduous task, pollsters often take a smaller sample that is hopefully representative of the population. Based on the results of this sample, pollsters hope to make claims about the entire population. Once we’ve covered Chapters 14 on sampling, 15 on confidence intervals, and 16 on hypothesis testing, we’ll revisit the regression models we studied in Chapters 17 and 19 in Chapter 18 on inference for regression. So far, we’ve only studied the estimate column of all our regression tables. The next four chapters focus on what the remaining columns mean: the standard error (std_error), the test statistic, the p_value, and the lower and upper bounds of confidence intervals (lower_ci and upper_ci). Furthermore in Chapter 18, we’ll revisit the concept of residuals \\(y - \\widehat{y}\\) and discuss their importance when interpreting the results of a regression model. We’ll perform what is known as a residual analysis of the residual variable of all get_regression_points() outputs. Residual analyses allow you to verify what are known as the conditions for inference for regression. On to Chapter 14 on sampling in Part III as shown in Figure 19.12! FIGURE 19.12: ModernDive flowchart - on to Part III! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
